{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/Assignment_3_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgzPiclr4Cun"
      },
      "source": [
        "# Step 0: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S471E7Q86cPH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.datasets import fetch_20newsgroups # We use the 20 news groups text dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW53Mx7p3LqE"
      },
      "source": [
        "# Step 1: Fetching data and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZHLDZSB2iOq"
      },
      "outputs": [],
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nLT1si_CqT4",
        "outputId": "62cf3f43-1505-4e80-feb0-5af29ab84904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "print(len(newsgroups_train.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejgOpsbANmyk"
      },
      "outputs": [],
      "source": [
        "# Split into smaller training sets in percentage\n",
        "percentage = 0.8\n",
        "split_index = int(len(newsgroups_train.data) * percentage)\n",
        "train_data_small = newsgroups_train.data[:split_index]\n",
        "train_targets_small = newsgroups_train.target[:split_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv0CKmXAOKnx",
        "outputId": "59f8e694-9e7d-40fa-af0e-a28a29b55e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_data_small[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEGD896W86n2",
        "outputId": "17b0acc4-d9bd-446c-84ca-c55c8e26b420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From lerxst wam umd edu where s my thing Subject WHAT car is this Nntp Posting Host wam umd edu Organization University of Maryland College Park Lines I was wondering if anyone out there could enlighten me on this car I saw the other day It was a door sports car looked to be from the late early It was called a Bricklin The doors were really small In addition the front bumper was separate from the rest of the body This is all I know If anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail Thanks IL brought to you by your neighborhood Lerxst\n"
          ]
        }
      ],
      "source": [
        "def extract_body(text):\n",
        "    # Extract only words using regex\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "\n",
        "    # Join the words with spaces (optional)\n",
        "    cleaned_text = \" \".join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "removed_headers = extract_body(train_data_small[0])\n",
        "print(removed_headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHCCZWe3b9X",
        "outputId": "b6f10aa3-9c59-4c07-f3e7-e62a4e92bd53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Articles"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9051/9051 [00:05<00:00, 1591.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique words: 74122\n",
            "Word-to-ID mapping example: {'aa': 0, 'aaa': 1, 'aaaa': 2, 'aaaaaaaaaaaa': 3, 'aaaaagggghhhh': 4}\n",
            "Integer Corpus example (first article): [36478, 70000, 66878, 18878, 64337, 61863, 8966, 44346, 49795, 28837]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize structures for the preprocessed corpus\n",
        "filtered_train = [[] for _ in range(len(train_data_small))]  # Preprocessed articles\n",
        "flattened_train = []  # A single list of all words in the corpus\n",
        "\n",
        "# Tokenizing and removing stopwords\n",
        "print(\"Processing Articles\")\n",
        "for i, article in tqdm(enumerate(train_data_small), total=len(train_data_small)):\n",
        "    article_body = extract_body(article) # Only use body and remove headers and footers\n",
        "    word_tokens = word_tokenize(article_body)  # Tokenize article\n",
        "    # Remove stop words and add to both filtered_train and flattened_train\n",
        "    filtered_words = [w.lower() for w in word_tokens if w.lower() not in stop_words]\n",
        "    filtered_train[i] = filtered_words\n",
        "    flattened_train.extend(filtered_words)\n",
        "\n",
        "# Create a vocabulary mapping\n",
        "unique_words = sorted(set(flattened_train))  # Get unique words\n",
        "word_to_id = {word: idx for idx, word in enumerate(unique_words)}  # Map word to ID\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}  # Reverse mapping\n",
        "\n",
        "# Map the filtered articles to integer IDs\n",
        "int_corpus = [[word_to_id[word] for word in article] for article in filtered_train]\n",
        "\n",
        "# Display mappings and a small example\n",
        "print(f\"Total unique words: {len(unique_words)}\")\n",
        "print(\"Word-to-ID mapping example:\", {k: word_to_id[k] for k in list(word_to_id)[:5]})\n",
        "print(\"Integer Corpus example (first article):\", int_corpus[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku3E7_9eW93G"
      },
      "outputs": [],
      "source": [
        "word_count = Counter(flattened_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3wP-jEL1u1"
      },
      "outputs": [],
      "source": [
        "# Extract low-frequency words (occurrence <= 10) into a set\n",
        "low_frequency_words = {word for word, count in word_count.items() if count <= 10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q7WUkGCFsIX",
        "outputId": "77a8de04-68af-4ef2-9fbc-d3b320f561b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Removing LF words: 100%|██████████| 9051/9051 [00:00<00:00, 81097.88it/s]\n"
          ]
        }
      ],
      "source": [
        "# Filter articles efficiently using set operations\n",
        "corpus_hf = []\n",
        "for article in tqdm(int_corpus, desc=\"Removing LF words\"):\n",
        "    article_set = set(article)\n",
        "    filtered_article = list(article_set - low_frequency_words)\n",
        "    corpus_hf.append(filtered_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLmxtbMSQNjs",
        "outputId": "c81e98d4-4400-4051-e797-5a650e0995a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in corpus 1467687\n",
            "Vocabulary size after removing LF words 14426\n",
            "74122\n"
          ]
        }
      ],
      "source": [
        "flattened_train = [word for word in flattened_train if word not in low_frequency_words]\n",
        "voc_size = len(sorted(set(flattened_train)))\n",
        "print(\"Number of words in corpus\", len(flattened_train))\n",
        "print(\"Vocabulary size after removing LF words\", voc_size)\n",
        "print(len(list(set(word for doc in corpus_hf for word in doc))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4pGSix73wx"
      },
      "source": [
        "# Step 2: Gibbs sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U85ycesL76R8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def lda_gibbs_sampling(corpus, K, alpha, beta, iterations):\n",
        "    \"\"\"\n",
        "    Implements Collapsed Gibbs Sampling for LDA.\n",
        "\n",
        "    :param corpus: List of lists, where each inner list contains word IDs in a document.\n",
        "    :param K: Number of topics.\n",
        "    :param alpha: Dirichlet prior for document-topic distribution.\n",
        "    :param beta: Dirichlet prior for topic-word distribution.\n",
        "    :param iterations: Number of Gibbs sampling iterations.\n",
        "    :return: Topic assignments, document-topic counts, topic-word counts, topic totals.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    D = len(corpus)  # Number of documents\n",
        "    V = len(list(set(word for doc in corpus for word in doc)))\n",
        "\n",
        "    doc_word_matrix = np.zeros((D, V), dtype=bool)\n",
        "\n",
        "    # Count matrices\n",
        "    ndk = np.zeros((D, K))  # Document-topic counts\n",
        "    nkw = np.zeros((K, V))  # Topic-word counts\n",
        "    nk = np.zeros(K)        # Total words in each topic\n",
        "\n",
        "    # Topic assignments for each word\n",
        "    z = []  # Topic assignment for each word in corpus\n",
        "    for d, doc in enumerate(corpus):\n",
        "        doc_topics = []\n",
        "        for word in doc:\n",
        "            topic = np.random.randint(K)  # Randomly assign a topic\n",
        "            doc_topics.append(topic)\n",
        "            ndk[d, topic] += 1\n",
        "            nkw[topic, word] += 1\n",
        "            nk[topic] += 1\n",
        "            doc_word_matrix[d, word] = True\n",
        "        z.append(doc_topics)\n",
        "\n",
        "    # Gibbs sampling\n",
        "    for _ in tqdm(range(iterations)):\n",
        "        for d, doc in enumerate(corpus):\n",
        "            for i, word in enumerate(doc):\n",
        "                current_topic = z[d][i]\n",
        "\n",
        "                # Decrement counts\n",
        "                ndk[d, current_topic] -= 1\n",
        "                nkw[current_topic, word] -= 1\n",
        "                nk[current_topic] -= 1\n",
        "\n",
        "                # Compute topic probabilities (Maybe do a for loop here instead)\n",
        "                topic_probs = (ndk[d] + alpha) * (nkw[:, word] + beta) / (nk + beta * V)\n",
        "                topic_probs /= np.sum(topic_probs)  # Normalize\n",
        "\n",
        "                # Sample new topic\n",
        "                new_topic = np.random.choice(K, p=topic_probs)\n",
        "                z[d][i] = new_topic\n",
        "\n",
        "                # Increment counts\n",
        "                ndk[d, new_topic] += 1\n",
        "                nkw[new_topic, word] += 1\n",
        "                nk[new_topic] += 1\n",
        "\n",
        "    return z, ndk, nkw, nk, doc_word_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMPOEWe8FB1",
        "outputId": "6c6cade7-0f60-44c0-828d-b3c7b352286c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [52:26<00:00, 20.98s/it]\n"
          ]
        }
      ],
      "source": [
        "# Initialize data\n",
        "corpus = corpus_hf.copy()\n",
        "targets = train_targets_small.copy()\n",
        "topics = newsgroups_train.target_names.copy()\n",
        "\n",
        "# First parameter combo\n",
        "z, ndk, nkw, nk, doc_word_matrix = lda_gibbs_sampling(corpus, alpha = 0.1, beta = 0.1, K=5, iterations=150)\n",
        "\n",
        "# Save output as pickle file\n",
        "with open('output.pkl', 'wb') as handle:\n",
        "    pickle.dump((z, ndk, nkw, nk, doc_word_matrix), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKX8MdD3cGWr",
        "outputId": "308cd8a5-ddb5-4899-884c-5f3a9d6b9d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9051 (9051, 5) (5, 74122) (5,) (9051, 74122)\n"
          ]
        }
      ],
      "source": [
        "# Load output from pickle\n",
        "with open('output.pkl', 'rb') as handle:\n",
        "    z, ndk, nkw, nk, doc_word_matrix = pickle.load(handle)\n",
        "\n",
        "print(len(z), ndk.shape, nkw.shape, nk.shape, doc_word_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AxH9Ge6fQIv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def get_top_words(nkw, id_to_word, top_n=20, method=\"raw\", beta=0.1):\n",
        "    \"\"\"\n",
        "    Get the top words for each topic.\n",
        "\n",
        "    :param nkw: Topic-word counts (K x V matrix).\n",
        "    :param id_to_word: Dictionary mapping word IDs to their original words.\n",
        "    :param top_n: Number of top words to retrieve per topic.\n",
        "    :param method: \"raw\" for raw counts, \"relative\" for relative frequencies.\n",
        "    :param beta: Dirichlet prior for smoothing (used in relative frequency).\n",
        "    :return: Dictionary of top words for each topic.\n",
        "    \"\"\"\n",
        "    K, V = nkw.shape\n",
        "    top_words_per_topic = {}\n",
        "\n",
        "    if method == \"raw\":\n",
        "        # Use raw counts\n",
        "        for k in range(K):\n",
        "            top_word_indices = np.argsort(nkw[k, :])[::-1][:top_n]  # Top N words by count\n",
        "            top_words_per_topic[k] = [idx for idx in top_word_indices]\n",
        "\n",
        "    elif method == \"relative\":\n",
        "        # Compute relative frequencies\n",
        "        word_totals = np.sum(nkw, axis=0)  # Total count of each word across all topics\n",
        "        for k in range(K): # for k in topics\n",
        "            relative_freqs = (nkw[k, :] + beta) / (word_totals + beta * K)  # Smoothed relative frequency\n",
        "            top_word_indices = np.argsort(relative_freqs)[::-1][:top_n]  # Top N words by relative frequency\n",
        "            top_words_per_topic[k] = [\n",
        "                idx for idx in top_word_indices\n",
        "            ]\n",
        "\n",
        "    return top_words_per_topic\n",
        "\n",
        "\n",
        "def top_words_to_df(top_words):\n",
        "    \"\"\"\n",
        "    Display the top words for each topic in a table format.\n",
        "\n",
        "    :param top_words_per_topic: Dictionary of top words for each topic.\n",
        "    :param method: Description of the method used (\"raw\" or \"relative\").\n",
        "    \"\"\"\n",
        "\n",
        "    top_words_per_topic = {t: [id_to_word[value] for value in values] for t, values in top_words.items()}\n",
        "    df_top_words = pd.DataFrame.from_dict(top_words_per_topic)\n",
        "    df_top_words.columns = [f\"Topic {i+1}\" for i in range(df_top_words.shape[1])]\n",
        "    return df_top_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBiSOQR0cZ0g",
        "outputId": "d5c9ccad-cc1a-4edc-bacc-2172870444d4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "      <th>Topic 4</th>\n",
              "      <th>Topic 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>encryption</td>\n",
              "      <td>israeli</td>\n",
              "      <td>ftp</td>\n",
              "      <td>baseball</td>\n",
              "      <td>bike</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>secure</td>\n",
              "      <td>turkish</td>\n",
              "      <td>ram</td>\n",
              "      <td>season</td>\n",
              "      <td>bmw</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nsa</td>\n",
              "      <td>armenia</td>\n",
              "      <td>shipping</td>\n",
              "      <td>hockey</td>\n",
              "      <td>riding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>escrow</td>\n",
              "      <td>arab</td>\n",
              "      <td>scsi</td>\n",
              "      <td>nhl</td>\n",
              "      <td>orbit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crypto</td>\n",
              "      <td>turks</td>\n",
              "      <td>floppy</td>\n",
              "      <td>atheists</td>\n",
              "      <td>honda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>amendment</td>\n",
              "      <td>arabs</td>\n",
              "      <td>vga</td>\n",
              "      <td>athos</td>\n",
              "      <td>bikes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>announcement</td>\n",
              "      <td>argic</td>\n",
              "      <td>motif</td>\n",
              "      <td>detroit</td>\n",
              "      <td>motorcycle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>committed</td>\n",
              "      <td>extermination</td>\n",
              "      <td>upgrade</td>\n",
              "      <td>playoffs</td>\n",
              "      <td>geb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>enforcement</td>\n",
              "      <td>israelis</td>\n",
              "      <td>cpu</td>\n",
              "      <td>stanley</td>\n",
              "      <td>shuttle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>armed</td>\n",
              "      <td>zuma</td>\n",
              "      <td>ide</td>\n",
              "      <td>atheism</td>\n",
              "      <td>ama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>criminal</td>\n",
              "      <td>sera</td>\n",
              "      <td>motherboard</td>\n",
              "      <td>playoff</td>\n",
              "      <td>cadre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>criminals</td>\n",
              "      <td>appressian</td>\n",
              "      <td>macintosh</td>\n",
              "      <td>braves</td>\n",
              "      <td>chastity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>governments</td>\n",
              "      <td>sahak</td>\n",
              "      <td>dos</td>\n",
              "      <td>clh</td>\n",
              "      <td>dsl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>firearms</td>\n",
              "      <td>melkonian</td>\n",
              "      <td>processor</td>\n",
              "      <td>bible</td>\n",
              "      <td>prb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>privacy</td>\n",
              "      <td>ohanus</td>\n",
              "      <td>simms</td>\n",
              "      <td>leafs</td>\n",
              "      <td>motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>threat</td>\n",
              "      <td>palestinian</td>\n",
              "      <td>expo</td>\n",
              "      <td>pitching</td>\n",
              "      <td>rider</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>trial</td>\n",
              "      <td>villages</td>\n",
              "      <td>quadra</td>\n",
              "      <td>scored</td>\n",
              "      <td>radar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>legitimate</td>\n",
              "      <td>palestine</td>\n",
              "      <td>client</td>\n",
              "      <td>sandvik</td>\n",
              "      <td>parking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>sexual</td>\n",
              "      <td>igc</td>\n",
              "      <td>widget</td>\n",
              "      <td>kings</td>\n",
              "      <td>gear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>agencies</td>\n",
              "      <td>cpr</td>\n",
              "      <td>desktop</td>\n",
              "      <td>devils</td>\n",
              "      <td>spacecraft</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Topic 1        Topic 2      Topic 3   Topic 4      Topic 5\n",
              "0     encryption        israeli          ftp  baseball         bike\n",
              "1         secure        turkish          ram    season          bmw\n",
              "2            nsa        armenia     shipping    hockey       riding\n",
              "3         escrow           arab         scsi       nhl        orbit\n",
              "4         crypto          turks       floppy  atheists        honda\n",
              "5      amendment          arabs          vga     athos        bikes\n",
              "6   announcement          argic        motif   detroit   motorcycle\n",
              "7      committed  extermination      upgrade  playoffs          geb\n",
              "8    enforcement       israelis          cpu   stanley      shuttle\n",
              "9          armed           zuma          ide   atheism          ama\n",
              "10      criminal           sera  motherboard   playoff        cadre\n",
              "11     criminals     appressian    macintosh    braves     chastity\n",
              "12   governments          sahak          dos       clh          dsl\n",
              "13      firearms      melkonian    processor     bible          prb\n",
              "14       privacy         ohanus        simms     leafs  motorcycles\n",
              "15        threat    palestinian         expo  pitching        rider\n",
              "16         trial       villages       quadra    scored        radar\n",
              "17    legitimate      palestine       client   sandvik      parking\n",
              "18        sexual            igc       widget     kings         gear\n",
              "19      agencies            cpr      desktop    devils   spacecraft"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_words_per_topic = get_top_words(nkw, id_to_word, top_n=20, method=\"relative\")\n",
        "df_top_words = top_words_to_df(top_words_per_topic)\n",
        "df_top_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6KVf1sx86n3",
        "outputId": "b1d4fd23-8cec-4745-d67b-a323b5e884b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "print((doc_word_matrix[:,0] & doc_word_matrix[:,1]).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUffjXYV86n3"
      },
      "outputs": [],
      "source": [
        "def umass(corpus, n_topics, top_words, top_n, epsilon=1, print_=False):\n",
        "\n",
        "        coherence = []\n",
        "        common_words = list(top_words.values())\n",
        "\n",
        "        for k in range(n_topics):\n",
        "            # for each topic\n",
        "\n",
        "\n",
        "            for vi in range(top_n-1):\n",
        "                # for each word in each topic\n",
        "                D_vi = 0\n",
        "\n",
        "\n",
        "                for doc in corpus:\n",
        "                    # for each document\n",
        "\n",
        "                    D_vi_vj = 0\n",
        "                    vj = vi+1\n",
        "\n",
        "                    if common_words[k][vi] in doc:\n",
        "                        # word is in the document\n",
        "                        D_vi = D_vi + 1\n",
        "\n",
        "                        if common_words[k][vj] in doc:\n",
        "                            # only check lag-1 words if word vi in doc\n",
        "                            D_vi_vj = D_vi_vj + 1\n",
        "\n",
        "                if D_vi != 0:\n",
        "                    # catch errors\n",
        "                    coherence.append(np.log10((D_vi_vj+epsilon)/D_vi))\n",
        "\n",
        "        model_coherence = np.sum(coherence)/n_topics\n",
        "\n",
        "        if print_: print(\"coherence for each topic: \", model_coherence)\n",
        "\n",
        "        return model_coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8gzns7c86n3",
        "outputId": "6afb84f2-c6a9-4841-a7bb-593ae4b86543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coherence for each topic:  -37.59953130389676\n",
            "-37.59953130389676\n"
          ]
        }
      ],
      "source": [
        "print(umass(corpus, 5, top_words_per_topic, top_n=20, print_=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}