{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparations and code from Assignment 1"
      ],
      "metadata": {
        "id": "Ash16YV-b6T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group 17: Jakob Svensson, Mahdi Afarideh, Maximilian Forsell"
      ],
      "metadata": {
        "id": "OWJbmNgzBECe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
        "%cd Intro-to-language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXuQ-VlzuRv",
        "outputId": "82f36801-7c09-4cd8-de2b-b13cd6e5a7db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-language-modeling'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 82 (delta 43), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (82/82), 33.44 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/Intro-to-language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5fv9gQcVafW3"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import nltk\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLeT6K6EDID5",
        "outputId": "af8c8fc3-a349-453d-fc7d-20cdc9a2a5a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "Qjj4IdOi08ms"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDybdl906rv",
        "outputId": "49e5fc2f-47f0-491c-e56e-5160c57d6c34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='lmdemo'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "!unzip -q $zip_file\n",
        "!rm $zip_file"
      ],
      "metadata": {
        "id": "IE8oAx8b3AWX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set=open(f'{dataset}/train.txt','r',encoding='utf-8').read()\n",
        "val_set=open(f'{dataset}/val.txt','r',encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "clFRaGPQ4Jc-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "class VocabularyBuilder:\n",
        "    def __init__(self, max_voc_size):\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.str_to_int = {}\n",
        "        self.int_to_str = {}\n",
        "        self.special_tokens = [\"BEGINNING\", \"END\", \"UNKNOWN\", \"PADDING\"] #Added padding\n",
        "        self.token_counter = Counter()\n",
        "\n",
        "    def build_vocabulary(self, text):\n",
        "\n",
        "        sents=nltk.word_tokenize(text.lower())\n",
        "\n",
        "        for token in sents:\n",
        "            self.token_counter[token] += 1\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = self.token_counter.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def create_premade_vocabulary(self, c):\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = c.most_common(max_words) # Here we can use a premade counter from a previous run\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def get_token_id(self, token):\n",
        "        return self.str_to_int.get(token.lower(), self.str_to_int[\"UNKNOWN\"])\n",
        "\n",
        "    def get_token_str(self, token_id):\n",
        "        return self.int_to_str.get(token_id, \"UNKNOWN\")\n",
        "\n",
        "    def sanity_check(self): # Here we run the sanity tests recommended in the assignment\n",
        "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
        "\n",
        "        for token in self.special_tokens:\n",
        "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
        "\n",
        "        common_words = [\"the\", \"and\"]\n",
        "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
        "\n",
        "        for word in common_words:\n",
        "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
        "\n",
        "        for word in rare_words:\n",
        "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
        "\n",
        "        test_word = \"the\"\n",
        "        token_id = self.get_token_id(test_word)\n",
        "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
        "\n",
        "        print(\"Sanity check passed!\")\n",
        "\n",
        "vocab_builder = VocabularyBuilder(max_voc_size=16384)\n"
      ],
      "metadata": {
        "id": "CZJ4k7STz96H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for paragraph in tqdm(training_set.splitlines()):\n",
        "  vocab_builder.build_vocabulary(paragraph)\n",
        "vocab_builder.create_vocabulary()"
      ],
      "metadata": {
        "id": "T1LhuLzfE2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a80e0da-135d-4111-ed59-31b729b4ee2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 294118/294118 [01:37<00:00, 3023.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sanity check\n",
        "vocab_builder.sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sB4W8Q0Qo9",
        "outputId": "d9872069-2b24-402b-da6b-f7fd6f80dda0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified for assignment 2\n",
        "class TrainingDataPreparerRNN:\n",
        "    def __init__(self, vocab_builder, max_sequence_length):\n",
        "        self.vocab_builder = vocab_builder\n",
        "        self.max = max_sequence_length\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"Tokenizes and encodes a single string with special symbols.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to encode.\n",
        "\n",
        "        Returns:\n",
        "        - List[int]: A list of token IDs including BEGINNING and END tokens.\n",
        "        \"\"\"\n",
        "        # Tokenize the text\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "        token_ids = [self.vocab_builder.get_token_id(token) for token in tokens]\n",
        "        modified_tokens = [0] # Add 1 BEGINNING\n",
        "        modified_tokens.extend(token_ids)\n",
        "        modified_tokens.append(1) # Add 1 END\n",
        "\n",
        "        return modified_tokens\n",
        "\n",
        "    def create_training_sequences(self, text):\n",
        "        \"\"\"\n",
        "        Creates training sequences from a single string by generating sequences of length N+1.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to create sequences from.\n",
        "\n",
        "        Returns:\n",
        "        - List[Tuple[List[int], int]]: A list of (context, target) pairs.\n",
        "        \"\"\"\n",
        "        encoded_text = self.encode_text(text)\n",
        "\n",
        "        # Taken from: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
        "        training_sequences = [encoded_text[i * self.max:(i + 1) * self.max] for i in range((len(encoded_text) + self.max - 1) // self.max )]\n",
        "\n",
        "        return training_sequences\n"
      ],
      "metadata": {
        "id": "uXLrr6YeF0AF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "8VLcOGFvb-pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting\n",
        "preparer = TrainingDataPreparerRNN(vocab_builder, max_sequence_length=128)\n",
        "\n",
        "training_sequences = []\n",
        "split_training_set = list(filter(''.__ne__, training_set.splitlines())) # Split and remove empty lines\n",
        "for paragraph in tqdm(split_training_set):\n",
        "  training_sequences.append(preparer.create_training_sequences(paragraph))\n",
        "flattened_training_sequences =  [\n",
        "    x\n",
        "    for xs in training_sequences\n",
        "    for x in xs\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOhqoltJEEYu",
        "outputId": "2708e8f8-4abe-48cd-9b9a-2826726a989c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 147059/147059 [01:26<00:00, 1703.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare validation data also\n",
        "val_sequences = []\n",
        "split_val_set = list(filter(''.__ne__, val_set.splitlines())) # Split and remove empty lines\n",
        "for paragraph in tqdm(split_val_set):\n",
        "  val_sequences.append(preparer.create_training_sequences(paragraph))\n",
        "flattened_val_sequences =  [\n",
        "    x\n",
        "    for xs in val_sequences\n",
        "    for x in xs\n",
        "]"
      ],
      "metadata": {
        "id": "2iMBAWK0VWxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5eca7f-4fa0-411b-8469-9015b4166076"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17874/17874 [00:10<00:00, 1710.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for context in flattened_training_sequences[:10]:  # Show the first few sequences\n",
        "    print([vocab_builder.get_token_str(id) for id in context])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWozIyMKLkhs",
        "outputId": "eacbb91f-3e34-4288-e4ec-85d183a07560"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BEGINNING', 'anatomy', 'END']\n",
            "['BEGINNING', 'anatomy', '(', 'greek', 'UNKNOWN', ',', '“', 'dissection', '”', ')', 'is', 'the', 'branch', 'of', 'biology', 'concerned', 'with', 'the', 'study', 'of', 'the', 'structure', 'of', 'organisms', 'and', 'their', 'parts', '.', 'anatomy', 'is', 'a', 'branch', 'of', 'natural', 'science', 'dealing', 'with', 'the', 'structural', 'organization', 'of', 'living', 'things', '.', 'it', 'is', 'an', 'old', 'science', ',', 'having', 'its', 'beginnings', 'in', 'prehistoric', 'times', '.', 'anatomy', 'is', 'inherently', 'tied', 'to', 'UNKNOWN', ',', 'comparative', 'anatomy', ',', 'evolutionary', 'biology', ',', 'and', 'phylogeny', ',', 'as', 'these', 'are', 'the', 'processes', 'by', 'which', 'anatomy', 'is', 'generated', 'over', 'immediate', '(', 'UNKNOWN', ')', 'and', 'long', '(', 'evolution', ')', 'UNKNOWN', '.', 'human', 'anatomy', 'is', 'one', 'of', 'the', 'basic', 'essential', 'sciences', 'of', 'medicine', '.', 'END']\n",
            "['BEGINNING', 'the', 'discipline', 'of', 'anatomy', 'is', 'divided', 'into', 'macroscopic', 'and', 'microscopic', 'anatomy', '.', 'macroscopic', 'anatomy', ',', 'or', 'gross', 'anatomy', ',', 'is', 'the', 'examination', 'of', 'an', 'animal', \"'s\", 'body', 'parts', 'using', 'UNKNOWN', 'UNKNOWN', '.', 'gross', 'anatomy', 'also', 'includes', 'the', 'branch', 'of', 'superficial', 'anatomy', '.', 'microscopic', 'anatomy', 'involves', 'the', 'use', 'of', 'optical', 'instruments', 'in', 'the', 'study', 'of', 'the', 'tissues', 'of', 'various', 'structures', ',', 'known', 'as', 'UNKNOWN', ',', 'and', 'also', 'in', 'the', 'study', 'of', 'cells', '.', 'END']\n",
            "['BEGINNING', 'the', 'history', 'of', 'anatomy', 'is', 'characterized', 'by', 'a', 'progressive', 'understanding', 'of', 'the', 'functions', 'of', 'the', 'organs', 'and', 'structures', 'of', 'the', 'human', 'body', '.', 'methods', 'have', 'also', 'improved', 'dramatically', ',', 'advancing', 'from', 'the', 'examination', 'of', 'animals', 'by', 'dissection', 'of', 'UNKNOWN', 'and', 'UNKNOWN', '(', 'UNKNOWN', ')', 'to', '20th', 'century', 'medical', 'imaging', 'techniques', 'including', 'x-ray', ',', 'ultrasound', ',', 'and', 'magnetic', 'resonance', 'imaging', '.', 'END']\n",
            "['BEGINNING', 'anatomy', 'and', 'physiology', ',', 'which', 'study', '(', 'respectively', ')', 'the', 'structure', 'and', 'function', 'of', 'organisms', 'and', 'their', 'parts', ',', 'make', 'a', 'natural', 'pair', 'of', 'related', 'disciplines', ',', 'and', 'they', 'are', 'often', 'studied', 'together', '.', 'END']\n",
            "['BEGINNING', 'derived', 'from', 'the', 'greek', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'dissection', \"''\", '(', 'from', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'i', 'cut', 'up', ',', 'cut', 'open', \"''\", 'from', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'up', \"''\", ',', 'and', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'i', 'cut', \"''\", ')', ',', 'anatomy', 'is', 'the', 'scientific', 'study', 'of', 'the', 'structure', 'of', 'organisms', 'including', 'their', 'systems', ',', 'organs', 'and', 'tissues', '.', 'it', 'includes', 'the', 'appearance', 'and', 'position', 'of', 'the', 'various', 'parts', ',', 'the', 'materials', 'from', 'which', 'they', 'are', 'composed', ',', 'their', 'locations', 'and', 'their', 'relationships', 'with', 'other', 'parts', '.', 'anatomy', 'is', 'quite', 'distinct', 'from', 'physiology', 'and', 'biochemistry', ',', 'which', 'deal', 'respectively', 'with', 'the', 'functions', 'of', 'those', 'parts', 'and', 'the', 'chemical', 'processes', 'involved', '.', 'for', 'example', ',', 'an', 'UNKNOWN', 'is', 'concerned', 'with', 'the', 'shape', ',', 'size']\n",
            "[',', 'position', ',', 'structure', ',', 'blood', 'supply', 'and', 'UNKNOWN', 'of', 'an', 'organ', 'such', 'as', 'the', 'liver', ';', 'while', 'a', 'UNKNOWN', 'is', 'interested', 'in', 'the', 'production', 'of', 'bile', ',', 'the', 'role', 'of', 'the', 'liver', 'in', 'nutrition', 'and', 'the', 'regulation', 'of', 'bodily', 'functions', '.', 'END']\n",
            "['BEGINNING', 'the', 'discipline', 'of', 'anatomy', 'can', 'be', 'subdivided', 'into', 'a', 'number', 'of', 'branches', 'including', 'gross', 'or', 'macroscopic', 'anatomy', 'and', 'microscopic', 'anatomy', '.', 'gross', 'anatomy', 'is', 'the', 'study', 'of', 'structures', 'large', 'enough', 'to', 'be', 'seen', 'with', 'the', 'naked', 'eye', ',', 'and', 'also', 'includes', 'superficial', 'anatomy', 'or', 'surface', 'anatomy', ',', 'the', 'study', 'by', 'sight', 'of', 'the', 'external', 'body', 'features', '.', 'microscopic', 'anatomy', 'is', 'the', 'study', 'of', 'structures', 'on', 'a', 'microscopic', 'scale', ',', 'including', 'UNKNOWN', '(', 'the', 'study', 'of', 'tissues', ')', ',', 'and', 'UNKNOWN', '(', 'the', 'study', 'of', 'an', 'organism', 'in', 'its', 'UNKNOWN', 'condition', ')', '.', 'END']\n",
            "['BEGINNING', 'anatomy', 'can', 'be', 'studied', 'using', 'both', 'invasive', 'and', 'UNKNOWN', 'methods', 'with', 'the', 'goal', 'of', 'obtaining', 'information', 'about', 'the', 'structure', 'and', 'organization', 'of', 'organs', 'and', 'systems', '.', 'methods', 'used', 'include', 'dissection', ',', 'in', 'which', 'a', 'body', 'is', 'opened', 'and', 'its', 'organs', 'studied', ',', 'and', 'UNKNOWN', ',', 'in', 'which', 'a', 'video', 'UNKNOWN', 'instrument', 'is', 'inserted', 'through', 'a', 'small', 'UNKNOWN', 'in', 'the', 'body', 'wall', 'and', 'used', 'to', 'explore', 'the', 'internal', 'organs', 'and', 'other', 'structures', '.', 'UNKNOWN', 'using', 'x-rays', 'or', 'magnetic', 'resonance', 'UNKNOWN', 'are', 'methods', 'to', 'UNKNOWN', 'blood', 'vessels', '.', 'END']\n",
            "['BEGINNING', 'the', 'term', '``', 'anatomy', \"''\", 'is', 'commonly', 'taken', 'to', 'refer', 'to', 'human', 'anatomy', '.', 'however', ',', 'substantially', 'the', 'same', 'structures', 'and', 'tissues', 'are', 'found', 'throughout', 'the', 'rest', 'of', 'the', 'animal', 'kingdom', 'and', 'the', 'term', 'also', 'includes', 'the', 'anatomy', 'of', 'other', 'animals', '.', 'the', 'term', '``', 'UNKNOWN', \"''\", 'is', 'also', 'sometimes', 'used', 'to', 'specifically', 'refer', 'to', 'animals', '.', 'the', 'structure', 'and', 'tissues', 'of', 'plants', 'are', 'of', 'a', 'UNKNOWN', 'nature', 'and', 'they', 'are', 'studied', 'in', 'plant', 'anatomy', '.', 'END']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check nr. 2\n",
        "print(len(flattened_training_sequences))\n",
        "print(len(flattened_val_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ECPe-uLMYy8",
        "outputId": "b423139e-1176-4286-cda5-34b8b22d6205"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179387\n",
            "22232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapted batcher\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def TorchDataLoaderRNN(training_sequences, batch_size):\n",
        "  # Find longest length in sequence\n",
        "  longest = len(max(training_sequences, key=len)) # Should never exceed max_sequence_length\n",
        "\n",
        "  # Padding\n",
        "  padded_sequences = [sequence +([3] * (longest - len(sequence))) for sequence in training_sequences] # PADDING has integer code 3\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  context_tensor = torch.tensor(padded_sequences, dtype=torch.long)  # Shape: (num_samples, 3)\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(context_tensor)\n",
        "\n",
        "  # Create a DataLoader for batching\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "DqKicZuza81v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = TorchDataLoaderRNN(flattened_training_sequences, 64)"
      ],
      "metadata": {
        "id": "5nNMuktjU1IC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valloader = TorchDataLoaderRNN(flattened_val_sequences, 64)"
      ],
      "metadata": {
        "id": "9FpFFCWqVl2j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for batch_context in trainloader:\n",
        "    print(batch_context[0])\n",
        "    print(batch_context[0].shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD0wfSt5OVvX",
        "outputId": "73816b58-8ecf-41fb-f484-3b2781e4650c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    4,   175,     6,  ...,     3,     3,     3],\n",
            "        [    0,    35,  6315,  ...,     3,     3,     3],\n",
            "        [    0,    32,   354,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    0,     4,  1112,  ...,    38,   212,   101],\n",
            "        [    0, 16347,  5158,  ...,     3,     3,     3],\n",
            "        [    0,  4661,    40,  ...,     3,     3,     3]])\n",
            "torch.Size([64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: RNN model"
      ],
      "metadata": {
        "id": "6cxG6iHlclgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# EarlyStopping class remains the same\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
        "        self.patience = patience  # Number of epochs to wait for improvement\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.path = path  # Path to save the best model\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss < self.best_score - self.delta:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Save model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, hidden_dim, vocab_size, embed_size, activation=nn.ReLU,last_layer_activation=nn.Softmax,dropout=0.05):\n",
        "\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embed_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_dim, num_layers=num_layers,\n",
        "                    dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        output, hidden = self.lstm(embedding)\n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc(output)\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "Q2CkGhOV1JpZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "model = RNN(num_layers=2, hidden_dim=1024, vocab_size=16384, embed_size=128)\n",
        "test_input = torch.tensor([0, 6 , 8 , 10, 15, 1])\n",
        "output = model(test_input)\n",
        "print(output.shape)\n",
        "\n",
        "test_input = torch.tensor([0, 7 , 7 , 32, 32, 18, 99, 500, 12, 1])\n",
        "output = model(test_input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU2JmLeTg2YR",
        "outputId": "abff8b67-91ae-47ca-d277-133de80e8d37"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 16384])\n",
            "torch.Size([10, 16384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(num_layers=2, hidden_dim=1024, vocab_size=16384, embed_size=128)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=3) # Ignore padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "patience = 5\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "number_of_epochs = 10\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch_context in tqdm(trainloader):\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context[0][:,:-1]\n",
        "        Y = batch_context[0][:,1:]\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        logits = model(X)  # Model output for X\n",
        "        targets = Y.view(-1)                      # 2-dimensional -> 1-dimensional\n",
        "        logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -> 2-dimensional\n",
        "        loss = criterion(logits, targets) # Compute the loss between model output and Y\n",
        "\n",
        "        #BACKWARD PASS (updating the model parameters):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{number_of_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradient computation for validation\n",
        "        for batch_context in valloader:\n",
        "        #FORWARD PASS:\n",
        "          X = batch_context[0][:,:-1]\n",
        "          Y = batch_context[0][:,1:]\n",
        "          X, Y = X.to(device), Y.to(device)\n",
        "          logits = model(X)  # Model output for X\n",
        "          targets = Y.view(-1)                      # 2-dimensional -> 1-dimensional\n",
        "          logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -> 2-dimensional\n",
        "          loss = criterion(logits, targets) # Compute the loss between model output and Y\n",
        "          val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
        "    print(f\"Epoch {epoch+1}/{number_of_epochs} - Perplexity: {np.exp(avg_val_loss):.6f}\")\n",
        "\n",
        "    # Call early stopping after each epoch\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, load the best model after training\n",
        "model.load_state_dict(torch.load('checkpoint.pth'))"
      ],
      "metadata": {
        "id": "7iUtwI8nbBO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880cc3d4-f8ba-4790-f3fd-26e10666e5b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [20:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.8157\n",
            "Epoch 1/10 - Perplexity: 123.971015\n",
            "Validation loss decreased (inf --> 4.820048).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:06<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 4.5877\n",
            "Epoch 2/10 - Perplexity: 92.578977\n",
            "Validation loss decreased (4.820048 --> 4.528062).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:10<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 4.3119\n",
            "Epoch 3/10 - Perplexity: 80.842643\n",
            "Validation loss decreased (4.528062 --> 4.392505).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:09<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 4.1769\n",
            "Epoch 4/10 - Perplexity: 75.559984\n",
            "Validation loss decreased (4.392505 --> 4.324927).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:10<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 4.0304\n",
            "Epoch 5/10 - Perplexity: 72.764355\n",
            "Validation loss decreased (4.324927 --> 4.287226).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:10<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 3.9923\n",
            "Epoch 6/10 - Perplexity: 71.632367\n",
            "Validation loss decreased (4.287226 --> 4.271547).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:12<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 3.7958\n",
            "Epoch 7/10 - Perplexity: 71.081673\n",
            "Validation loss decreased (4.271547 --> 4.263830).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:12<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 3.8374\n",
            "Epoch 8/10 - Perplexity: 71.249013\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:13<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 3.8346\n",
            "Epoch 9/10 - Perplexity: 71.963848\n",
            "EarlyStopping counter: 2 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2803/2803 [21:12<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 3.7444\n",
            "Epoch 10/10 - Perplexity: 72.932845\n",
            "EarlyStopping counter: 3 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-a5c91e3d1f90>:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('checkpoint.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 generating text"
      ],
      "metadata": {
        "id": "Hsm3CLoWgub5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular test sentence using argmax"
      ],
      "metadata": {
        "id": "XfYGnWtmZ3Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"he lives in san\"\n",
        "\n",
        "encoded_sentence = [vocab_builder.get_token_id(word) for word in test_sentence.split(\" \")]\n",
        "\n",
        "output = model(torch.tensor(encoded_sentence).to(device))\n",
        "\n",
        "# Predict\n",
        "prediction = torch.argmax(output[-1])\n",
        "\n",
        "print(vocab_builder.get_token_str(prediction.item()))"
      ],
      "metadata": {
        "id": "HOp6TdEPgtmJ",
        "outputId": "55d9353b-e091-48a7-e982-237e5882811c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "francisco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random algorithm"
      ],
      "metadata": {
        "id": "GobxeiM2UW-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "def random_sampling(model, prompt, max_length, temperature, topk):\n",
        "    # First, encode the input\n",
        "    encoded_prompt = [vocab_builder.get_token_id(word) for word in prompt.split(\" \")]\n",
        "\n",
        "    logits = model(torch.tensor(encoded_prompt).to(device))\n",
        "\n",
        "    # Apply temperature\n",
        "    softmax = torch.nn.Softmax()\n",
        "    tempered_logits = softmax(logits / temperature)\n",
        "\n",
        "    # Apply topk\n",
        "    # From https://gist.github.com/bsantraigi/5752667525d88d375207f099bd78818b\n",
        "    indices_to_remove = logits < torch.topk(logits, topk, dim=1)[0][..., -1, None]\n",
        "    tempered_logits[indices_to_remove] = -np.Inf\n",
        "\n",
        "    # Sample from the distribution\n",
        "    distribution = Categorical(logits=tempered_logits)\n",
        "    prediction = distribution.sample()\n",
        "\n",
        "    encoded_prompt.append(prediction[-1].item())\n",
        "\n",
        "    end_of_sentence = (prediction[-1].item() == 1)\n",
        "    words_generated = 1\n",
        "\n",
        "    # Repeat with its own outputs:\n",
        "    while (words_generated < max_length) and not end_of_sentence:\n",
        "\n",
        "        # The logits\n",
        "        logits = model(torch.tensor(encoded_prompt).to(device))\n",
        "\n",
        "        # Apply temperature\n",
        "        tempered_logits = softmax(logits / temperature)\n",
        "\n",
        "        # Apply topk\n",
        "        indices_to_remove = logits < torch.topk(logits, topk, dim=1)[0][..., -1, None]\n",
        "        tempered_logits[indices_to_remove] = -np.Inf\n",
        "\n",
        "        # Sample from the distribution\n",
        "        distribution = Categorical(logits=tempered_logits)\n",
        "        prediction = distribution.sample()\n",
        "        encoded_prompt.append(prediction[-1].item())\n",
        "\n",
        "        # Check if end of sentence and update word counter\n",
        "        if (prediction[-1].item() == 1):\n",
        "            end_of_sentence = True\n",
        "        words_generated += 1\n",
        "    return [vocab_builder.get_token_str(word) for word in encoded_prompt]\n",
        "\n",
        "# Test it\n",
        "print(random_sampling(model, \"he lives in san\", 1, 1, 1)) # Sanity check\n",
        "print(random_sampling(model, \"he lives in san\", 1, 0.000001, 10)) # Sanity check\n",
        "print(random_sampling(model, \"he lives in san\", 30, 0.5, 5))\n",
        "print(random_sampling(model, \"which is very\", 30, 1, 5))\n",
        "print(random_sampling(model, \"which is very\", 30, 2, 10))\n",
        "print(random_sampling(model, \"and here is another interesting fact about\", 30, 0.5, 10))\n",
        "print(random_sampling(model, \"and here is another interesting fact about\", 30, 0.5, 3))\n",
        "print(random_sampling(model, \"and here is another interesting fact about\", 30, 2, 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKs4_tg1T3C6",
        "outputId": "65e3337a-aa59-44d5-e732-74beb99e246f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'lives', 'in', 'san', 'francisco']\n",
            "['he', 'lives', 'in', 'san', 'francisco']\n",
            "['he', 'lives', 'in', 'san', 'francisco', '.', 'in', 'the', 'same', 'month', ',', 'UNKNOWN', \"'s\", 'father', 'died', 'in', 'an', 'accident', '.', 'he', 'also', 'UNKNOWN', 'a', 'new', 'family', 'for', 'the', 'family', '.', 'END']\n",
            "['which', 'is', 'very', 'close', '.', 'in', 'the', 'past', ',', 'UNKNOWN', 'is', 'UNKNOWN', '.', 'he', 'was', 'also', 'a', 'member', 'who', 'had', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', ',', 'and', 'his', 'wife', 'was', 'murdered', 'by', 'the', 'UNKNOWN', ',']\n",
            "['which', 'is', 'very', 'well', 'received', 'by', 'its', 'use', ',', 'but', 'rather', 'the', 'UNKNOWN', '.', 'this', 'allows', 'them', 'with', 'a', 'higher', 'chance', 'to', 'have', 'to', 'have', 'any', 'more', 'effect', ',', 'such', 'UNKNOWN', '.', 'a']\n",
            "['and', 'here', 'is', 'another', 'interesting', 'fact', 'about', 'that', ',', 'in', 'order', 'to', 'prove', 'what', '``', 'a', 'certain', 'part', 'or', 'the', 'UNKNOWN', 'of', 'the', 'soul', 'and', 'an', 'event', 'on', 'which', 'one', \"'s\", 'life', 'is', '.', 'UNKNOWN', \"''\", ';']\n",
            "['and', 'here', 'is', 'another', 'interesting', 'fact', 'about', 'the', 'nature', 'of', 'the', 'UNKNOWN', ',', 'and', 'it', 'is', 'the', 'only', 'one', 'that', 'is', 'UNKNOWN', 'in', 'the', 'UNKNOWN', '.', 'the', '``', 'three', 'UNKNOWN', \"''\", 'are', 'the', 'two', 'UNKNOWN', 'UNKNOWN', ',']\n",
            "['and', 'here', 'is', 'another', 'interesting', 'fact', 'about', 'how', 'as', 'for', 'you', 'actually', 'begin', 'against', 'this', '[', 'he', 'in', 'albania', '’', 'john', 'von', 'wells', 'visited', 'or', 'near', 'it', 'when', 'only', 'from', 'somewhere', 'down', 'so', 'is', 'even', 'better', 'him']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}