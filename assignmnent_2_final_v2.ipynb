{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ash16YV-b6T5"
   },
   "source": [
    "# Step 0: Preparations and code from Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWJbmNgzBECe"
   },
   "source": [
    "Group 17: Jakob Svensson, Mahdi Afarideh, Maximilian Forsell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMXuQ-VlzuRv",
    "outputId": "82f36801-7c09-4cd8-de2b-b13cd6e5a7db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] Det går inte att hitta filen: 'Intro-to-language-modeling'\n",
      "c:\\Users\\ANv\\Desktop\\Chalmers\\NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "C:\\Users\\ANv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
    "%cd Intro-to-language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5fv9gQcVafW3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLeT6K6EDID5",
    "outputId": "af8c8fc3-a349-453d-fc7d-20cdc9a2a5a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qjj4IdOi08ms"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=2024):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEDybdl906rv",
    "outputId": "49e5fc2f-47f0-491c-e56e-5160c57d6c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IE8oAx8b3AWX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "dataset='lmdemo'\n",
    "zip_file = f\"{dataset}.zip\"\n",
    "!unzip -q $zip_file\n",
    "!rm $zip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "clFRaGPQ4Jc-"
   },
   "outputs": [],
   "source": [
    "training_set=open(f'{dataset}/train.txt','r',encoding='utf-8').read()\n",
    "val_set=open(f'{dataset}/val.txt','r',encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CZJ4k7STz96H"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class VocabularyBuilder:\n",
    "    def __init__(self, max_voc_size):\n",
    "        self.max_voc_size = max_voc_size\n",
    "        self.str_to_int = {}\n",
    "        self.int_to_str = {}\n",
    "        self.special_tokens = [\"BEGINNING\", \"END\", \"UNKNOWN\", \"PADDING\"] #Added padding\n",
    "        self.token_counter = Counter()\n",
    "\n",
    "    def build_vocabulary(self, text):\n",
    "\n",
    "        sents=nltk.word_tokenize(text.lower())\n",
    "\n",
    "        for token in sents:\n",
    "            self.token_counter[token] += 1\n",
    "\n",
    "    def create_vocabulary(self):\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.str_to_int[token] = idx\n",
    "            self.int_to_str[idx] = token\n",
    "\n",
    "        max_words = self.max_voc_size - len(self.special_tokens)\n",
    "        most_common_tokens = self.token_counter.most_common(max_words)\n",
    "\n",
    "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
    "            self.str_to_int[token] = idx\n",
    "            self.int_to_str[idx] = token\n",
    "\n",
    "    def create_premade_vocabulary(self, c):\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.str_to_int[token] = idx\n",
    "            self.int_to_str[idx] = token\n",
    "\n",
    "        max_words = self.max_voc_size - len(self.special_tokens)\n",
    "        most_common_tokens = c.most_common(max_words) # Here we can use a premade counter from a previous run\n",
    "\n",
    "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
    "            self.str_to_int[token] = idx\n",
    "            self.int_to_str[idx] = token\n",
    "\n",
    "    def get_token_id(self, token):\n",
    "        return self.str_to_int.get(token.lower(), self.str_to_int[\"UNKNOWN\"])\n",
    "\n",
    "    def get_token_str(self, token_id):\n",
    "        return self.int_to_str.get(token_id, \"UNKNOWN\")\n",
    "\n",
    "    def sanity_check(self): # Here we run the sanity tests recommended in the assignment\n",
    "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
    "\n",
    "        for token in self.special_tokens:\n",
    "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
    "\n",
    "        common_words = [\"the\", \"and\"]\n",
    "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
    "\n",
    "        for word in common_words:\n",
    "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
    "\n",
    "        for word in rare_words:\n",
    "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
    "\n",
    "        test_word = \"the\"\n",
    "        token_id = self.get_token_id(test_word)\n",
    "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
    "\n",
    "        print(\"Sanity check passed!\")\n",
    "\n",
    "vocab_builder = VocabularyBuilder(max_voc_size=16384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1LhuLzfE2eb",
    "outputId": "2a80e0da-135d-4111-ed59-31b729b4ee2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294118/294118 [00:45<00:00, 6473.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for paragraph in tqdm(training_set.splitlines()):\n",
    "  vocab_builder.build_vocabulary(paragraph)\n",
    "vocab_builder.create_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1sB4W8Q0Qo9",
    "outputId": "d9872069-2b24-402b-da6b-f7fd6f80dda0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "# Perform sanity check\n",
    "vocab_builder.sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uXLrr6YeF0AF"
   },
   "outputs": [],
   "source": [
    "# Modified for assignment 2\n",
    "class TrainingDataPreparerRNN:\n",
    "    def __init__(self, vocab_builder, max_sequence_length):\n",
    "        self.vocab_builder = vocab_builder\n",
    "        self.max = max_sequence_length\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Tokenizes and encodes a single string with special symbols.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input string to encode.\n",
    "\n",
    "        Returns:\n",
    "        - List[int]: A list of token IDs including BEGINNING and END tokens.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        token_ids = [self.vocab_builder.get_token_id(token) for token in tokens]\n",
    "        modified_tokens = [0] # Add 1 BEGINNING\n",
    "        modified_tokens.extend(token_ids)\n",
    "        modified_tokens.append(1) # Add 1 END\n",
    "\n",
    "        return modified_tokens\n",
    "\n",
    "    def create_training_sequences(self, text):\n",
    "        \"\"\"\n",
    "        Creates training sequences from a single string by generating sequences of length N+1.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input string to create sequences from.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[List[int], int]]: A list of (context, target) pairs.\n",
    "        \"\"\"\n",
    "        encoded_text = self.encode_text(text)\n",
    "\n",
    "        # Taken from: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "        training_sequences = [encoded_text[i * self.max:(i + 1) * self.max] for i in range((len(encoded_text) + self.max - 1) // self.max )]\n",
    "\n",
    "        return training_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VLcOGFvb-pb"
   },
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOhqoltJEEYu",
    "outputId": "2708e8f8-4abe-48cd-9b9a-2826726a989c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147059/147059 [00:45<00:00, 3228.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting\n",
    "preparer = TrainingDataPreparerRNN(vocab_builder, max_sequence_length=128)\n",
    "\n",
    "training_sequences = []\n",
    "split_training_set = list(filter(''.__ne__, training_set.splitlines())) # Split and remove empty lines\n",
    "for paragraph in tqdm(split_training_set):\n",
    "  training_sequences.append(preparer.create_training_sequences(paragraph))\n",
    "flattened_training_sequences =  [\n",
    "    x\n",
    "    for xs in training_sequences\n",
    "    for x in xs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iMBAWK0VWxP",
    "outputId": "6a5eca7f-4fa0-411b-8469-9015b4166076"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17874/17874 [00:06<00:00, 2968.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare validation data also\n",
    "val_sequences = []\n",
    "split_val_set = list(filter(''.__ne__, val_set.splitlines())) # Split and remove empty lines\n",
    "for paragraph in tqdm(split_val_set):\n",
    "  val_sequences.append(preparer.create_training_sequences(paragraph))\n",
    "flattened_val_sequences =  [\n",
    "    x\n",
    "    for xs in val_sequences\n",
    "    for x in xs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWozIyMKLkhs",
    "outputId": "eacbb91f-3e34-4288-e4ec-85d183a07560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BEGINNING', 'anatomy', 'END']\n",
      "['BEGINNING', 'anatomy', '(', 'greek', 'UNKNOWN', ',', '“', 'dissection', '”', ')', 'is', 'the', 'branch', 'of', 'biology', 'concerned', 'with', 'the', 'study', 'of', 'the', 'structure', 'of', 'organisms', 'and', 'their', 'parts', '.', 'anatomy', 'is', 'a', 'branch', 'of', 'natural', 'science', 'dealing', 'with', 'the', 'structural', 'organization', 'of', 'living', 'things', '.', 'it', 'is', 'an', 'old', 'science', ',', 'having', 'its', 'beginnings', 'in', 'prehistoric', 'times', '.', 'anatomy', 'is', 'inherently', 'tied', 'to', 'UNKNOWN', ',', 'comparative', 'anatomy', ',', 'evolutionary', 'biology', ',', 'and', 'phylogeny', ',', 'as', 'these', 'are', 'the', 'processes', 'by', 'which', 'anatomy', 'is', 'generated', 'over', 'immediate', '(', 'UNKNOWN', ')', 'and', 'long', '(', 'evolution', ')', 'UNKNOWN', '.', 'human', 'anatomy', 'is', 'one', 'of', 'the', 'basic', 'essential', 'sciences', 'of', 'medicine', '.', 'END']\n",
      "['BEGINNING', 'the', 'discipline', 'of', 'anatomy', 'is', 'divided', 'into', 'macroscopic', 'and', 'microscopic', 'anatomy', '.', 'macroscopic', 'anatomy', ',', 'or', 'gross', 'anatomy', ',', 'is', 'the', 'examination', 'of', 'an', 'animal', \"'s\", 'body', 'parts', 'using', 'UNKNOWN', 'UNKNOWN', '.', 'gross', 'anatomy', 'also', 'includes', 'the', 'branch', 'of', 'superficial', 'anatomy', '.', 'microscopic', 'anatomy', 'involves', 'the', 'use', 'of', 'optical', 'instruments', 'in', 'the', 'study', 'of', 'the', 'tissues', 'of', 'various', 'structures', ',', 'known', 'as', 'UNKNOWN', ',', 'and', 'also', 'in', 'the', 'study', 'of', 'cells', '.', 'END']\n",
      "['BEGINNING', 'the', 'history', 'of', 'anatomy', 'is', 'characterized', 'by', 'a', 'progressive', 'understanding', 'of', 'the', 'functions', 'of', 'the', 'organs', 'and', 'structures', 'of', 'the', 'human', 'body', '.', 'methods', 'have', 'also', 'improved', 'dramatically', ',', 'advancing', 'from', 'the', 'examination', 'of', 'animals', 'by', 'dissection', 'of', 'UNKNOWN', 'and', 'UNKNOWN', '(', 'UNKNOWN', ')', 'to', '20th', 'century', 'medical', 'imaging', 'techniques', 'including', 'x-ray', ',', 'ultrasound', ',', 'and', 'magnetic', 'resonance', 'imaging', '.', 'END']\n",
      "['BEGINNING', 'anatomy', 'and', 'physiology', ',', 'which', 'study', '(', 'respectively', ')', 'the', 'structure', 'and', 'function', 'of', 'organisms', 'and', 'their', 'parts', ',', 'make', 'a', 'natural', 'pair', 'of', 'related', 'disciplines', ',', 'and', 'they', 'are', 'often', 'studied', 'together', '.', 'END']\n",
      "['BEGINNING', 'derived', 'from', 'the', 'greek', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'dissection', \"''\", '(', 'from', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'i', 'cut', 'up', ',', 'cut', 'open', \"''\", 'from', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'up', \"''\", ',', 'and', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'i', 'cut', \"''\", ')', ',', 'anatomy', 'is', 'the', 'scientific', 'study', 'of', 'the', 'structure', 'of', 'organisms', 'including', 'their', 'systems', ',', 'organs', 'and', 'tissues', '.', 'it', 'includes', 'the', 'appearance', 'and', 'position', 'of', 'the', 'various', 'parts', ',', 'the', 'materials', 'from', 'which', 'they', 'are', 'composed', ',', 'their', 'locations', 'and', 'their', 'relationships', 'with', 'other', 'parts', '.', 'anatomy', 'is', 'quite', 'distinct', 'from', 'physiology', 'and', 'biochemistry', ',', 'which', 'deal', 'respectively', 'with', 'the', 'functions', 'of', 'those', 'parts', 'and', 'the', 'chemical', 'processes', 'involved', '.', 'for', 'example', ',', 'an', 'UNKNOWN', 'is', 'concerned', 'with', 'the', 'shape', ',', 'size']\n",
      "[',', 'position', ',', 'structure', ',', 'blood', 'supply', 'and', 'UNKNOWN', 'of', 'an', 'organ', 'such', 'as', 'the', 'liver', ';', 'while', 'a', 'UNKNOWN', 'is', 'interested', 'in', 'the', 'production', 'of', 'bile', ',', 'the', 'role', 'of', 'the', 'liver', 'in', 'nutrition', 'and', 'the', 'regulation', 'of', 'bodily', 'functions', '.', 'END']\n",
      "['BEGINNING', 'the', 'discipline', 'of', 'anatomy', 'can', 'be', 'subdivided', 'into', 'a', 'number', 'of', 'branches', 'including', 'gross', 'or', 'macroscopic', 'anatomy', 'and', 'microscopic', 'anatomy', '.', 'gross', 'anatomy', 'is', 'the', 'study', 'of', 'structures', 'large', 'enough', 'to', 'be', 'seen', 'with', 'the', 'naked', 'eye', ',', 'and', 'also', 'includes', 'superficial', 'anatomy', 'or', 'surface', 'anatomy', ',', 'the', 'study', 'by', 'sight', 'of', 'the', 'external', 'body', 'features', '.', 'microscopic', 'anatomy', 'is', 'the', 'study', 'of', 'structures', 'on', 'a', 'microscopic', 'scale', ',', 'including', 'UNKNOWN', '(', 'the', 'study', 'of', 'tissues', ')', ',', 'and', 'UNKNOWN', '(', 'the', 'study', 'of', 'an', 'organism', 'in', 'its', 'UNKNOWN', 'condition', ')', '.', 'END']\n",
      "['BEGINNING', 'anatomy', 'can', 'be', 'studied', 'using', 'both', 'invasive', 'and', 'UNKNOWN', 'methods', 'with', 'the', 'goal', 'of', 'obtaining', 'information', 'about', 'the', 'structure', 'and', 'organization', 'of', 'organs', 'and', 'systems', '.', 'methods', 'used', 'include', 'dissection', ',', 'in', 'which', 'a', 'body', 'is', 'opened', 'and', 'its', 'organs', 'studied', ',', 'and', 'UNKNOWN', ',', 'in', 'which', 'a', 'video', 'UNKNOWN', 'instrument', 'is', 'inserted', 'through', 'a', 'small', 'UNKNOWN', 'in', 'the', 'body', 'wall', 'and', 'used', 'to', 'explore', 'the', 'internal', 'organs', 'and', 'other', 'structures', '.', 'UNKNOWN', 'using', 'x-rays', 'or', 'magnetic', 'resonance', 'UNKNOWN', 'are', 'methods', 'to', 'UNKNOWN', 'blood', 'vessels', '.', 'END']\n",
      "['BEGINNING', 'the', 'term', '``', 'anatomy', \"''\", 'is', 'commonly', 'taken', 'to', 'refer', 'to', 'human', 'anatomy', '.', 'however', ',', 'substantially', 'the', 'same', 'structures', 'and', 'tissues', 'are', 'found', 'throughout', 'the', 'rest', 'of', 'the', 'animal', 'kingdom', 'and', 'the', 'term', 'also', 'includes', 'the', 'anatomy', 'of', 'other', 'animals', '.', 'the', 'term', '``', 'UNKNOWN', \"''\", 'is', 'also', 'sometimes', 'used', 'to', 'specifically', 'refer', 'to', 'animals', '.', 'the', 'structure', 'and', 'tissues', 'of', 'plants', 'are', 'of', 'a', 'UNKNOWN', 'nature', 'and', 'they', 'are', 'studied', 'in', 'plant', 'anatomy', '.', 'END']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for context in flattened_training_sequences[:10]:  # Show the first few sequences\n",
    "    print([vocab_builder.get_token_str(id) for id in context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ECPe-uLMYy8",
    "outputId": "b423139e-1176-4286-cda5-34b8b22d6205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179387\n",
      "22232\n"
     ]
    }
   ],
   "source": [
    "# Sanity check nr. 2\n",
    "print(len(flattened_training_sequences))\n",
    "print(len(flattened_val_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DqKicZuza81v"
   },
   "outputs": [],
   "source": [
    "#Adapted batcher\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def TorchDataLoaderRNN(training_sequences, batch_size):\n",
    "  # Find longest length in sequence\n",
    "  longest = len(max(training_sequences, key=len)) # Should never exceed max_sequence_length\n",
    "\n",
    "  # Padding\n",
    "  padded_sequences = [sequence +([3] * (longest - len(sequence))) for sequence in training_sequences] # PADDING has integer code 3\n",
    "\n",
    "  # Convert lists to tensors\n",
    "  context_tensor = torch.tensor(padded_sequences, dtype=torch.long)  # Shape: (num_samples, 3)\n",
    "\n",
    "  # Create a TensorDataset\n",
    "  dataset = TensorDataset(context_tensor)\n",
    "\n",
    "  # Create a DataLoader for batching\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5nNMuktjU1IC"
   },
   "outputs": [],
   "source": [
    "trainloader = TorchDataLoaderRNN(flattened_training_sequences, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9FpFFCWqVl2j"
   },
   "outputs": [],
   "source": [
    "valloader = TorchDataLoaderRNN(flattened_val_sequences, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nD0wfSt5OVvX",
    "outputId": "73816b58-8ecf-41fb-f484-3b2781e4650c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    4,   175,     6,  ...,     3,     3,     3],\n",
      "        [    0,    35,  6315,  ...,     3,     3,     3],\n",
      "        [    0,    32,   354,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    0,     4,  1112,  ...,    38,   212,   101],\n",
      "        [    0, 16347,  5158,  ...,     3,     3,     3],\n",
      "        [    0,  4661,    40,  ...,     3,     3,     3]])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for batch_context in trainloader:\n",
    "    print(batch_context[0])\n",
    "    print(batch_context[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cxG6iHlclgq"
   },
   "source": [
    "# Step 2: RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q2CkGhOV1JpZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# EarlyStopping class remains the same\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
    "        self.patience = patience  # Number of epochs to wait for improvement\n",
    "        self.delta = delta  # Minimum change to qualify as an improvement\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.path = path  # Path to save the best model\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss < self.best_score - self.delta:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, hidden_dim, vocab_size, embed_size, activation=nn.ReLU,last_layer_activation=nn.Softmax,dropout=0.05):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embed_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.lstm(embedding)\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU2JmLeTg2YR",
    "outputId": "abff8b67-91ae-47ca-d277-133de80e8d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 16384])\n",
      "torch.Size([10, 16384])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "model = RNN(num_layers=2, hidden_dim=1024, vocab_size=16384, embed_size=128)\n",
    "test_input = torch.tensor([0, 6 , 8 , 10, 15, 1])\n",
    "output = model(test_input)\n",
    "print(output.shape)\n",
    "\n",
    "test_input = torch.tensor([0, 7 , 7 , 32, 32, 18, 99, 500, 12, 1])\n",
    "output = model(test_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iUtwI8nbBO-",
    "outputId": "880cc3d4-f8ba-4790-f3fd-26e10666e5b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [12:55<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 5.1086\n",
      "Epoch 1/50 - Perplexity: 136.535683\n",
      "Validation loss decreased (inf --> 4.916586).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [12:43<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 4.6352\n",
      "Epoch 2/50 - Perplexity: 98.893685\n",
      "Validation loss decreased (4.916586 --> 4.594045).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:04<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 4.3473\n",
      "Epoch 3/50 - Perplexity: 85.315011\n",
      "Validation loss decreased (4.594045 --> 4.446350).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 4.3956\n",
      "Epoch 4/50 - Perplexity: 78.606981\n",
      "Validation loss decreased (4.446350 --> 4.364461).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 4.0576\n",
      "Epoch 5/50 - Perplexity: 75.114154\n",
      "Validation loss decreased (4.364461 --> 4.319009).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 4.1500\n",
      "Epoch 6/50 - Perplexity: 73.223759\n",
      "Validation loss decreased (4.319009 --> 4.293520).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 4.0517\n",
      "Epoch 7/50 - Perplexity: 72.340086\n",
      "Validation loss decreased (4.293520 --> 4.281378).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 3.8514\n",
      "Epoch 8/50 - Perplexity: 71.889069\n",
      "Validation loss decreased (4.281378 --> 4.275124).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 3.9135\n",
      "Epoch 9/50 - Perplexity: 72.459354\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 3.8704\n",
      "Epoch 10/50 - Perplexity: 73.233355\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [13:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 3.7215\n",
      "Epoch 11/50 - Perplexity: 73.735552\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [12:59<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 3.7551\n",
      "Epoch 12/50 - Perplexity: 75.189513\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2803/2803 [12:11<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 3.5707\n",
      "Epoch 13/50 - Perplexity: 76.428654\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(num_layers=2, hidden_dim=1024, vocab_size=16384, embed_size=128)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=3) # Ignore padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "patience = 5\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "number_of_epochs = 50\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch_context in tqdm(trainloader):\n",
    "        #FORWARD PASS:\n",
    "        X = batch_context[0][:,:-1]\n",
    "        Y = batch_context[0][:,1:]\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits = model(X)  # Model output for X\n",
    "        targets = Y.view(-1)                      # 2-dimensional -> 1-dimensional\n",
    "        logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -> 2-dimensional\n",
    "        loss = criterion(logits, targets) # Compute the loss between model output and Y\n",
    "\n",
    "        #BACKWARD PASS (updating the model parameters):\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update model parameters\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{number_of_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No gradient computation for validation\n",
    "        for batch_context in valloader:\n",
    "        #FORWARD PASS:\n",
    "          X = batch_context[0][:,:-1]\n",
    "          Y = batch_context[0][:,1:]\n",
    "          X, Y = X.to(device), Y.to(device)\n",
    "          logits = model(X)  # Model output for X\n",
    "          targets = Y.view(-1)                      # 2-dimensional -> 1-dimensional\n",
    "          logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -> 2-dimensional\n",
    "          loss = criterion(logits, targets) # Compute the loss between model output and Y\n",
    "          val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
    "    print(f\"Epoch {epoch+1}/{number_of_epochs} - Perplexity: {np.exp(avg_val_loss):.6f}\")\n",
    "\n",
    "    # Call early stopping after each epoch\n",
    "    early_stopping(avg_val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model after training\n",
    "model.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsm3CLoWgub5"
   },
   "source": [
    "# Step 3 generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfYGnWtmZ3Cy"
   },
   "source": [
    "Regular test sentence using argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOp6TdEPgtmJ",
    "outputId": "55d9353b-e091-48a7-e982-237e5882811c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diego\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"he lives in san\"\n",
    "\n",
    "encoded_sentence = [vocab_builder.get_token_id(word) for word in test_sentence.split(\" \")]\n",
    "\n",
    "output = model(torch.tensor(encoded_sentence).to(device))\n",
    "\n",
    "# Predict\n",
    "prediction = torch.argmax(output[-1])\n",
    "\n",
    "print(vocab_builder.get_token_str(prediction.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GobxeiM2UW-2"
   },
   "source": [
    "Random algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKs4_tg1T3C6",
    "outputId": "65e3337a-aa59-44d5-e732-74beb99e246f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'lives', 'in', 'san', 'diego']\n",
      "['he', 'lives', 'in', 'san', 'lin']\n",
      "['he', 'lives', 'in', 'san', 'juan', ',', 'massachusetts', 'on', 'december', '6', 'to', 'march', '31', ',', 'and', 'on', 'december', '30', 'of', '``', 'the', 'new', 'south', \"''\", ',', 'he', 'wrote', ':', 'a', '``', 'new', 'look', 'for', 'the']\n",
      "['which', 'is', 'very', 'rare', 'for', 'its', 'own', 'unique', ',', 'but', 'still', 'the', '``', 'albertosaurus', 'phase', '.', \"''\", 'it', 'can', 'not', 'mean', 'that', 'the', '``', 'chloroplast', \"''\", 'chloroplast', 'was', 'formed', 'by', 'two', 'nuclei', ',']\n",
      "['which', 'is', 'very', 'rare', 'as', '``', 'apatosaurus', '``', '(', 'a', 'hypothetical', 'group', 'or', 'the', 'giant', 'chloroplast', 'is', 'formed', 'into', '``', 'a', 'large', ',', 'more', 'distant', '``', 'red', 'algal', 'chloroplasts', 'of', 'dna', ')', ')']\n",
      "['and', 'here', 'is', 'another', 'interesting', 'fact', 'about', 'what', 'it', \"'s\", 'is', 'in', '``', 'f', \"'s\", ',', 'and', 'therefore', 'the', 'first', 'truth', 'to', 'make', 'our', 'first', 'true', 'knowledge', 'for', 'a', 'single', '``', 'a', 'priori', \"''\", ',', '``', 'to']\n",
      "['and', 'here', 'is', 'another', 'interesting', 'fact', 'about', 'a', 'person', \"'s\", 'life', ',', 'but', 'the', 'existence', 'of', 'the', 'demiurge', 'is', 'a', 'source', '.', 'in', 'the', 'sense', 'of', 'a', 'new', 'understanding', 'of', 'the', 'universe', 'the', 'demiurge', 'is', 'not', 'the']\n",
      "['and', 'here', 'is', 'another', 'interesting', 'fact', 'about', 'one', 'place', ';', 'both', 'terms', 'relate', 'them', 'not', 'by', 'far', 'simpler', 'meaning', '—', 'they', 'learn', 'to', 'talk', 'out', 'without', 'falling', 'its', 'place', 'back', 'across', 'into', 'god', 'merely', 'will', 'by', 'be']\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def random_sampling(model, prompt, max_length, temperature, topk):\n",
    "    # First, encode the input\n",
    "    encoded_prompt = [vocab_builder.get_token_id(word) for word in prompt.split(\" \")]\n",
    "\n",
    "    logits = model(torch.tensor(encoded_prompt).to(device))\n",
    "\n",
    "    # Mask \"unknown\" and \"padding\" tokens\n",
    "    logits[..., [2, 3]] = -float('inf')  # Assuming 2 and 3 correspond to \"unknown\" and \"padding\"\n",
    "\n",
    "    # Apply temperature\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    tempered_logits = softmax(logits / temperature)\n",
    "\n",
    "    # Apply top-k filtering\n",
    "    indices_to_remove = logits < torch.topk(logits, topk, dim=1)[0][..., -1, None]\n",
    "    tempered_logits[indices_to_remove] = -np.Inf\n",
    "\n",
    "    # Sample from the distribution\n",
    "    distribution = Categorical(logits=tempered_logits)\n",
    "    prediction = distribution.sample()\n",
    "\n",
    "    encoded_prompt.append(prediction[-1].item())\n",
    "\n",
    "    end_of_sentence = (prediction[-1].item() == 1)\n",
    "    words_generated = 1\n",
    "\n",
    "    # Repeat with its own outputs:\n",
    "    while (words_generated < max_length) and not end_of_sentence:\n",
    "\n",
    "        # The logits\n",
    "        logits = model(torch.tensor(encoded_prompt).to(device))\n",
    "\n",
    "        # Mask \"unknown\" and \"padding\" tokens\n",
    "        logits[..., [2, 3]] = -float('inf')\n",
    "\n",
    "        # Apply temperature\n",
    "        tempered_logits = softmax(logits / temperature)\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        indices_to_remove = logits < torch.topk(logits, topk, dim=1)[0][..., -1, None]\n",
    "        tempered_logits[indices_to_remove] = -np.Inf\n",
    "\n",
    "        # Sample from the distribution\n",
    "        distribution = Categorical(logits=tempered_logits)\n",
    "        prediction = distribution.sample()\n",
    "        encoded_prompt.append(prediction[-1].item())\n",
    "\n",
    "        # Check if end of sentence and update word counter\n",
    "        if (prediction[-1].item() == 1):\n",
    "            end_of_sentence = True\n",
    "        words_generated += 1\n",
    "\n",
    "    return [vocab_builder.get_token_str(word) for word in encoded_prompt]\n",
    "\n",
    "# Test it\n",
    "print(random_sampling(model, \"he lives in san\", 1, 1, 1)) # Sanity check\n",
    "print(random_sampling(model, \"he lives in san\", 1, 0.000001, 10)) # Sanity check\n",
    "print(random_sampling(model, \"he lives in san\", 30, 0.5, 5))\n",
    "print(random_sampling(model, \"which is very\", 30, 1, 5))\n",
    "print(random_sampling(model, \"which is very\", 30, 2, 10))\n",
    "print(random_sampling(model, \"and here is another interesting fact about\", 30, 0.5, 10))\n",
    "print(random_sampling(model, \"and here is another interesting fact about\", 30, 0.5, 3))\n",
    "print(random_sampling(model, \"and here is another interesting fact about\", 30, 2, 100))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
