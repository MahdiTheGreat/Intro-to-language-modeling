{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/Intro_to_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
        "%cd Intro-to-language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXuQ-VlzuRv",
        "outputId": "42e1802d-f5df-4412-abcb-4e9c2c6d53b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-language-modeling'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (7/7), 28.27 MiB | 18.40 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/Intro-to-language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "yO3xXRA_0ppt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eFpoOrObrug"
      },
      "outputs": [],
      "source": [
        "!pip install ipdb\n",
        "!pip install -U spacy\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5fv9gQcVafW3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import ipdb\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "# %pdb on"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to plot the training metrics\n",
        "\n",
        "def plot_training_metrics(train_acc, val_acc, train_loss, title, save_path):\n",
        "    # Ensure that all input lists have the same length\n",
        "    assert len(train_acc) == len(val_acc) == len(train_loss), \"All input histories must have the same length.\"\n",
        "\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    # Create the metrics DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Accuracy (%)': train_acc,\n",
        "        'Validation Accuracy (%)': val_acc,\n",
        "        'Training Loss': train_loss\n",
        "    })\n",
        "\n",
        "    # Initialize the plot\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Training and Validation Accuracy on ax1\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy (%)', color=color)\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Training Accuracy (%)'], label='Train Acc', color='tab:blue')\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Validation Accuracy (%)'], label='Val Acc', color='tab:cyan')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Create a second y-axis for Training Loss\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Train Loss', color='tab:red')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
        "\n",
        "    # Set plot title and layout\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save and display the plot\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TDDGQTI51AF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class SimpleANN(nn.Module):\n",
        "\n",
        "    def __init__(self,layer_sizes,activation=nn.ReLU,last_layer_activation=nn.ReLU,dropout=0):\n",
        "\n",
        "        super(SimpleANN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes)-2):\n",
        "          self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "          self.layers.append(nn.Dropout(dropout))\n",
        "          self.layers.append(activation())\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "        if last_layer_activation is not None:\n",
        "         self.layers.append(nn.Dropout(dropout))\n",
        "         self.layers.append(last_layer_activation())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, np.prod(x.shape[1:])) # Flatten the input\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Q2CkGhOV1JpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "Qjj4IdOi08ms"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDybdl906rv",
        "outputId": "afee9ef2-6a19-4260-93df-50e7a3660b6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='lmdemo'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "!unzip -q $zip_file\n",
        "!rm $zip_file"
      ],
      "metadata": {
        "id": "IE8oAx8b3AWX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=open(f'{dataset}/train.txt','r',encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "clFRaGPQ4Jc-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[0:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "kCOBnKat4PbB",
        "outputId": "b0935bf1-5d6e-4d12-8a14-be6a4cc11b83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Anatomy\\n\\nAnatomy (Greek anatomē, “dissection”) is the branch of biology concerned with the study of the structure of organisms and their parts.  Anatomy is a branch of natural science dealing with the structural organization of living things.  It is an old science, having its beginnings in prehistoric times.  Anatomy is inherently tied to embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "example_text = \"\"\"This is a sample text with several sentences. We want to split it into parts\n",
        "without cutting off in the middle of sentences. This approach helps keep each part\n",
        "meaningful and easy to read. It can be useful for processing large texts or preparing\n",
        "them for models that have a maximum input size.\"\"\""
      ],
      "metadata": {
        "id": "jpLhPUwr9NyJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy model for tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class VocabularyBuilder:\n",
        "    def __init__(self, max_voc_size):\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.str_to_int = {}\n",
        "        self.int_to_str = {}\n",
        "        self.special_tokens = [\"BEGINNING\", \"END\", \"UNKNOWN\"]\n",
        "\n",
        "    def build_vocabulary(self, text):\n",
        "\n",
        "        tokens = []\n",
        "\n",
        "        if isinstance(text, list):\n",
        "          sents=text\n",
        "        else:\n",
        "          doc = nlp(text)\n",
        "          sents=doc.sents\n",
        "\n",
        "        # Process each sentence in the text\n",
        "        for sent in sents:\n",
        "            tokens.append(nlp(\"BEGINNING\")[0])  # Add \"BEGINNING\" at the start of each sentence\n",
        "            tokens.extend([token for token in sent])  # Add sentence tokens\n",
        "            tokens.append(nlp(\"END\")[0])  # Add \"END\" at the end of each sentence\n",
        "\n",
        "        token_counter = Counter()\n",
        "        for token in tokens:\n",
        "         if not token.is_space and not token.is_punct:\n",
        "             token_counter[token.text.lower()] += 1\n",
        "\n",
        "        # Start vocabulary with special tokens\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Select the most common tokens, considering max_voc_size - len(special_tokens)\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = token_counter.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def get_token_id(self, token):\n",
        "        # Return the integer ID for a given token\n",
        "        return self.str_to_int.get(token.lower(), self.str_to_int[\"UNKNOWN\"])\n",
        "\n",
        "    def get_token_str(self, token_id):\n",
        "        # Return the original token string for a given integer ID\n",
        "        return self.int_to_str.get(token_id, \"UNKNOWN\")\n",
        "\n",
        "    def add_special_tokens_to_text(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes the text by sentence and adds special 'BEGINNING' and 'END' tokens\n",
        "        around each sentence.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input text.\n",
        "\n",
        "        Returns:\n",
        "        - List[str]: A list of tokens with special 'BEGINNING' and 'END' tokens added.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def sanity_check(self):\n",
        "        # Check vocabulary size\n",
        "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
        "\n",
        "        # Check special tokens exist and are unique\n",
        "        for token in self.special_tokens:\n",
        "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
        "\n",
        "        # Check if highly frequent words are included and rare ones are not\n",
        "        common_words = [\"the\", \"and\"]\n",
        "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
        "\n",
        "        for word in common_words:\n",
        "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
        "\n",
        "        for word in rare_words:\n",
        "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
        "\n",
        "        # Check that mapping back and forth works for a test word\n",
        "        test_word = \"The\"\n",
        "        token_id = self.get_token_id(test_word)\n",
        "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
        "\n",
        "        print(\"Sanity check passed!\")\n",
        "\n",
        "# Example usage\n",
        "#example_text = [\n",
        "#   \"This is a simple example sentence.\",\n",
        "#   \"Here's another example sentence in a different paragraph.\"\n",
        "#   \"The quick brown fox jumps over the lazy dog and cat.\"\n",
        "#]\n",
        "#\n",
        "# Initialize VocabularyBuilder with a max vocabulary size\n",
        "vocab_builder = VocabularyBuilder(max_voc_size=50)\n",
        "vocab_builder.build_vocabulary(example_text)\n",
        "\n",
        "# Example mappings\n",
        "print(\"str_to_int:\", vocab_builder.str_to_int)\n",
        "print(\"int_to_str:\", vocab_builder.int_to_str)\n",
        "\n",
        "# Convert a token to integer ID and back to string\n",
        "token_id = vocab_builder.get_token_id(\"example\")\n",
        "print(\"Token ID for 'example':\", token_id)\n",
        "print(\"Original token from ID:\", vocab_builder.get_token_str(token_id))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZJ4k7STz96H",
        "outputId": "af695d9a-86a7-448d-ddba-797d2c98cf2a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "str_to_int: {'BEGINNING': 0, 'END': 1, 'UNKNOWN': 2, 'beginning': 3, 'end': 4, 'this': 5, 'a': 6, 'sentences': 7, 'to': 8, 'it': 9, 'for': 10, 'is': 11, 'sample': 12, 'text': 13, 'with': 14, 'several': 15, 'we': 16, 'want': 17, 'split': 18, 'into': 19, 'parts': 20, 'without': 21, 'cutting': 22, 'off': 23, 'in': 24, 'the': 25, 'middle': 26, 'of': 27, 'approach': 28, 'helps': 29, 'keep': 30, 'each': 31, 'part': 32, 'meaningful': 33, 'and': 34, 'easy': 35, 'read': 36, 'can': 37, 'be': 38, 'useful': 39, 'processing': 40, 'large': 41, 'texts': 42, 'or': 43, 'preparing': 44, 'them': 45, 'models': 46, 'that': 47, 'have': 48, 'maximum': 49}\n",
            "int_to_str: {0: 'BEGINNING', 1: 'END', 2: 'UNKNOWN', 3: 'beginning', 4: 'end', 5: 'this', 6: 'a', 7: 'sentences', 8: 'to', 9: 'it', 10: 'for', 11: 'is', 12: 'sample', 13: 'text', 14: 'with', 15: 'several', 16: 'we', 17: 'want', 18: 'split', 19: 'into', 20: 'parts', 21: 'without', 22: 'cutting', 23: 'off', 24: 'in', 25: 'the', 26: 'middle', 27: 'of', 28: 'approach', 29: 'helps', 30: 'keep', 31: 'each', 32: 'part', 33: 'meaningful', 34: 'and', 35: 'easy', 36: 'read', 37: 'can', 38: 'be', 39: 'useful', 40: 'processing', 41: 'large', 42: 'texts', 43: 'or', 44: 'preparing', 45: 'them', 46: 'models', 47: 'that', 48: 'have', 49: 'maximum'}\n",
            "Token ID for 'example': 2\n",
            "Original token from ID: UNKNOWN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sanity check\n",
        "vocab_builder.sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sB4W8Q0Qo9",
        "outputId": "a71c93e2-80c5-4b6b-8477-3243f696bb4b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingDataPreparer:\n",
        "    def __init__(self, vocab_builder, context_window_size):\n",
        "        self.vocab_builder = vocab_builder\n",
        "        self.N = context_window_size\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"Tokenizes and encodes a single string with special symbols.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to encode.\n",
        "\n",
        "        Returns:\n",
        "        - List[int]: A list of token IDs including BEGINNING and END tokens.\n",
        "        \"\"\"\n",
        "        # Tokenize the text\n",
        "        doc = nlp(text)\n",
        "        tokens = [\"BEGINNING\"] * self.N  # Add N BEGINNING tokens at the start\n",
        "        tokens.extend([token.text.lower() for token in doc])  # Add the actual tokens\n",
        "        tokens.append(\"END\")  # Add END token at the end\n",
        "\n",
        "        # Map tokens to integer IDs, using \"UNKNOWN\" for out-of-vocabulary words\n",
        "        token_ids = [self.vocab_builder.get_token_id(token) for token in tokens]\n",
        "        return token_ids\n",
        "\n",
        "    def create_training_sequences(self, text):\n",
        "        \"\"\"\n",
        "        Creates training sequences from a single string by generating sequences of length N+1.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to create sequences from.\n",
        "\n",
        "        Returns:\n",
        "        - List[Tuple[List[int], int]]: A list of (context, target) pairs.\n",
        "        \"\"\"\n",
        "        training_sequences = []\n",
        "\n",
        "        # Encode the text with BEGINNING, END, and UNKNOWN tokens\n",
        "        encoded_text = self.encode_text(text)\n",
        "\n",
        "        # Generate sequences of length N+1\n",
        "        for i in range(len(encoded_text) - self.N):\n",
        "            context = encoded_text[i : i + self.N]  # N tokens for context\n",
        "            target = encoded_text[i + self.N]       # Next token as the target\n",
        "            training_sequences.append((context, target))\n",
        "\n",
        "        return training_sequences\n"
      ],
      "metadata": {
        "id": "uXLrr6YeF0AF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_window_size = 10\n",
        "data_preparer = TrainingDataPreparer(vocab_builder, context_window_size)\n",
        "\n",
        "# Tokenize text for training sequences\n",
        "#paragraphs = [\n",
        "#    [\"this\", \"is\", \"a\", \"simple\", \"example\", \"sentence\"],\n",
        "#    [\"here's\", \"another\", \"example\", \"sentence\", \"in\", \"a\", \"different\", \"paragraph\"],\n",
        "#    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "#]\n",
        "\n",
        "# Create training sequences\n",
        "training_sequences = data_preparer.create_training_sequences(example_text)\n",
        "\n",
        "# Display some training sequences\n",
        "print(\"Training sequences (context, target):\")\n",
        "for context, target in training_sequences[:5]:  # Show the first few sequences\n",
        "    print([vocab_builder.get_token_str(id) for id in context], \"->\", vocab_builder.get_token_str(target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A5cX52Y73i1",
        "outputId": "c2160608-bd8e-4edb-880b-c860bd884a0c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sequences (context, target):\n",
            "['beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning'] -> this\n",
            "['beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'this'] -> is\n",
            "['beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'this', 'is'] -> a\n",
            "['beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'this', 'is', 'a'] -> sample\n",
            "['beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'beginning', 'this', 'is', 'a', 'sample'] -> text\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNJPBP9OlvPnS645n5Yil7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}