{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparations and code from Assignment 1"
      ],
      "metadata": {
        "id": "Ash16YV-b6T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group 17: Jakob Svensson, Mahdi Afarideh, Maximilian Forsell"
      ],
      "metadata": {
        "id": "OWJbmNgzBECe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
        "%cd Intro-to-language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXuQ-VlzuRv",
        "outputId": "b6745c6e-ba2d-4ea1-a41b-3047424e5bd9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-language-modeling'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 76 (delta 40), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (76/76), 31.83 MiB | 9.61 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "/content/Intro-to-language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5fv9gQcVafW3"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import nltk\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLeT6K6EDID5",
        "outputId": "4ce4c071-7fd4-497b-f232-9be13733eafe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "Qjj4IdOi08ms"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDybdl906rv",
        "outputId": "edb5d022-9daa-44fd-c3eb-0ed072726b7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='lmdemo'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "!unzip -q $zip_file\n",
        "!rm $zip_file"
      ],
      "metadata": {
        "id": "IE8oAx8b3AWX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set=open(f'{dataset}/train.txt','r',encoding='utf-8').read()\n",
        "val_set=open(f'{dataset}/val.txt','r',encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "clFRaGPQ4Jc-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "class VocabularyBuilder:\n",
        "    def __init__(self, max_voc_size):\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.str_to_int = {}\n",
        "        self.int_to_str = {}\n",
        "        self.special_tokens = [\"BEGINNING\", \"END\", \"UNKNOWN\", \"PADDING\"] #Added padding\n",
        "        self.token_counter = Counter()\n",
        "\n",
        "    def build_vocabulary(self, text):\n",
        "\n",
        "        sents=nltk.word_tokenize(text.lower())\n",
        "\n",
        "        for token in sents:\n",
        "            self.token_counter[token] += 1\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = self.token_counter.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def create_premade_vocabulary(self, c):\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = c.most_common(max_words) # Here we can use a premade counter from a previous run\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def get_token_id(self, token):\n",
        "        return self.str_to_int.get(token.lower(), self.str_to_int[\"UNKNOWN\"])\n",
        "\n",
        "    def get_token_str(self, token_id):\n",
        "        return self.int_to_str.get(token_id, \"UNKNOWN\")\n",
        "\n",
        "    def sanity_check(self): # Here we run the sanity tests recommended in the assignment\n",
        "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
        "\n",
        "        for token in self.special_tokens:\n",
        "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
        "\n",
        "        common_words = [\"the\", \"and\"]\n",
        "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
        "\n",
        "        for word in common_words:\n",
        "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
        "\n",
        "        for word in rare_words:\n",
        "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
        "\n",
        "        test_word = \"the\"\n",
        "        token_id = self.get_token_id(test_word)\n",
        "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
        "\n",
        "        print(\"Sanity check passed!\")\n",
        "\n",
        "vocab_builder = VocabularyBuilder(max_voc_size=16384)\n"
      ],
      "metadata": {
        "id": "CZJ4k7STz96H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run only once!\n",
        "for paragraph in tqdm(training_set.splitlines()):\n",
        "  vocab_builder.build_vocabulary(paragraph)\n",
        "vocab_builder.create_vocabulary()"
      ],
      "metadata": {
        "id": "T1LhuLzfE2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7016d79a-cca1-49c8-eda7-c8f424970550"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 294118/294118 [01:24<00:00, 3466.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save vocab so we don't have to rerun it\n",
        "counter= vocab_builder.token_counter\n",
        "with open(\"full_vocab\", 'w') as f:\n",
        "    for k,v in  counter.most_common():\n",
        "        f.write( \"{} {}\\n\".format(k,v) )"
      ],
      "metadata": {
        "id": "Z4cv1HQlQ0ya"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this using full_vocab from first run\n",
        "premade_counter = Counter()\n",
        "\n",
        "with open(\"/content/Intro-to-language-modeling/full_vocab\", 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.split(\" \")\n",
        "        if len(parts) == 2:\n",
        "            word, freq = parts[0], int(parts[1])\n",
        "            premade_counter[word] = freq\n",
        "vocab_builder.create_premade_vocabulary(premade_counter)\n"
      ],
      "metadata": {
        "id": "aIiqpkWcE8sM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sanity check\n",
        "vocab_builder.sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sB4W8Q0Qo9",
        "outputId": "472e475f-c0f2-4ebb-d3b9-c724c3c23e2d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified for assignment 2\n",
        "class TrainingDataPreparerRNN:\n",
        "    def __init__(self, vocab_builder, max_sequence_length):\n",
        "        self.vocab_builder = vocab_builder\n",
        "        self.max = max_sequence_length\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"Tokenizes and encodes a single string with special symbols.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to encode.\n",
        "\n",
        "        Returns:\n",
        "        - List[int]: A list of token IDs including BEGINNING and END tokens.\n",
        "        \"\"\"\n",
        "        # Tokenize the text\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "        token_ids = [self.vocab_builder.get_token_id(token) for token in tokens]\n",
        "        modified_tokens = [0] # Add 1 BEGINNING\n",
        "        modified_tokens.extend(token_ids)\n",
        "        modified_tokens.append(1) # Add 1 END\n",
        "\n",
        "        return modified_tokens\n",
        "\n",
        "    def create_training_sequences(self, text):\n",
        "        \"\"\"\n",
        "        Creates training sequences from a single string by generating sequences of length N+1.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to create sequences from.\n",
        "\n",
        "        Returns:\n",
        "        - List[Tuple[List[int], int]]: A list of (context, target) pairs.\n",
        "        \"\"\"\n",
        "        encoded_text = self.encode_text(text)\n",
        "\n",
        "        # Taken from: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
        "        training_sequences = [encoded_text[i * self.max:(i + 1) * self.max] for i in range((len(encoded_text) + self.max - 1) // self.max )]\n",
        "\n",
        "        return training_sequences\n"
      ],
      "metadata": {
        "id": "uXLrr6YeF0AF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "8VLcOGFvb-pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting\n",
        "preparer = TrainingDataPreparerRNN(vocab_builder, max_sequence_length=50)\n",
        "\n",
        "training_sequences = []\n",
        "split_training_set = list(filter(''.__ne__, training_set.splitlines())) # Split and remove empty lines\n",
        "for paragraph in tqdm(split_training_set):\n",
        "  training_sequences.append(preparer.create_training_sequences(paragraph))\n",
        "flattened_training_sequences =  [\n",
        "    x\n",
        "    for xs in training_sequences\n",
        "    for x in xs\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOhqoltJEEYu",
        "outputId": "3c879da7-bb38-48c1-9cc7-32585d00ed01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 147059/147059 [01:26<00:00, 1694.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare validation data also\n",
        "val_sequences = []\n",
        "split_val_set = list(filter(''.__ne__, val_set.splitlines())) # Split and remove empty lines\n",
        "for paragraph in tqdm(split_val_set):\n",
        "  val_sequences.append(preparer.create_training_sequences(paragraph))\n",
        "flattened_val_sequences =  [\n",
        "    x\n",
        "    for xs in val_sequences\n",
        "    for x in xs\n",
        "]"
      ],
      "metadata": {
        "id": "2iMBAWK0VWxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7bf779-81d7-4b82-d763-21493a4e0681"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17874/17874 [00:11<00:00, 1620.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for context in flattened_training_sequences[:10]:  # Show the first few sequences\n",
        "    print([vocab_builder.get_token_str(id) for id in context])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWozIyMKLkhs",
        "outputId": "a42ca575-f668-4fa3-bbd2-1053f9a91cde"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BEGINNING', 'anatomy', 'END']\n",
            "['BEGINNING', 'anatomy', '(', 'greek', 'UNKNOWN', ',', '“', 'dissection', '”', ')', 'is', 'the', 'branch', 'of', 'biology', 'concerned', 'with', 'the', 'study', 'of', 'the', 'structure', 'of', 'organisms', 'and', 'their', 'parts', '.', 'anatomy', 'is', 'a', 'branch', 'of', 'natural', 'science', 'dealing', 'with', 'the', 'structural', 'organization', 'of', 'living', 'things', '.', 'it', 'is', 'an', 'old', 'science', ',']\n",
            "['having', 'its', 'beginnings', 'in', 'prehistoric', 'times', '.', 'anatomy', 'is', 'inherently', 'tied', 'to', 'UNKNOWN', ',', 'comparative', 'anatomy', ',', 'evolutionary', 'biology', ',', 'and', 'phylogeny', ',', 'as', 'these', 'are', 'the', 'processes', 'by', 'which', 'anatomy', 'is', 'generated', 'over', 'immediate', '(', 'UNKNOWN', ')', 'and', 'long', '(', 'evolution', ')', 'UNKNOWN', '.', 'human', 'anatomy', 'is', 'one', 'of']\n",
            "['the', 'basic', 'essential', 'sciences', 'of', 'medicine', '.', 'END']\n",
            "['BEGINNING', 'the', 'discipline', 'of', 'anatomy', 'is', 'divided', 'into', 'macroscopic', 'and', 'microscopic', 'anatomy', '.', 'macroscopic', 'anatomy', ',', 'or', 'gross', 'anatomy', ',', 'is', 'the', 'examination', 'of', 'an', 'animal', \"'s\", 'body', 'parts', 'using', 'UNKNOWN', 'UNKNOWN', '.', 'gross', 'anatomy', 'also', 'includes', 'the', 'branch', 'of', 'superficial', 'anatomy', '.', 'microscopic', 'anatomy', 'involves', 'the', 'use', 'of', 'optical']\n",
            "['instruments', 'in', 'the', 'study', 'of', 'the', 'tissues', 'of', 'various', 'structures', ',', 'known', 'as', 'UNKNOWN', ',', 'and', 'also', 'in', 'the', 'study', 'of', 'cells', '.', 'END']\n",
            "['BEGINNING', 'the', 'history', 'of', 'anatomy', 'is', 'characterized', 'by', 'a', 'progressive', 'understanding', 'of', 'the', 'functions', 'of', 'the', 'organs', 'and', 'structures', 'of', 'the', 'human', 'body', '.', 'methods', 'have', 'also', 'improved', 'dramatically', ',', 'advancing', 'from', 'the', 'examination', 'of', 'animals', 'by', 'dissection', 'of', 'UNKNOWN', 'and', 'UNKNOWN', '(', 'UNKNOWN', ')', 'to', '20th', 'century', 'medical', 'imaging']\n",
            "['techniques', 'including', 'x-ray', ',', 'ultrasound', ',', 'and', 'magnetic', 'resonance', 'imaging', '.', 'END']\n",
            "['BEGINNING', 'anatomy', 'and', 'physiology', ',', 'which', 'study', '(', 'respectively', ')', 'the', 'structure', 'and', 'function', 'of', 'organisms', 'and', 'their', 'parts', ',', 'make', 'a', 'natural', 'pair', 'of', 'related', 'disciplines', ',', 'and', 'they', 'are', 'often', 'studied', 'together', '.', 'END']\n",
            "['BEGINNING', 'derived', 'from', 'the', 'greek', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'dissection', \"''\", '(', 'from', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'i', 'cut', 'up', ',', 'cut', 'open', \"''\", 'from', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'up', \"''\", ',', 'and', 'UNKNOWN', '``', 'UNKNOWN', \"''\", '``', 'i', 'cut', \"''\", ')', ',', 'anatomy', 'is', 'the', 'scientific']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check nr. 2\n",
        "print(len(flattened_training_sequences))\n",
        "print(len(flattened_val_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ECPe-uLMYy8",
        "outputId": "a8716091-a86f-4529-e412-968bb1d02084"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "323198\n",
            "40499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapted batcher\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def TorchDataLoaderRNN(training_sequences, max_sequence_length, batch_size):\n",
        "  # Padding\n",
        "  padded_sequences = [sequence +([3] * (max_sequence_length - len(sequence))) for sequence in training_sequences] # PADDING has integer code 3\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  context_tensor = torch.tensor(padded_sequences, dtype=torch.long)  # Shape: (num_samples, 3)\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(context_tensor)\n",
        "\n",
        "  # Create a DataLoader for batching\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "DqKicZuza81v"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = TorchDataLoaderRNN(flattened_training_sequences, 50, 64)"
      ],
      "metadata": {
        "id": "5nNMuktjU1IC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valloader = TorchDataLoaderRNN(flattened_val_sequences, 50, 64)"
      ],
      "metadata": {
        "id": "9FpFFCWqVl2j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for batch_context in trainloader:\n",
        "    print(batch_context[0])\n",
        "    print(batch_context[0].shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD0wfSt5OVvX",
        "outputId": "4afe7355-9a2d-46d5-885b-339b87e82c9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   14,   524,     4,  ...,     1,     3,     3],\n",
            "        [    2,   265,    10,  ...,   319,   787,     8],\n",
            "        [   11,   678,     7,  ...,     4,  6738,  1639],\n",
            "        ...,\n",
            "        [ 2302,     2,    10,  ...,   121,     6,   262],\n",
            "        [    0,    61,    29,  ..., 11305,    47,  2404],\n",
            "        [   38,    50,   488,  ...,     3,     3,     3]])\n",
            "torch.Size([64, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: RNN model"
      ],
      "metadata": {
        "id": "6cxG6iHlclgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# EarlyStopping class remains the same\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
        "        self.patience = patience  # Number of epochs to wait for improvement\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.path = path  # Path to save the best model\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss < self.best_score - self.delta:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Save model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, layer_sizes,activation=nn.ReLU,last_layer_activation=nn.Softmax,dropout=0):\n",
        "\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes)-2):\n",
        "          self.layers.append(nn.LSTM(layer_sizes[i], layer_sizes[i+1]))\n",
        "          self.layers.append(nn.Dropout(dropout))\n",
        "          self.layers.append(activation())\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "        if last_layer_activation is not None:\n",
        "         self.layers.append(nn.Dropout(dropout))\n",
        "         self.layers.append(last_layer_activation())\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embeddings(x)  # Get word embeddings for each word in the batch\n",
        "\n",
        "        # Flatten the input embeddings\n",
        "        x = embeddings.view(-1, np.prod(embeddings.shape[1:]))\n",
        "\n",
        "        x = x.float()\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, nn.LSTM):\n",
        "                x = layer(x)[0]\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Q2CkGhOV1JpZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(layer_sizes=[6272, 2048, 16384], vocab_size=16384, embed_size=128)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=3) # Ignore padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "patience = 5\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "number_of_epochs = 50\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch_context in tqdm(trainloader):\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context[0][:,:-1]\n",
        "        Y = batch_context[0][:,1]\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        logits = model(X)  # Model output for X\n",
        "        targets = Y.view(-1)                      # 2-dimensional -> 1-dimensional\n",
        "        logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -> 2-dimensional\n",
        "        loss = criterion(logits, targets) # Compute the loss between model output and Y\n",
        "\n",
        "        #BACKWARD PASS (updating the model parameters):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{number_of_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradient computation for validation\n",
        "        for batch_context in valloader:\n",
        "        #FORWARD PASS:\n",
        "          X = batch_context[0][:,:-1]\n",
        "          Y = batch_context[0][:,1]\n",
        "          X, Y = X.to(device), Y.to(device)\n",
        "          logits = model(X)  # Model output for X\n",
        "          targets = Y.view(-1)                      # 2-dimensional -> 1-dimensional\n",
        "          logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -> 2-dimensional\n",
        "          loss = criterion(logits, targets) # Compute the loss between model output and Y\n",
        "          val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
        "    print(f\"Epoch {epoch+1}/{number_of_epochs} - Perplexity: {np.exp(avg_val_loss):.6f}\")\n",
        "\n",
        "    # Call early stopping after each epoch\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, load the best model after training\n",
        "model.load_state_dict(torch.load('checkpoint.pth'))"
      ],
      "metadata": {
        "id": "7iUtwI8nbBO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a411b6-77a5-4cae-8469-666d39a814ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 9.3220\n",
            "Epoch 1/50 - Perplexity: 10561.115899\n",
            "Validation loss decreased (inf --> 9.264934).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50], Loss: 9.1418\n",
            "Epoch 2/50 - Perplexity: 9141.887963\n",
            "Validation loss decreased (9.264934 --> 9.120622).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50], Loss: 8.9855\n",
            "Epoch 3/50 - Perplexity: 8797.847437\n",
            "Validation loss decreased (9.120622 --> 9.082262).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50], Loss: 9.1425\n",
            "Epoch 4/50 - Perplexity: 8575.104197\n",
            "Validation loss decreased (9.082262 --> 9.056618).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50], Loss: 9.0167\n",
            "Epoch 5/50 - Perplexity: 8346.672884\n",
            "Validation loss decreased (9.056618 --> 9.029618).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50], Loss: 9.0408\n",
            "Epoch 6/50 - Perplexity: 8197.973643\n",
            "Validation loss decreased (9.029618 --> 9.011642).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50], Loss: 8.9946\n",
            "Epoch 7/50 - Perplexity: 8086.192458\n",
            "Validation loss decreased (9.011642 --> 8.997913).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50], Loss: 9.0698\n",
            "Epoch 8/50 - Perplexity: 7959.843709\n",
            "Validation loss decreased (8.997913 --> 8.982165).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50], Loss: 8.9187\n",
            "Epoch 9/50 - Perplexity: 7882.849640\n",
            "Validation loss decreased (8.982165 --> 8.972445).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 9.0476\n",
            "Epoch 10/50 - Perplexity: 7772.631272\n",
            "Validation loss decreased (8.972445 --> 8.958364).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/50], Loss: 8.9230\n",
            "Epoch 11/50 - Perplexity: 7696.322012\n",
            "Validation loss decreased (8.958364 --> 8.948498).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/50], Loss: 8.8927\n",
            "Epoch 12/50 - Perplexity: 7624.042373\n",
            "Validation loss decreased (8.948498 --> 8.939062).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/50], Loss: 8.9311\n",
            "Epoch 13/50 - Perplexity: 7594.505649\n",
            "Validation loss decreased (8.939062 --> 8.935180).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/50], Loss: 8.9774\n",
            "Epoch 14/50 - Perplexity: 7529.155276\n",
            "Validation loss decreased (8.935180 --> 8.926538).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/50], Loss: 8.7668\n",
            "Epoch 15/50 - Perplexity: 7489.050019\n",
            "Validation loss decreased (8.926538 --> 8.921197).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/50], Loss: 8.8468\n",
            "Epoch 16/50 - Perplexity: 7459.713686\n",
            "Validation loss decreased (8.921197 --> 8.917272).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/50], Loss: 8.8608\n",
            "Epoch 17/50 - Perplexity: 7449.566681\n",
            "Validation loss decreased (8.917272 --> 8.915911).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/50], Loss: 8.8518\n",
            "Epoch 18/50 - Perplexity: 7410.085304\n",
            "Validation loss decreased (8.915911 --> 8.910597).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/50], Loss: 8.7979\n",
            "Epoch 19/50 - Perplexity: 7360.674195\n",
            "Validation loss decreased (8.910597 --> 8.903907).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50], Loss: 8.8067\n",
            "Epoch 20/50 - Perplexity: 7339.012128\n",
            "Validation loss decreased (8.903907 --> 8.900960).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/50], Loss: 8.8293\n",
            "Epoch 21/50 - Perplexity: 7341.449532\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/50], Loss: 8.7980\n",
            "Epoch 22/50 - Perplexity: 7315.444738\n",
            "Validation loss decreased (8.900960 --> 8.897743).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/50], Loss: 8.7979\n",
            "Epoch 23/50 - Perplexity: 7273.188920\n",
            "Validation loss decreased (8.897743 --> 8.891950).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/50], Loss: 8.8292\n",
            "Epoch 24/50 - Perplexity: 7268.474944\n",
            "Validation loss decreased (8.891950 --> 8.891302).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/50], Loss: 8.7991\n",
            "Epoch 25/50 - Perplexity: 7238.942326\n",
            "Validation loss decreased (8.891302 --> 8.887230).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/50], Loss: 8.7050\n",
            "Epoch 26/50 - Perplexity: 7251.909553\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/50], Loss: 8.7667\n",
            "Epoch 27/50 - Perplexity: 7232.273846\n",
            "Validation loss decreased (8.887230 --> 8.886309).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/50], Loss: 8.7364\n",
            "Epoch 28/50 - Perplexity: 7229.802467\n",
            "Validation loss decreased (8.886309 --> 8.885967).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/50], Loss: 8.7667\n",
            "Epoch 29/50 - Perplexity: 7213.790645\n",
            "Validation loss decreased (8.885967 --> 8.883750).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50], Loss: 8.7666\n",
            "Epoch 30/50 - Perplexity: 7205.643353\n",
            "Validation loss decreased (8.883750 --> 8.882620).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/50], Loss: 8.7979\n",
            "Epoch 31/50 - Perplexity: 7184.845195\n",
            "Validation loss decreased (8.882620 --> 8.879729).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/50], Loss: 8.7354\n",
            "Epoch 32/50 - Perplexity: 7195.504554\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/50], Loss: 8.7043\n",
            "Epoch 33/50 - Perplexity: 7205.505086\n",
            "EarlyStopping counter: 2 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/50], Loss: 8.7042\n",
            "Epoch 34/50 - Perplexity: 7200.551284\n",
            "EarlyStopping counter: 3 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/50], Loss: 8.7354\n",
            "Epoch 35/50 - Perplexity: 7181.211498\n",
            "Validation loss decreased (8.879729 --> 8.879223).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/50], Loss: 8.7175\n",
            "Epoch 36/50 - Perplexity: 7147.298655\n",
            "Validation loss decreased (8.879223 --> 8.874490).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/50], Loss: 8.7354\n",
            "Epoch 37/50 - Perplexity: 7141.136954\n",
            "Validation loss decreased (8.874490 --> 8.873627).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/50], Loss: 8.7667\n",
            "Epoch 38/50 - Perplexity: 7119.064970\n",
            "Validation loss decreased (8.873627 --> 8.870532).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/50], Loss: 8.7042\n",
            "Epoch 39/50 - Perplexity: 7133.990782\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50], Loss: 8.7354\n",
            "Epoch 40/50 - Perplexity: 7147.621151\n",
            "EarlyStopping counter: 2 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/50], Loss: 8.7042\n",
            "Epoch 41/50 - Perplexity: 7108.413663\n",
            "Validation loss decreased (8.870532 --> 8.869034).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/50], Loss: 8.7042\n",
            "Epoch 42/50 - Perplexity: 7134.516708\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/50], Loss: 8.7042\n",
            "Epoch 43/50 - Perplexity: 7134.201174\n",
            "EarlyStopping counter: 2 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/50], Loss: 8.7351\n",
            "Epoch 44/50 - Perplexity: 7107.376060\n",
            "Validation loss decreased (8.869034 --> 8.868888).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/50], Loss: 8.7042\n",
            "Epoch 45/50 - Perplexity: 7111.834872\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/50], Loss: 8.7042\n",
            "Epoch 46/50 - Perplexity: 7112.617610\n",
            "EarlyStopping counter: 2 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/50], Loss: 8.7042\n",
            "Epoch 47/50 - Perplexity: 7126.282333\n",
            "EarlyStopping counter: 3 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:07<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/50], Loss: 8.7042\n",
            "Epoch 48/50 - Perplexity: 7095.597942\n",
            "Validation loss decreased (8.868888 --> 8.867230).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:08<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/50], Loss: 8.7042\n",
            "Epoch 49/50 - Perplexity: 7115.973756\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:07<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50], Loss: 8.7042\n",
            "Epoch 50/50 - Perplexity: 7135.911362\n",
            "EarlyStopping counter: 2 out of 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-ad83c4d7c463>:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('checkpoint.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 generating text"
      ],
      "metadata": {
        "id": "Hsm3CLoWgub5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular test sentence using argmax"
      ],
      "metadata": {
        "id": "XfYGnWtmZ3Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want padding at the start of the sentence for testing:\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def TorchTestLoaderRNN(training_sequences, max_sequence_length, batch_size = 1):\n",
        "  # Padding\n",
        "  padded_sequences = [([3] * (max_sequence_length - len(sequence)-1) + sequence) for sequence in training_sequences] # PADDING has integer code 3\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  context_tensor = torch.tensor(padded_sequences, dtype=torch.long)  # Shape: (num_samples, 3)\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(context_tensor)\n",
        "\n",
        "  # Create a DataLoader for batching\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "gmvFgJguHnqK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"he lives in san\"\n",
        "\n",
        "encoded_sentence = [vocab_builder.get_token_id(word) for word in test_sentence.split(\" \")]\n",
        "\n",
        "test_sentence_loader= TorchTestLoaderRNN([encoded_sentence], 50)\n",
        "for sentence in test_sentence_loader:\n",
        "  encoded_sentence = sentence[0][:,:]\n",
        "  encoded_sentence = encoded_sentence.to(device)\n",
        "  output = model(encoded_sentence).detach()\n",
        "\n",
        "# Predict\n",
        "prediction = torch.argmax(output)\n",
        "\n",
        "print(vocab_builder.get_token_str(prediction.item()))"
      ],
      "metadata": {
        "id": "HOp6TdEPgtmJ",
        "outputId": "c5dfe076-6bfc-4b39-8594-537ce87b6857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random algorithm"
      ],
      "metadata": {
        "id": "GobxeiM2UW-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "def random_sampling(model, prompt, max_length, temperature, topk):\n",
        "    # First, encode the input\n",
        "    encoded_prompt = [vocab_builder.get_token_id(word) for word in prompt.split(\" \")]\n",
        "\n",
        "    test_sentence_loader= TorchTestLoaderRNN([encoded_prompt], 50)\n",
        "    for sentence in test_sentence_loader:\n",
        "      encoded_sentence = sentence[0][:,:]\n",
        "      encoded_sentence = encoded_sentence.to(device)\n",
        "      logits = model(encoded_sentence).detach()\n",
        "\n",
        "    # Apply temperature\n",
        "    softmax = torch.nn.Softmax()\n",
        "    tempered_logits = softmax(logits / temperature)\n",
        "\n",
        "    # Apply topk\n",
        "    topk_logits = torch.topk(tempered_logits, k=topk)\n",
        "\n",
        "    # Sample from the distribution\n",
        "    distribution = Categorical(logits=topk_logits[0])\n",
        "    prediction = distribution.sample()\n",
        "    encoded_prompt.append(prediction.item())\n",
        "\n",
        "    end_of_sentence = (prediction == 1)\n",
        "    words_generated = 1\n",
        "\n",
        "    # Repeat with its own outputs:\n",
        "    while words_generated < max_length and not end_of_sentence:\n",
        "        test_sentence_loader= TorchTestLoaderRNN([encoded_prompt], 50)\n",
        "        # The logits\n",
        "        for sentence in test_sentence_loader:\n",
        "          encoded_sentence = sentence[0][:,:]\n",
        "          encoded_sentence = encoded_sentence.to(device)\n",
        "          logits = model(encoded_sentence).detach()\n",
        "\n",
        "        # Apply temperature\n",
        "        tempered_logits = softmax(logits / temperature)\n",
        "\n",
        "        # Apply topk\n",
        "        topk_logits = torch.topk(tempered_logits, k=topk)\n",
        "\n",
        "        # Sample from the distribution\n",
        "        distribution = Categorical(logits=topk_logits[0])\n",
        "        prediction = distribution.sample()\n",
        "        encoded_prompt.append(prediction.item())\n",
        "\n",
        "        # Check if end of sentence and update word counter\n",
        "        if prediction == 1:\n",
        "            end_of_sentence = True\n",
        "        words_generated += 1\n",
        "    return [vocab_builder.get_token_str(word) for word in encoded_prompt]\n",
        "\n",
        "# Test it\n",
        "print(random_sampling(model, \"he lives in san\", 30, 0.001, 10)) # Sanity check\n",
        "print(random_sampling(model, \"he lives in san\", 30, 0.5, 5))\n",
        "print(random_sampling(model, \"which is very\", 30, 1, 5))\n",
        "print(random_sampling(model, \"which is very\", 30, 2, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKs4_tg1T3C6",
        "outputId": "e09fca22-c701-4b9f-8e7d-28e26673f53d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'lives', 'in', 'san', 'PADDING', '.', 'PADDING', 'BEGINNING', 'BEGINNING', 'BEGINNING', 'END']\n",
            "['he', 'lives', 'in', 'san', 'PADDING', 'END']\n",
            "['which', 'is', 'very', 'PADDING', 'BEGINNING', 'PADDING', 'UNKNOWN', 'the', 'the', 'UNKNOWN', 'UNKNOWN', 'PADDING', 'the', 'PADDING', 'UNKNOWN', 'END']\n",
            "['which', 'is', 'very', 'END']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}