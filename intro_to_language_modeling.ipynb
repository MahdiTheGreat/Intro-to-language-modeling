{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/intro_to_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
        "%cd Intro-to-language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXuQ-VlzuRv",
        "outputId": "d7c29141-5000-49a6-8269-d85bb3e4efd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-language-modeling'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 34 (delta 17), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (34/34), 28.30 MiB | 24.17 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "/content/Intro-to-language-modeling/Intro-to-language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "yO3xXRA_0ppt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eFpoOrObrug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3771ca0-15b8-457a-fc92-ec5178d4d84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
            "  Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
            "  Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.4 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.0.1 numpy-2.0.2 spacy-3.8.2 thinc-8.3.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipdb\n",
        "!pip install -U spacy\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fv9gQcVafW3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import ipdb\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxLmzqoUc339",
        "outputId": "f99efe95-66e1-456d-dd2d-12bcd2b13592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "Qjj4IdOi08ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDybdl906rv",
        "outputId": "a3817812-1fed-4631-e759-09f3b0f47501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='lmdemo'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "!unzip -q $zip_file\n",
        "!rm $zip_file"
      ],
      "metadata": {
        "id": "IE8oAx8b3AWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=open(f'{dataset}/train.txt','r',encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "clFRaGPQ4Jc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_filepath=\"example.txt\""
      ],
      "metadata": {
        "id": "xDdQVDPjdCq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "\n",
        "def get_token_counter(filepath,nlp,encoding):\n",
        "\n",
        "    buffer = \"\"  # Buffer to store partial sentences between lines\n",
        "    token_counter = Counter()\n",
        "\n",
        "    def sent_processor(sent,complete=True):\n",
        "     tokens=[]\n",
        "     if complete: tokens.append(nlp(\"BEGINNING\")[0])  # Add \"BEGINNING\" at the start of each sentence\n",
        "     tokens.extend([token for token in sent])  # Add sentence tokens\n",
        "     if complete: tokens.append(nlp(\"END\")[0])  # Add \"END\" at the end of each sentence\n",
        "     for token in tokens:\n",
        "      if not token.is_space:\n",
        "       token_counter[token.text.lower()] += 1\n",
        "\n",
        "\n",
        "    with open(filepath, 'r') as file:\n",
        "\n",
        "\n",
        "        for line in file:\n",
        "            # Add line to buffer and process with spaCy\n",
        "            buffer += \" \" + line.strip()\n",
        "            doc = nlp(buffer)\n",
        "            # Extract complete sentences\n",
        "            sentences = list(doc.sents)\n",
        "            for i, sent in enumerate(sentences):\n",
        "                # If it's not the last sentence, we print it as it's complete\n",
        "                if i < len(sentences) - 1:\n",
        "                    print(sent)\n",
        "                    sent_processor(sent)\n",
        "                else:\n",
        "                    # If it's the last sentence, store it in the buffer in case it's incomplete\n",
        "                    buffer = sent.text\n",
        "                    # Process sentences and identify complete sentences\n",
        "            for sent in doc.sents:\n",
        "                if sent.end_char < len(buffer):\n",
        "                    print(sent)\n",
        "                    sent_processor(sent)\n",
        "\n",
        "        # Process any remaining content in the buffer\n",
        "        doc = nlp(buffer)\n",
        "        for sent in doc.sents:\n",
        "         print(sent)\n",
        "         sent_processor(sent)\n",
        "\n",
        "    return token_counter\n",
        "\n",
        "# Load spaCy model for tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class VocabularyBuilder:\n",
        "    def __init__(self, max_voc_size=None):\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.str_to_int = {}\n",
        "        self.int_to_str = {}\n",
        "        self.special_tokens = [\"BEGINNING\", \"END\", \"UNKNOWN\"]\n",
        "\n",
        "    def build_vocabulary(self, filepath, nlp,token_counter_savepath=None,token_counter_loadpath=None,encoding=\"utf-8\"):\n",
        "\n",
        "        # Tokenize text and count tokens\n",
        "        if token_counter_loadpath is not None:\n",
        "         with open(token_counter_loadpath, \"r\") as file:\n",
        "            token_counter = Counter(json.load(file))\n",
        "        else:\n",
        "         token_counter =get_token_counter(filepath=filepath,nlp=nlp,encoding=encoding)\n",
        "\n",
        "        # Start vocabulary with special tokens\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Select the most common tokens, considering max_voc_size - len(special_tokens)\n",
        "        if self.max_voc_size is None:\n",
        "            max_words = len(token_counter) - len(self.special_tokens)\n",
        "            self.max_voc_size = max_words + len(self.special_tokens)\n",
        "        else:\n",
        "         max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = token_counter.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Save to a JSON file\n",
        "        if token_counter_savepath is not None:\n",
        "         with open(token_counter_savepath, \"w\") as file:\n",
        "             json.dump(token_counter, file)\n",
        "\n",
        "\n",
        "    def get_token_id(self, token):\n",
        "        # Return the integer ID for a given token\n",
        "        return self.str_to_int.get(token.lower(), self.str_to_int[\"UNKNOWN\"])\n",
        "\n",
        "    def get_token_str(self, token_id):\n",
        "        # Return the original token string for a given integer ID\n",
        "        return self.int_to_str.get(token_id, \"UNKNOWN\")\n",
        "\n",
        "    def add_special_tokens_to_text(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes the text by sentence and adds special 'BEGINNING' and 'END' tokens\n",
        "        around each sentence.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input text.\n",
        "\n",
        "        Returns:\n",
        "        - List[str]: A list of tokens with special 'BEGINNING' and 'END' tokens added.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def sanity_check(self):\n",
        "        # Check vocabulary size\n",
        "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
        "\n",
        "        # Check special tokens exist and are unique\n",
        "        for token in self.special_tokens:\n",
        "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
        "\n",
        "        # Check if highly frequent words are included and rare ones are not\n",
        "        common_words = [\"the\", \"and\"]\n",
        "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
        "\n",
        "        for word in common_words:\n",
        "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
        "\n",
        "        for word in rare_words:\n",
        "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
        "\n",
        "        # Check that mapping back and forth works for a test word\n",
        "        test_word = \"The\"\n",
        "        token_id = self.get_token_id(test_word)\n",
        "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
        "\n",
        "        print(\"Sanity check passed!\")\n",
        "\n",
        "token_counter_filepath=\"token_counter.json\"\n",
        "vocab_builder = VocabularyBuilder()\n",
        "vocab_builder.build_vocabulary(filepath=example_filepath, nlp=nlp,token_counter_savepath=token_counter_filepath)\n",
        "\n",
        "# Example mappings\n",
        "print(\"str_to_int:\", vocab_builder.str_to_int)\n",
        "print(\"int_to_str:\", vocab_builder.int_to_str)\n",
        "\n",
        "# Convert a token to integer ID and back to string\n",
        "token_id = vocab_builder.get_token_id(\"example\")\n",
        "print(\"Token ID for 'example':\", token_id)\n",
        "print(\"Original token from ID:\", vocab_builder.get_token_str(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZJ4k7STz96H",
        "outputId": "b2fc4318-1df7-4835-a3ba-ecbd9c4bcb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Anatomy Anatomy (Greek anatomē, “dissection”) is the branch of biology concerned with the study of the structure of organisms and their parts.  \n",
            "Anatomy is a branch of natural science dealing with the structural organization of living things.  \n",
            "It is an old science, having its beginnings in prehistoric times.  \n",
            "Anatomy is inherently tied to embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and long (evolution) timescales.  \n",
            "Human anatomy is one of the basic essential sciences of medicine.\n",
            "The discipline of anatomy is divided into macroscopic and microscopic anatomy.  \n",
            "Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight.  \n",
            "Gross anatomy also includes the branch of superficial anatomy.  \n",
            "Human anatomy is one of the basic essential sciences of medicine.\n",
            "The discipline of anatomy is divided into macroscopic and microscopic anatomy.  \n",
            "Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.\n",
            "str_to_int: {'BEGINNING': 0, 'END': 1, 'UNKNOWN': 2, 'anatomy': 3, 'of': 4, 'the': 5, 'beginning': 6, '.': 7, 'end': 8, ',': 9, 'is': 10, 'and': 11, '(': 12, ')': 13, 'branch': 14, 'study': 15, 'in': 16, 'macroscopic': 17, 'microscopic': 18, 'biology': 19, 'with': 20, 'parts': 21, 'science': 22, 'an': 23, 'embryology': 24, 'as': 25, 'human': 26, 'one': 27, 'basic': 28, 'essential': 29, 'sciences': 30, 'medicine': 31, 'discipline': 32, 'divided': 33, 'into': 34, 'gross': 35, 'also': 36, 'greek': 37, 'anatomē': 38, '“': 39, 'dissection': 40, '”': 41, 'concerned': 42, 'structure': 43, 'organisms': 44, 'their': 45, 'a': 46, 'natural': 47, 'dealing': 48, 'structural': 49, 'organization': 50, 'living': 51, 'things': 52, 'it': 53, 'old': 54, 'having': 55, 'its': 56, 'beginnings': 57, 'prehistoric': 58, 'times': 59, 'inherently': 60, 'tied': 61, 'to': 62, 'comparative': 63, 'evolutionary': 64, 'phylogeny': 65, 'these': 66, 'are': 67, 'processes': 68, 'by': 69, 'which': 70, 'generated': 71, 'over': 72, 'immediate': 73, 'long': 74, 'evolution': 75, 'timescales': 76, 'or': 77, 'examination': 78, 'animal': 79, \"'s\": 80, 'body': 81, 'using': 82, 'unaided': 83, 'eyesight': 84, 'includes': 85, 'superficial': 86, 'involves': 87, 'use': 88, 'optical': 89, 'instruments': 90, 'tissues': 91, 'various': 92, 'structures': 93}\n",
            "int_to_str: {0: 'BEGINNING', 1: 'END', 2: 'UNKNOWN', 3: 'anatomy', 4: 'of', 5: 'the', 6: 'beginning', 7: '.', 8: 'end', 9: ',', 10: 'is', 11: 'and', 12: '(', 13: ')', 14: 'branch', 15: 'study', 16: 'in', 17: 'macroscopic', 18: 'microscopic', 19: 'biology', 20: 'with', 21: 'parts', 22: 'science', 23: 'an', 24: 'embryology', 25: 'as', 26: 'human', 27: 'one', 28: 'basic', 29: 'essential', 30: 'sciences', 31: 'medicine', 32: 'discipline', 33: 'divided', 34: 'into', 35: 'gross', 36: 'also', 37: 'greek', 38: 'anatomē', 39: '“', 40: 'dissection', 41: '”', 42: 'concerned', 43: 'structure', 44: 'organisms', 45: 'their', 46: 'a', 47: 'natural', 48: 'dealing', 49: 'structural', 50: 'organization', 51: 'living', 52: 'things', 53: 'it', 54: 'old', 55: 'having', 56: 'its', 57: 'beginnings', 58: 'prehistoric', 59: 'times', 60: 'inherently', 61: 'tied', 62: 'to', 63: 'comparative', 64: 'evolutionary', 65: 'phylogeny', 66: 'these', 67: 'are', 68: 'processes', 69: 'by', 70: 'which', 71: 'generated', 72: 'over', 73: 'immediate', 74: 'long', 75: 'evolution', 76: 'timescales', 77: 'or', 78: 'examination', 79: 'animal', 80: \"'s\", 81: 'body', 82: 'using', 83: 'unaided', 84: 'eyesight', 85: 'includes', 86: 'superficial', 87: 'involves', 88: 'use', 89: 'optical', 90: 'instruments', 91: 'tissues', 92: 'various', 93: 'structures'}\n",
            "Token ID for 'example': 2\n",
            "Original token from ID: UNKNOWN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_counter_filepath=\"token_counter.json\"\n",
        "vocab_builder = VocabularyBuilder()\n",
        "vocab_builder.build_vocabulary(filepath=example_filepath, nlp=nlp,token_counter_loadpath=token_counter_filepath)\n",
        "\n",
        "# Example mappings\n",
        "print(\"str_to_int:\", vocab_builder.str_to_int)\n",
        "print(\"int_to_str:\", vocab_builder.int_to_str)\n",
        "\n",
        "# Convert a token to integer ID and back to string\n",
        "token_id = vocab_builder.get_token_id(\"example\")\n",
        "print(\"Token ID for 'example':\", token_id)\n",
        "print(\"Original token from ID:\", vocab_builder.get_token_str(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA1UytBVGKT8",
        "outputId": "44ca5fd9-327d-4f61-a6d9-c92c3f5a6845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "str_to_int: {'BEGINNING': 0, 'END': 1, 'UNKNOWN': 2, 'anatomy': 3, 'of': 4, 'the': 5, 'beginning': 6, '.': 7, 'end': 8, ',': 9, 'is': 10, 'and': 11, '(': 12, ')': 13, 'branch': 14, 'study': 15, 'in': 16, 'macroscopic': 17, 'microscopic': 18, 'biology': 19, 'with': 20, 'parts': 21, 'science': 22, 'an': 23, 'embryology': 24, 'as': 25, 'human': 26, 'one': 27, 'basic': 28, 'essential': 29, 'sciences': 30, 'medicine': 31, 'discipline': 32, 'divided': 33, 'into': 34, 'gross': 35, 'also': 36, 'greek': 37, 'anatomē': 38, '“': 39, 'dissection': 40, '”': 41, 'concerned': 42, 'structure': 43, 'organisms': 44, 'their': 45, 'a': 46, 'natural': 47, 'dealing': 48, 'structural': 49, 'organization': 50, 'living': 51, 'things': 52, 'it': 53, 'old': 54, 'having': 55, 'its': 56, 'beginnings': 57, 'prehistoric': 58, 'times': 59, 'inherently': 60, 'tied': 61, 'to': 62, 'comparative': 63, 'evolutionary': 64, 'phylogeny': 65, 'these': 66, 'are': 67, 'processes': 68, 'by': 69, 'which': 70, 'generated': 71, 'over': 72, 'immediate': 73, 'long': 74, 'evolution': 75, 'timescales': 76, 'or': 77, 'examination': 78, 'animal': 79, \"'s\": 80, 'body': 81, 'using': 82, 'unaided': 83, 'eyesight': 84, 'includes': 85, 'superficial': 86, 'involves': 87, 'use': 88, 'optical': 89, 'instruments': 90, 'tissues': 91, 'various': 92, 'structures': 93}\n",
            "int_to_str: {0: 'BEGINNING', 1: 'END', 2: 'UNKNOWN', 3: 'anatomy', 4: 'of', 5: 'the', 6: 'beginning', 7: '.', 8: 'end', 9: ',', 10: 'is', 11: 'and', 12: '(', 13: ')', 14: 'branch', 15: 'study', 16: 'in', 17: 'macroscopic', 18: 'microscopic', 19: 'biology', 20: 'with', 21: 'parts', 22: 'science', 23: 'an', 24: 'embryology', 25: 'as', 26: 'human', 27: 'one', 28: 'basic', 29: 'essential', 30: 'sciences', 31: 'medicine', 32: 'discipline', 33: 'divided', 34: 'into', 35: 'gross', 36: 'also', 37: 'greek', 38: 'anatomē', 39: '“', 40: 'dissection', 41: '”', 42: 'concerned', 43: 'structure', 44: 'organisms', 45: 'their', 46: 'a', 47: 'natural', 48: 'dealing', 49: 'structural', 50: 'organization', 51: 'living', 52: 'things', 53: 'it', 54: 'old', 55: 'having', 56: 'its', 57: 'beginnings', 58: 'prehistoric', 59: 'times', 60: 'inherently', 61: 'tied', 62: 'to', 63: 'comparative', 64: 'evolutionary', 65: 'phylogeny', 66: 'these', 67: 'are', 68: 'processes', 69: 'by', 70: 'which', 71: 'generated', 72: 'over', 73: 'immediate', 74: 'long', 75: 'evolution', 76: 'timescales', 77: 'or', 78: 'examination', 79: 'animal', 80: \"'s\", 81: 'body', 82: 'using', 83: 'unaided', 84: 'eyesight', 85: 'includes', 86: 'superficial', 87: 'involves', 88: 'use', 89: 'optical', 90: 'instruments', 91: 'tissues', 92: 'various', 93: 'structures'}\n",
            "Token ID for 'example': 2\n",
            "Original token from ID: UNKNOWN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sanity check\n",
        "vocab_builder.sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sB4W8Q0Qo9",
        "outputId": "26f9b735-b845-4759-a9cf-f2d0641f65e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "class TrainingDataPreparer:\n",
        "    def __init__(self, vocab_builder, context_window_size=3):\n",
        "        self.vocab_builder = vocab_builder\n",
        "        self.context_window_size = context_window_size\n",
        "        self.nlp = spacy.blank(\"en\")  # Use a blank model for tokenization\n",
        "\n",
        "    def encode_token(self, token):\n",
        "        return self.vocab_builder.get_token_id(token.text)\n",
        "\n",
        "    def process_line(self, line, add_beginning_padding=False):\n",
        "        tokens = [self.nlp(token_text) for token_text in line.split()]\n",
        "        token_ids = [self.encode_token(token) for token in tokens]\n",
        "\n",
        "        if add_beginning_padding:\n",
        "            # Add N \"BEGINNING\" tokens at the start only for the first line\n",
        "            padded_tokens = [self.vocab_builder.get_token_id(\"BEGINNING\")] * self.context_window_size + token_ids\n",
        "        else:\n",
        "            # For subsequent lines, no BEGINNING padding is added\n",
        "            padded_tokens = token_ids\n",
        "\n",
        "        sequences = []\n",
        "        for i in range(len(padded_tokens) - self.context_window_size):\n",
        "            context = padded_tokens[i:i + self.context_window_size]\n",
        "            target = padded_tokens[i + self.context_window_size]\n",
        "            sequences.append(context + [target])\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def prepare_training_data(self, input_file, output_file):\n",
        "        with open(input_file, \"r\") as infile, open(output_file, \"w\", newline=\"\") as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([f\"Token_{i+1}\" for i in range(self.context_window_size)] + [\"Target\"])\n",
        "\n",
        "            first_line = True\n",
        "            buffer = \"\"\n",
        "            for line in infile:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                # Add \"BEGINNING\" padding only for the first line\n",
        "                add_beginning_padding = first_line\n",
        "                first_line = False  # After the first line, we stop adding \"BEGINNING\" padding\n",
        "\n",
        "                buffer += \" \" + line\n",
        "                doc = self.nlp(buffer)\n",
        "\n",
        "                sentences = list(doc.sents)\n",
        "                for i, sent in enumerate(sentences):\n",
        "                    # Process each sentence independently\n",
        "                    sentence_text = sent.text.strip()\n",
        "                    if sentence_text:  # Only process non-empty sentences\n",
        "                        sequences = self.process_line(sentence_text, add_beginning_padding)\n",
        "                        for sequence in sequences:\n",
        "                            writer.writerow(sequence)\n",
        "\n",
        "                    # If this is not the last sentence, we continue to the next sentence\n",
        "                    if i < len(sentences) - 1:\n",
        "                        buffer = sent.text\n",
        "                    else:\n",
        "                        buffer = \"\"\n",
        "\n",
        "        print(\"Training data preparation complete.\")\n"
      ],
      "metadata": {
        "id": "uXLrr6YeF0AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_window_size = 3\n",
        "data_preparer = TrainingDataPreparer(vocab_builder=vocab_builder, context_window_size=3)\n",
        "data_preparer.prepare_training_data(input_file=example_filepath, output_file=\"training_sequences.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "3A5cX52Y73i1",
        "outputId": "c4dcbd44-c1b6-4870-e7f6-7031c7458c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-891e3f36cb87>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontext_window_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_preparer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingDataPreparer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_builder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_builder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_window_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_preparer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training_sequences.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-67373520cd33>\u001b[0m in \u001b[0;36mprepare_training_data\u001b[0;34m(self, input_file, output_file)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;31m# Process each sentence independently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[0;32m/content/Intro-to-language-modeling/Intro-to-language-modeling/spacy/tokens/doc.pyx\u001b[0m(926)\u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\n",
            "--KeyboardInterrupt--\n",
            "\n",
            "KeyboardInterrupt: Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=pd.read_csv(\"training_sequences.csv\")\n",
        "print(training_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXru39twOD3D",
        "outputId": "7ca85d33-0933-4a27-f390-3f878e80a6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Token_1  Token_2  Token_3  Target\n",
            "0        6        6        6       3\n",
            "1        6        6        3       8\n",
            "2        3        2        2       2\n",
            "3        2        2        2      10\n",
            "4        2        2       10       5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a tensor dataset ##\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def TorchDataLoader(training_sequences, batch_size):\n",
        "  context_words = [item[0] for item in training_sequences]  # List of [context]\n",
        "  target_words = [item[1] for item in training_sequences]   # List of target words\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  context_tensor = torch.tensor(context_words, dtype=torch.long)  # Shape: (num_samples, 3)\n",
        "  target_tensor = torch.tensor(target_words, dtype=torch.long)    # Shape: (num_samples,)\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(context_tensor, target_tensor)\n",
        "\n",
        "  # Create a DataLoader for batching\n",
        "  batch_size = 4\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Iterate over the DataLoader\n",
        "  for batch_context, batch_target in dataloader:\n",
        "      print(\"Batch context:\", batch_context)\n",
        "      print(\"Batch target:\", batch_target)\n",
        "      # You can now use batch_context and batch_target for model training\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "aM8fZiApXo14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class SimpleANN(nn.Module):\n",
        "\n",
        "    def __init__(self,layer_sizes,activation=nn.ReLU,last_layer_activation=nn.ReLU,dropout=0):\n",
        "\n",
        "        super(SimpleANN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes)-2):\n",
        "          self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "          self.layers.append(nn.Dropout(dropout))\n",
        "          self.layers.append(activation())\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "        if last_layer_activation is not None:\n",
        "         self.layers.append(nn.Dropout(dropout))\n",
        "         self.layers.append(last_layer_activation())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, np.prod(x.shape[1:])) # Flatten the input\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RFgjvuVxXyh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "def dense_arch_builder(input_size,scale_factor=0,hidden_layers_num=0,repeat=0,output_size=1):\n",
        "  layer_sizes=[input_size]\n",
        "\n",
        "  if scale_factor!=0:\n",
        "\n",
        "   if scale_factor>1:\n",
        "    for i in range(hidden_layers_num):\n",
        "     layer_sizes.append(layer_sizes[-1]*scale_factor)\n",
        "    while layer_sizes[-1]<output_size:\n",
        "     layer_sizes.append(layer_sizes[-1]*scale_factor)\n",
        "\n",
        "   elif scale_factor==1:\n",
        "     for i in range(2,hidden_layers_num+2):\n",
        "      layer_sizes.append(layer_sizes[0]*i)\n",
        "     i+=1\n",
        "     while layer_sizes[-1]<output_size:\n",
        "      layer_sizes.append(layer_sizes[0]*i)\n",
        "      i+=1\n",
        "\n",
        "   mirrored_layer_sizes=deepcopy(layer_sizes)\n",
        "   mirrored_layer_sizes.reverse()\n",
        "   mirrored_layer_sizes=mirrored_layer_sizes[1:-1]\n",
        "\n",
        "   for i in range(repeat):\n",
        "    layer_sizes.append(layer_sizes[-1])\n",
        "\n",
        "   if output_size>0:\n",
        "    layer_sizes+=mirrored_layer_sizes\n",
        "    downscale_factor=scale_factor if scale_factor>1 else 2\n",
        "\n",
        "    while layer_sizes[-1]!=output_size:\n",
        "     if layer_sizes[-1]//downscale_factor>=output_size:\n",
        "      layer_sizes.append(layer_sizes[-1]//downscale_factor)\n",
        "     else:\n",
        "      layer_sizes.append(output_size)\n",
        "\n",
        "  else:\n",
        "    downscale_factor = (input_size / output_size) ** (1 / hidden_layers_num)\n",
        "    for i in range(hidden_layers_num):\n",
        "        layer_sizes.append(int(input_size / (downscale_factor ** i)))\n",
        "    layer_sizes.append(output_size)\n",
        "\n",
        "  return layer_sizes\n"
      ],
      "metadata": {
        "id": "YZ2hnRbQX-bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to plot the training metrics\n",
        "\n",
        "def plot_training_metrics(train_acc, val_acc, train_loss, title, save_path):\n",
        "    # Ensure that all input lists have the same length\n",
        "    assert len(train_acc) == len(val_acc) == len(train_loss), \"All input histories must have the same length.\"\n",
        "\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    # Create the metrics DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Accuracy (%)': train_acc,\n",
        "        'Validation Accuracy (%)': val_acc,\n",
        "        'Training Loss': train_loss\n",
        "    })\n",
        "\n",
        "    # Initialize the plot\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Training and Validation Accuracy on ax1\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy (%)', color=color)\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Training Accuracy (%)'], label='Train Acc', color='tab:blue')\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Validation Accuracy (%)'], label='Val Acc', color='tab:cyan')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Create a second y-axis for Training Loss\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Train Loss', color='tab:red')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
        "\n",
        "    # Set plot title and layout\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save and display the plot\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aaXic0GgbFbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLDeM3fI2AWNLl+Rce4JyC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}