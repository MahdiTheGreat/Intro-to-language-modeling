{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgzPiclr4Cun"
      },
      "source": [
        "# Step 0: Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.datasets import fetch_20newsgroups # We use the 20 news groups text dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "S471E7Q86cPH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW53Mx7p3LqE"
      },
      "source": [
        "# Step 1: Fetching data and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qZHLDZSB2iOq"
      },
      "outputs": [],
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups_train.target_names)"
      ],
      "metadata": {
        "id": "-nLT1si_CqT4",
        "outputId": "b8a479d0-fff1-4110-b510-5c6ac165435b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into smaller training sets in percentage\n",
        "percentage = 0.2\n",
        "split_index = int(len(newsgroups_train.data) * percentage)\n",
        "train_data_small = newsgroups_train.data[:split_index]\n",
        "train_targets_small = newsgroups_train.target[:split_index]"
      ],
      "metadata": {
        "id": "ejgOpsbANmyk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data_small))"
      ],
      "metadata": {
        "id": "wv0CKmXAOKnx",
        "outputId": "3e6e0722-3599-47b8-bef7-a6b124f0b2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHCCZWe3b9X",
        "outputId": "e41d1a3e-e153-41b2-8416-32eb138cd0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Processing Articles: 100%|██████████| 2262/2262 [00:08<00:00, 267.94it/s]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Tokenizing and removing stop words\n",
        "filtered_train = [[]] * len(train_data_small) # First index is the article\n",
        "flattened_train = []\n",
        "for i, article in tqdm(enumerate(train_data_small), total=len(train_data_small), desc=\"Processing Articles\"):\n",
        "    word_tokens = word_tokenize(article)\n",
        "\n",
        "    text = [w for w in word_tokens if w.lower() not in stop_words]\n",
        "\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            flattened_train.append(w)\n",
        "            filtered_train[i].append(w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(flattened_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04gc2uRrTqtf",
        "outputId": "8685e8f1-858d-49bf-fe1d-f44e8f018f56"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "761817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = Counter(flattened_train)"
      ],
      "metadata": {
        "id": "Ku3E7_9eW93G"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract low-frequency words (occurrence <= 10) into a set\n",
        "low_frequency_words = {word for word, count in word_count.items() if count <= 10}"
      ],
      "metadata": {
        "id": "vU3wP-jEL1u1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter articles efficiently using set operations\n",
        "filtered_hf_train = []\n",
        "for article in tqdm(filtered_train, desc=\"Removing LF words\"):\n",
        "    article_set = set(article)\n",
        "    filtered_article = list(article_set - low_frequency_words)\n",
        "    filtered_hf_train.append(filtered_article)\n",
        "\n"
      ],
      "metadata": {
        "id": "2Q7WUkGCFsIX",
        "outputId": "3c393bea-3ece-4292-a210-d27f9dff6b05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing LF words: 100%|██████████| 2262/2262 [02:03<00:00, 18.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_train = [word for word in flattened_train if word not in low_frequency_words]"
      ],
      "metadata": {
        "id": "NLmxtbMSQNjs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(flattened_train))"
      ],
      "metadata": {
        "id": "moc_V6EYQd29",
        "outputId": "d2bad6a3-381c-4c73-b4c3-9dfbe930d62b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "620610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(flattened_train[0])"
      ],
      "metadata": {
        "id": "i1ejAGOMR36r",
        "outputId": "406f6acf-73b9-4f66-95c2-ff484caa8dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Gibbs sampling"
      ],
      "metadata": {
        "id": "8X4pGSix73wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gibbs_sampling(data, alpha, beta, K, iterations = 150):\n",
        "  q = np.zeros(K)\n",
        "  p = np.zeros(K)\n",
        "  z = np.zeros(len(data))\n",
        "  for i in tqdm(iterations):\n",
        "    # Start with arbitrary z:\n",
        "    z = data[0][0]\n",
        "\n",
        "    # Iterate sequentially through the data:\n",
        "    for j in data[:]:\n",
        "      for d in data[j][:]:\n",
        "        for k in range(K):\n",
        "          q[k] = (alpha +\n",
        "          p[k] = q[k]/sum(q)\n",
        "        # Update z[d][j]\n",
        "        z[d][j] =\n"
      ],
      "metadata": {
        "id": "U85ycesL76R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First parameter combo\n",
        "gibbs_sampling(filtered_train, alpha = 0.1, beta = 0.1, K=10)"
      ],
      "metadata": {
        "id": "QaMPOEWe8FB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second parameter combo\n",
        "gibbs_sampling(filtered_train, alpha = 0.01, beta = 0.01, K=10)"
      ],
      "metadata": {
        "id": "8vaUYQRL8FRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Third parameter combo\n",
        "gibbs_sampling(filtered_train, alpha = 0.1, beta = 0.1, K=50)"
      ],
      "metadata": {
        "id": "Wz35TUsz8Uxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fourth parameter combo\n",
        "gibbs_sampling(filtered_train, alpha = 0.01, beta = 0.01, K=50)"
      ],
      "metadata": {
        "id": "4E47G0_B8Wa0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}