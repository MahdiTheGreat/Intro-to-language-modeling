{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgzPiclr4Cun"
      },
      "source": [
        "# Step 0: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S471E7Q86cPH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.datasets import fetch_20newsgroups # We use the 20 news groups text dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW53Mx7p3LqE"
      },
      "source": [
        "# Step 1: Fetching data and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZHLDZSB2iOq"
      },
      "outputs": [],
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nLT1si_CqT4",
        "outputId": "62cf3f43-1505-4e80-feb0-5af29ab84904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "print(len(newsgroups_train.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejgOpsbANmyk"
      },
      "outputs": [],
      "source": [
        "# Split into smaller training sets in percentage\n",
        "percentage = 0.8\n",
        "split_index = int(len(newsgroups_train.data) * percentage)\n",
        "train_data_small = newsgroups_train.data[:split_index]\n",
        "train_targets_small = newsgroups_train.target[:split_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv0CKmXAOKnx",
        "outputId": "59f8e694-9e7d-40fa-af0e-a28a29b55e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_data_small[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzPiBMTD7gTK",
        "outputId": "9856616f-f32a-4646-e4d9-942c05051c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n"
          ]
        }
      ],
      "source": [
        "def extract_email_body(email):\n",
        "    # Split on the first blank line\n",
        "    parts = email.split(\"\\n\\n\", 1)\n",
        "    if len(parts) > 1:\n",
        "        body = parts[1]\n",
        "        # Remove footer if it exists\n",
        "        body = re.split(r\"\\n\\s*-{2,}\\s*\", body)[0]\n",
        "        return body.strip()\n",
        "    return \"\"\n",
        "\n",
        "removed_headers = extract_email_body(train_data_small[0])\n",
        "print(removed_headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHCCZWe3b9X",
        "outputId": "b6f10aa3-9c59-4c07-f3e7-e62a4e92bd53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Articles\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9051/9051 [00:10<00:00, 843.86it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique words: 109209\n",
            "Word-to-ID mapping example: {'\\x03': 0, '\\x03\\x03\\x1b': 1, '\\x1a': 2, '!': 3, '#': 4}\n",
            "Integer Corpus example (first article): []\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize structures for the preprocessed corpus\n",
        "filtered_train = [[] for _ in range(len(train_data_small))]  # Preprocessed articles\n",
        "flattened_train = []  # A single list of all words in the corpus\n",
        "\n",
        "# Tokenizing and removing stopwords\n",
        "print(\"Processing Articles\")\n",
        "for i, article in tqdm(enumerate(train_data_small), total=len(train_data_small)):\n",
        "    article_body = extract_email_body(article) # Only use body and remove headers and footers\n",
        "    word_tokens = word_tokenize(article_body)  # Tokenize article\n",
        "    # Remove stop words and add to both filtered_train and flattened_train\n",
        "    filtered_words = [w.lower() for w in word_tokens if w.lower() not in stop_words]\n",
        "    filtered_train[i] = filtered_words\n",
        "    flattened_train.extend(filtered_words)\n",
        "\n",
        "# Create a vocabulary mapping\n",
        "unique_words = sorted(set(flattened_train))  # Get unique words\n",
        "word_to_id = {word: idx for idx, word in enumerate(unique_words)}  # Map word to ID\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}  # Reverse mapping\n",
        "\n",
        "# Map the filtered articles to integer IDs\n",
        "int_corpus = [[word_to_id[word] for word in article] for article in filtered_train]\n",
        "\n",
        "# Display mappings and a small example\n",
        "print(f\"Total unique words: {len(unique_words)}\")\n",
        "print(\"Word-to-ID mapping example:\", {k: word_to_id[k] for k in list(word_to_id)[:5]})\n",
        "print(\"Integer Corpus example (first article):\", int_corpus[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku3E7_9eW93G"
      },
      "outputs": [],
      "source": [
        "word_count = Counter(flattened_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3wP-jEL1u1"
      },
      "outputs": [],
      "source": [
        "# Extract low-frequency words (occurrence <= 10) into a set\n",
        "low_frequency_words = {word for word, count in word_count.items() if count <= 10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q7WUkGCFsIX",
        "outputId": "77a8de04-68af-4ef2-9fbc-d3b320f561b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Removing LF words: 100%|██████████| 9051/9051 [00:00<00:00, 80738.78it/s]\n"
          ]
        }
      ],
      "source": [
        "# Filter articles efficiently using set operations\n",
        "corpus_hf = []\n",
        "for article in tqdm(int_corpus, desc=\"Removing LF words\"):\n",
        "    article_set = set(article)\n",
        "    filtered_article = list(article_set - low_frequency_words)\n",
        "    corpus_hf.append(filtered_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLmxtbMSQNjs",
        "outputId": "c81e98d4-4400-4051-e797-5a650e0995a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in corpus 1550053\n",
            "Vocabulary size after removing LF words 11537\n"
          ]
        }
      ],
      "source": [
        "flattened_train = [word for word in flattened_train if word not in low_frequency_words]\n",
        "voc_size = len(sorted(set(flattened_train)))\n",
        "print(\"Number of words in corpus\", len(flattened_train))\n",
        "print(\"Vocabulary size after removing LF words\", voc_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4pGSix73wx"
      },
      "source": [
        "# Step 2: Gibbs sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U85ycesL76R8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def lda_gibbs_sampling(corpus, K, alpha, beta, iterations):\n",
        "    \"\"\"\n",
        "    Implements Collapsed Gibbs Sampling for LDA.\n",
        "\n",
        "    :param corpus: List of lists, where each inner list contains word IDs in a document.\n",
        "    :param K: Number of topics.\n",
        "    :param alpha: Dirichlet prior for document-topic distribution.\n",
        "    :param beta: Dirichlet prior for topic-word distribution.\n",
        "    :param iterations: Number of Gibbs sampling iterations.\n",
        "    :return: Topic assignments, document-topic counts, topic-word counts, topic totals.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    D = len(corpus)  # Number of documents\n",
        "    V = len(list(set(word for doc in corpus for word in doc)))\n",
        "\n",
        "    # Count matrices\n",
        "    ndk = np.zeros((D, K))  # Document-topic counts\n",
        "    nkw = np.zeros((K, V))  # Topic-word counts\n",
        "    nk = np.zeros(K)        # Total words in each topic\n",
        "\n",
        "    # Topic assignments for each word\n",
        "    z = []  # Topic assignment for each word in corpus\n",
        "    for d, doc in enumerate(corpus):\n",
        "        doc_topics = []\n",
        "        for word in doc:\n",
        "            topic = np.random.randint(K)  # Randomly assign a topic\n",
        "            doc_topics.append(topic)\n",
        "            ndk[d, topic] += 1\n",
        "            nkw[topic, word] += 1\n",
        "            nk[topic] += 1\n",
        "        z.append(doc_topics)\n",
        "\n",
        "    # Gibbs sampling\n",
        "    for _ in tqdm(range(iterations)):\n",
        "        for d, doc in enumerate(corpus):\n",
        "            for i, word in enumerate(doc):\n",
        "                current_topic = z[d][i]\n",
        "\n",
        "                # Decrement counts\n",
        "                ndk[d, current_topic] -= 1\n",
        "                nkw[current_topic, word] -= 1\n",
        "                nk[current_topic] -= 1\n",
        "\n",
        "                # Compute topic probabilities (Maybe do a for loop here instead)\n",
        "                topic_probs = (ndk[d] + alpha) * (nkw[:, word] + beta) / (nk + beta * V)\n",
        "                topic_probs /= np.sum(topic_probs)  # Normalize\n",
        "\n",
        "                # Sample new topic\n",
        "                new_topic = np.random.choice(K, p=topic_probs)\n",
        "                z[d][i] = new_topic\n",
        "\n",
        "                # Increment counts\n",
        "                ndk[d, new_topic] += 1\n",
        "                nkw[new_topic, word] += 1\n",
        "                nk[new_topic] += 1\n",
        "\n",
        "    return z, ndk, nkw, nk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMPOEWe8FB1",
        "outputId": "6c6cade7-0f60-44c0-828d-b3c7b352286c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [1:03:52<00:00, 19.16s/it]\n"
          ]
        }
      ],
      "source": [
        "# Initialize data\n",
        "corpus = corpus_hf.copy()\n",
        "targets = train_targets_small.copy()\n",
        "topics = newsgroups_train.target_names.copy()\n",
        "\n",
        "# First parameter combo\n",
        "z, ndk, nkw, nk = lda_gibbs_sampling(corpus, alpha = 0.1, beta = 0.1, K=len(topics), iterations=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKX8MdD3cGWr",
        "outputId": "8d433676-bc1b-4008-be15-27d577921be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9051, 20) (20, 109209) (20,)\n"
          ]
        }
      ],
      "source": [
        "print(ndk.shape, nkw.shape, nk.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AxH9Ge6fQIv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def get_top_words(nkw, id_to_word, top_n=20, method=\"raw\", beta=0.1):\n",
        "    \"\"\"\n",
        "    Get the top words for each topic.\n",
        "\n",
        "    :param nkw: Topic-word counts (K x V matrix).\n",
        "    :param id_to_word: Dictionary mapping word IDs to their original words.\n",
        "    :param top_n: Number of top words to retrieve per topic.\n",
        "    :param method: \"raw\" for raw counts, \"relative\" for relative frequencies.\n",
        "    :param beta: Dirichlet prior for smoothing (used in relative frequency).\n",
        "    :return: Dictionary of top words for each topic.\n",
        "    \"\"\"\n",
        "    K, V = nkw.shape\n",
        "    top_words_per_topic = {}\n",
        "\n",
        "    if method == \"raw\":\n",
        "        # Use raw counts\n",
        "        for k in range(K):\n",
        "            top_word_indices = np.argsort(nkw[k, :])[::-1][:top_n]  # Top N words by count\n",
        "            top_words_per_topic[k] = [id_to_word[idx] for idx in top_word_indices]\n",
        "\n",
        "    elif method == \"relative\":\n",
        "        # Compute relative frequencies\n",
        "        word_totals = np.sum(nkw, axis=0)  # Total count of each word across all topics\n",
        "        for k in range(K):\n",
        "            relative_freqs = (nkw[k, :] + beta) / (word_totals + beta * K)  # Smoothed relative frequency\n",
        "            top_word_indices = np.argsort(relative_freqs)[::-1][:top_n]  # Top N words by relative frequency\n",
        "            top_words_per_topic[k] = [\n",
        "                id_to_word[idx] for idx in top_word_indices\n",
        "            ]\n",
        "\n",
        "    return top_words_per_topic\n",
        "\n",
        "\n",
        "def top_words_to_df(top_words_per_topic):\n",
        "    \"\"\"\n",
        "    Display the top words for each topic in a table format.\n",
        "\n",
        "    :param top_words_per_topic: Dictionary of top words for each topic.\n",
        "    :param method: Description of the method used (\"raw\" or \"relative\").\n",
        "    \"\"\"\n",
        "    df_top_words = pd.DataFrame.from_dict(top_words_per_topic)\n",
        "    df_top_words.columns = [f\"Topic {i+1}\" for i in range(df_top_words.shape[1])]\n",
        "    return df_top_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBiSOQR0cZ0g",
        "outputId": "7b7f000e-4e6b-4781-9f21-8e74e90ee2c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "      <th>Topic 4</th>\n",
              "      <th>Topic 5</th>\n",
              "      <th>Topic 6</th>\n",
              "      <th>Topic 7</th>\n",
              "      <th>Topic 8</th>\n",
              "      <th>Topic 9</th>\n",
              "      <th>Topic 10</th>\n",
              "      <th>Topic 11</th>\n",
              "      <th>Topic 12</th>\n",
              "      <th>Topic 13</th>\n",
              "      <th>Topic 14</th>\n",
              "      <th>Topic 15</th>\n",
              "      <th>Topic 16</th>\n",
              "      <th>Topic 17</th>\n",
              "      <th>Topic 18</th>\n",
              "      <th>Topic 19</th>\n",
              "      <th>Topic 20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>foods</td>\n",
              "      <td>bike</td>\n",
              "      <td>season</td>\n",
              "      <td>smtp</td>\n",
              "      <td>todamhyp</td>\n",
              "      <td>crime</td>\n",
              "      <td>christianity</td>\n",
              "      <td>1066</td>\n",
              "      <td>o\\</td>\n",
              "      <td>m\\</td>\n",
              "      <td>koufax</td>\n",
              "      <td>gmt</td>\n",
              "      <td>wagon</td>\n",
              "      <td>shuttle</td>\n",
              "      <td>cheaper</td>\n",
              "      <td>scsi</td>\n",
              "      <td>bezier</td>\n",
              "      <td>turkish</td>\n",
              "      <td>3.1</td>\n",
              "      <td>encryption</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>physician</td>\n",
              "      <td>bikes</td>\n",
              "      <td>nhl</td>\n",
              "      <td>sombody</td>\n",
              "      <td>charles.unlv.edu</td>\n",
              "      <td>weapons</td>\n",
              "      <td>bible</td>\n",
              "      <td>isu</td>\n",
              "      <td>\\x/</td>\n",
              "      <td>i5</td>\n",
              "      <td>36</td>\n",
              "      <td>apr</td>\n",
              "      <td>uokmax.ecn.uoknor.edu</td>\n",
              "      <td>orbit</td>\n",
              "      <td>heat</td>\n",
              "      <td>controller</td>\n",
              "      <td>3-</td>\n",
              "      <td>turks</td>\n",
              "      <td>microsoft</td>\n",
              "      <td>privacy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>geb</td>\n",
              "      <td>rider</td>\n",
              "      <td>hockey</td>\n",
              "      <td>jurgen</td>\n",
              "      <td>unlv.edu</td>\n",
              "      <td>waco</td>\n",
              "      <td>athos.rutgers.edu</td>\n",
              "      <td>promiscuous</td>\n",
              "      <td>m3</td>\n",
              "      <td>f-</td>\n",
              "      <td>vb30</td>\n",
              "      <td>fri</td>\n",
              "      <td>mustang</td>\n",
              "      <td>lunar</td>\n",
              "      <td>electrical</td>\n",
              "      <td>motherboard</td>\n",
              "      <td>ferdinand</td>\n",
              "      <td>armenia</td>\n",
              "      <td>motif</td>\n",
              "      <td>escrow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diet</td>\n",
              "      <td>yamaha</td>\n",
              "      <td>playoffs</td>\n",
              "      <td>jbotz</td>\n",
              "      <td>information/supplies</td>\n",
              "      <td>firearms</td>\n",
              "      <td>biblical</td>\n",
              "      <td>printf</td>\n",
              "      <td>/\\__/</td>\n",
              "      <td>d-</td>\n",
              "      <td>lowenstein</td>\n",
              "      <td>mwilson</td>\n",
              "      <td>convertible</td>\n",
              "      <td>spacecraft</td>\n",
              "      <td>circuit</td>\n",
              "      <td>ide</td>\n",
              "      <td>4-</td>\n",
              "      <td>armenians</td>\n",
              "      <td>x11r5</td>\n",
              "      <td>encrypted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cs.pitt.edu</td>\n",
              "      <td>riding</td>\n",
              "      <td>pitching</td>\n",
              "      <td>botz</td>\n",
              "      <td>1993apr19.205615.1013</td>\n",
              "      <td>guilty</td>\n",
              "      <td>atheists</td>\n",
              "      <td>exnet.iastate.edu</td>\n",
              "      <td>m+</td>\n",
              "      <td>ln</td>\n",
              "      <td>stankowitz</td>\n",
              "      <td>ncratl.atlantaga.ncr.com</td>\n",
              "      <td>compartment</td>\n",
              "      <td>nsmca</td>\n",
              "      <td>wire</td>\n",
              "      <td>simms</td>\n",
              "      <td>cusp</td>\n",
              "      <td>arab</td>\n",
              "      <td>widget</td>\n",
              "      <td>crypto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>diagnosed</td>\n",
              "      <td>motorcycle</td>\n",
              "      <td>braves</td>\n",
              "      <td>mtholyoke.edu</td>\n",
              "      <td>ey</td>\n",
              "      <td>assault</td>\n",
              "      <td>jesus</td>\n",
              "      <td>gay/bi</td>\n",
              "      <td>lq</td>\n",
              "      <td>mq</td>\n",
              "      <td>lafibm.lafayette.edu</td>\n",
              "      <td>thu</td>\n",
              "      <td>sedan</td>\n",
              "      <td>prb</td>\n",
              "      <td>circuits</td>\n",
              "      <td>quadra</td>\n",
              "      <td>kk</td>\n",
              "      <td>israeli</td>\n",
              "      <td>font</td>\n",
              "      <td>classified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>infection</td>\n",
              "      <td>egreen</td>\n",
              "      <td>scored</td>\n",
              "      <td>sendmail</td>\n",
              "      <td>appartus</td>\n",
              "      <td>batf</td>\n",
              "      <td>religions</td>\n",
              "      <td>surveyed</td>\n",
              "      <td>5s</td>\n",
              "      <td>aj</td>\n",
              "      <td>hank</td>\n",
              "      <td>undefined</td>\n",
              "      <td>porsche</td>\n",
              "      <td>nasa</td>\n",
              "      <td>drain</td>\n",
              "      <td>centris</td>\n",
              "      <td>oeinck.waterland.wlink.nl</td>\n",
              "      <td>armenian</td>\n",
              "      <td>menu</td>\n",
              "      <td>denning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>treatments</td>\n",
              "      <td>harley</td>\n",
              "      <td>leafs</td>\n",
              "      <td>comp</td>\n",
              "      <td>spikes</td>\n",
              "      <td>deaths</td>\n",
              "      <td>geneva.rutgers.edu</td>\n",
              "      <td>argc</td>\n",
              "      <td>+j</td>\n",
              "      <td>pj</td>\n",
              "      <td>15apr93.14691229.0062</td>\n",
              "      <td>rosicrucian</td>\n",
              "      <td>jimf</td>\n",
              "      <td>propulsion</td>\n",
              "      <td>wiring</td>\n",
              "      <td>vram</td>\n",
              "      <td>h0</td>\n",
              "      <td>argic</td>\n",
              "      <td>directory</td>\n",
              "      <td>wiretap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>syndrome</td>\n",
              "      <td>hamer</td>\n",
              "      <td>teams</td>\n",
              "      <td>++49</td>\n",
              "      <td>/-</td>\n",
              "      <td>guns</td>\n",
              "      <td>teachings</td>\n",
              "      <td>argv</td>\n",
              "      <td>m6</td>\n",
              "      <td>x5</td>\n",
              "      <td>145</td>\n",
              "      <td>-lxmu</td>\n",
              "      <td>boyle</td>\n",
              "      <td>astronomy</td>\n",
              "      <td>reminds</td>\n",
              "      <td>vlb</td>\n",
              "      <td>v=</td>\n",
              "      <td>israelis</td>\n",
              "      <td>shareware</td>\n",
              "      <td>cryptography</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>disorder</td>\n",
              "      <td>behanna</td>\n",
              "      <td>coach</td>\n",
              "      <td>4.1/nist</td>\n",
              "      <td>n4tmi</td>\n",
              "      <td>victims</td>\n",
              "      <td>atheism</td>\n",
              "      <td>20-39</td>\n",
              "      <td>nv</td>\n",
              "      <td>ry</td>\n",
              "      <td>139</td>\n",
              "      <td>chiu</td>\n",
              "      <td>centerline.com</td>\n",
              "      <td>satellites</td>\n",
              "      <td>voltage</td>\n",
              "      <td>simm</td>\n",
              "      <td>ia522b1w165w</td>\n",
              "      <td>appressian</td>\n",
              "      <td>xlib</td>\n",
              "      <td>sternlight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>illness</td>\n",
              "      <td>maven</td>\n",
              "      <td>playoff</td>\n",
              "      <td>centrally</td>\n",
              "      <td>q\\</td>\n",
              "      <td>criminals</td>\n",
              "      <td>atheist</td>\n",
              "      <td>15378</td>\n",
              "      <td>rv</td>\n",
              "      <td>l6</td>\n",
              "      <td>greenberg</td>\n",
              "      <td>clintonites</td>\n",
              "      <td>mercedes</td>\n",
              "      <td>kelvin.jpl.nasa.gov</td>\n",
              "      <td>cycle</td>\n",
              "      <td>slots</td>\n",
              "      <td>9f</td>\n",
              "      <td>ohanus</td>\n",
              "      <td>xterm</td>\n",
              "      <td>clipper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>flavor</td>\n",
              "      <td>countersteering</td>\n",
              "      <td>scoring</td>\n",
              "      <td>17:01:34</td>\n",
              "      <td>ux</td>\n",
              "      <td>firearm</td>\n",
              "      <td>worship</td>\n",
              "      <td>xtappcontext</td>\n",
              "      <td>g\\</td>\n",
              "      <td>=1</td>\n",
              "      <td>118</td>\n",
              "      <td>ohmite</td>\n",
              "      <td>taurus</td>\n",
              "      <td>higgins</td>\n",
              "      <td>cylinder</td>\n",
              "      <td>adapter</td>\n",
              "      <td>ez</td>\n",
              "      <td>sahak</td>\n",
              "      <td>windows</td>\n",
              "      <td>cryptographic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>yeast</td>\n",
              "      <td>bikers</td>\n",
              "      <td>players</td>\n",
              "      <td>129.6.54.11</td>\n",
              "      <td>tcp</td>\n",
              "      <td>rape</td>\n",
              "      <td>sins</td>\n",
              "      <td>natonal</td>\n",
              "      <td>.0</td>\n",
              "      <td>ei</td>\n",
              "      <td>119</td>\n",
              "      <td>tue</td>\n",
              "      <td>miata</td>\n",
              "      <td>baalke</td>\n",
              "      <td>diameter</td>\n",
              "      <td>fpu</td>\n",
              "      <td>kubilay</td>\n",
              "      <td>'in</td>\n",
              "      <td>openwindows</td>\n",
              "      <td>strnlght</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>pitt.uucp</td>\n",
              "      <td>riders</td>\n",
              "      <td>phillies</td>\n",
              "      <td>conduit</td>\n",
              "      <td>w6</td>\n",
              "      <td>violent</td>\n",
              "      <td>morality</td>\n",
              "      <td>arg</td>\n",
              "      <td>q2</td>\n",
              "      <td>+9</td>\n",
              "      <td>172</td>\n",
              "      <td>stonewalling</td>\n",
              "      <td>liter</td>\n",
              "      <td>aurora.alaska.edu</td>\n",
              "      <td>cents</td>\n",
              "      <td>connector</td>\n",
              "      <td>8=</td>\n",
              "      <td>melkonian</td>\n",
              "      <td>animation</td>\n",
              "      <td>dorothy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>allergic</td>\n",
              "      <td>overpass</td>\n",
              "      <td>detroit</td>\n",
              "      <td>'vrfy</td>\n",
              "      <td>706</td>\n",
              "      <td>davidians</td>\n",
              "      <td>resurrection</td>\n",
              "      <td>biberdorf</td>\n",
              "      <td>q3</td>\n",
              "      <td>uj</td>\n",
              "      <td>39</td>\n",
              "      <td>immunizing</td>\n",
              "      <td>t-bird</td>\n",
              "      <td>zoo.toronto.edu</td>\n",
              "      <td>wierd</td>\n",
              "      <td>nubus</td>\n",
              "      <td>+t</td>\n",
              "      <td>villages</td>\n",
              "      <td>bitmap</td>\n",
              "      <td>eff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>noring</td>\n",
              "      <td>research.nj.nec.com</td>\n",
              "      <td>espn</td>\n",
              "      <td>hess</td>\n",
              "      <td>krillean</td>\n",
              "      <td>handgun</td>\n",
              "      <td>christ</td>\n",
              "      <td>stdio.h</td>\n",
              "      <td>^e</td>\n",
              "      <td>m_</td>\n",
              "      <td>racking</td>\n",
              "      <td>zaphod.mps.ohio-state.edu</td>\n",
              "      <td>callison</td>\n",
              "      <td>jacked</td>\n",
              "      <td>inches</td>\n",
              "      <td>cache</td>\n",
              "      <td>,4</td>\n",
              "      <td>lebanon</td>\n",
              "      <td>ms-windows</td>\n",
              "      <td>sci.crypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>seizures</td>\n",
              "      <td>sixgun.east.sun.com</td>\n",
              "      <td>montreal</td>\n",
              "      <td>'expn</td>\n",
              "      <td>fevered</td>\n",
              "      <td>agenda</td>\n",
              "      <td>scripture</td>\n",
              "      <td>guttmacher</td>\n",
              "      <td>yn</td>\n",
              "      <td>dz</td>\n",
              "      <td>94</td>\n",
              "      <td>galki.toppoint.de</td>\n",
              "      <td>freeway</td>\n",
              "      <td>sci.space</td>\n",
              "      <td>garage</td>\n",
              "      <td>jumpers</td>\n",
              "      <td>=j</td>\n",
              "      <td>occupied</td>\n",
              "      <td>sunos</td>\n",
              "      <td>encrypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>dyer</td>\n",
              "      <td>shoei</td>\n",
              "      <td>islanders</td>\n",
              "      <td>gratification</td>\n",
              "      <td>6g</td>\n",
              "      <td>suicide</td>\n",
              "      <td>livesey</td>\n",
              "      <td>lfoard</td>\n",
              "      <td>i^</td>\n",
              "      <td>w8</td>\n",
              "      <td>133</td>\n",
              "      <td>ch981</td>\n",
              "      <td>nissan</td>\n",
              "      <td>pasadena</td>\n",
              "      <td>tight</td>\n",
              "      <td>5.25</td>\n",
              "      <td>,8</td>\n",
              "      <td>occupation</td>\n",
              "      <td>ms-dos</td>\n",
              "      <td>nsa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>infections</td>\n",
              "      <td>vax5.cit.cornell.edu</td>\n",
              "      <td>hitter</td>\n",
              "      <td>expn</td>\n",
              "      <td>dis_pkt</td>\n",
              "      <td>income</td>\n",
              "      <td>eternal</td>\n",
              "      <td>foard</td>\n",
              "      <td>\\7</td>\n",
              "      <td>a_</td>\n",
              "      <td>77</td>\n",
              "      <td>ulrich</td>\n",
              "      <td>rec.autos</td>\n",
              "      <td>ssto</td>\n",
              "      <td>bucks</td>\n",
              "      <td>lciii</td>\n",
              "      <td>qb</td>\n",
              "      <td>arabs</td>\n",
              "      <td>sparc</td>\n",
              "      <td>des</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>cnsvax.uwec.edu</td>\n",
              "      <td>compdyn.questor.org</td>\n",
              "      <td>flyers</td>\n",
              "      <td>etrat</td>\n",
              "      <td>2t</td>\n",
              "      <td>virginia.edu</td>\n",
              "      <td>clh</td>\n",
              "      <td>press-democrat</td>\n",
              "      <td>m4</td>\n",
              "      <td>m-</td>\n",
              "      <td>571</td>\n",
              "      <td>doorsteps</td>\n",
              "      <td>frost</td>\n",
              "      <td>titan</td>\n",
              "      <td>amps</td>\n",
              "      <td>mhz</td>\n",
              "      <td>zm</td>\n",
              "      <td>cpr</td>\n",
              "      <td>linux</td>\n",
              "      <td>escrowed</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Topic 1               Topic 2    Topic 3        Topic 4   \n",
              "0             foods                  bike     season           smtp  \\\n",
              "1         physician                 bikes        nhl        sombody   \n",
              "2               geb                 rider     hockey         jurgen   \n",
              "3              diet                yamaha   playoffs          jbotz   \n",
              "4       cs.pitt.edu                riding   pitching           botz   \n",
              "5         diagnosed            motorcycle     braves  mtholyoke.edu   \n",
              "6         infection                egreen     scored       sendmail   \n",
              "7        treatments                harley      leafs           comp   \n",
              "8          syndrome                 hamer      teams           ++49   \n",
              "9          disorder               behanna      coach       4.1/nist   \n",
              "10          illness                 maven    playoff      centrally   \n",
              "11           flavor       countersteering    scoring       17:01:34   \n",
              "12            yeast                bikers    players    129.6.54.11   \n",
              "13        pitt.uucp                riders   phillies        conduit   \n",
              "14         allergic              overpass    detroit          'vrfy   \n",
              "15           noring   research.nj.nec.com       espn           hess   \n",
              "16         seizures   sixgun.east.sun.com   montreal          'expn   \n",
              "17             dyer                 shoei  islanders  gratification   \n",
              "18       infections  vax5.cit.cornell.edu     hitter           expn   \n",
              "19  cnsvax.uwec.edu   compdyn.questor.org     flyers          etrat   \n",
              "\n",
              "                  Topic 5       Topic 6             Topic 7   \n",
              "0                todamhyp         crime        christianity  \\\n",
              "1        charles.unlv.edu       weapons               bible   \n",
              "2                unlv.edu          waco   athos.rutgers.edu   \n",
              "3    information/supplies      firearms            biblical   \n",
              "4   1993apr19.205615.1013        guilty            atheists   \n",
              "5                      ey       assault               jesus   \n",
              "6                appartus          batf           religions   \n",
              "7                  spikes        deaths  geneva.rutgers.edu   \n",
              "8                      /-          guns           teachings   \n",
              "9                   n4tmi       victims             atheism   \n",
              "10                     q\\     criminals             atheist   \n",
              "11                     ux       firearm             worship   \n",
              "12                    tcp          rape                sins   \n",
              "13                     w6       violent            morality   \n",
              "14                    706     davidians        resurrection   \n",
              "15               krillean       handgun              christ   \n",
              "16                fevered        agenda           scripture   \n",
              "17                     6g       suicide             livesey   \n",
              "18                dis_pkt        income             eternal   \n",
              "19                     2t  virginia.edu                 clh   \n",
              "\n",
              "              Topic 8 Topic 9 Topic 10               Topic 11   \n",
              "0                1066      o\\       m\\                 koufax  \\\n",
              "1                 isu     \\x/       i5                     36   \n",
              "2         promiscuous      m3       f-                   vb30   \n",
              "3              printf   /\\__/       d-             lowenstein   \n",
              "4   exnet.iastate.edu      m+       ln             stankowitz   \n",
              "5              gay/bi      lq       mq   lafibm.lafayette.edu   \n",
              "6            surveyed      5s       aj                   hank   \n",
              "7                argc      +j       pj  15apr93.14691229.0062   \n",
              "8                argv      m6       x5                    145   \n",
              "9               20-39      nv       ry                    139   \n",
              "10              15378      rv       l6              greenberg   \n",
              "11       xtappcontext      g\\       =1                    118   \n",
              "12            natonal      .0       ei                    119   \n",
              "13                arg      q2       +9                    172   \n",
              "14          biberdorf      q3       uj                     39   \n",
              "15            stdio.h      ^e       m_                racking   \n",
              "16         guttmacher      yn       dz                     94   \n",
              "17             lfoard      i^       w8                    133   \n",
              "18              foard      \\7       a_                     77   \n",
              "19     press-democrat      m4       m-                    571   \n",
              "\n",
              "                     Topic 12               Topic 13             Topic 14   \n",
              "0                         gmt                  wagon              shuttle  \\\n",
              "1                         apr  uokmax.ecn.uoknor.edu                orbit   \n",
              "2                         fri                mustang                lunar   \n",
              "3                     mwilson            convertible           spacecraft   \n",
              "4    ncratl.atlantaga.ncr.com            compartment                nsmca   \n",
              "5                         thu                  sedan                  prb   \n",
              "6                   undefined                porsche                 nasa   \n",
              "7                 rosicrucian                   jimf           propulsion   \n",
              "8                       -lxmu                  boyle            astronomy   \n",
              "9                        chiu         centerline.com           satellites   \n",
              "10                clintonites               mercedes  kelvin.jpl.nasa.gov   \n",
              "11                     ohmite                 taurus              higgins   \n",
              "12                        tue                  miata               baalke   \n",
              "13               stonewalling                  liter    aurora.alaska.edu   \n",
              "14                 immunizing                 t-bird      zoo.toronto.edu   \n",
              "15  zaphod.mps.ohio-state.edu               callison               jacked   \n",
              "16          galki.toppoint.de                freeway            sci.space   \n",
              "17                      ch981                 nissan             pasadena   \n",
              "18                     ulrich              rec.autos                 ssto   \n",
              "19                  doorsteps                  frost                titan   \n",
              "\n",
              "      Topic 15     Topic 16                   Topic 17    Topic 18   \n",
              "0      cheaper         scsi                     bezier     turkish  \\\n",
              "1         heat   controller                         3-       turks   \n",
              "2   electrical  motherboard                  ferdinand     armenia   \n",
              "3      circuit          ide                         4-   armenians   \n",
              "4         wire        simms                       cusp        arab   \n",
              "5     circuits       quadra                         kk     israeli   \n",
              "6        drain      centris  oeinck.waterland.wlink.nl    armenian   \n",
              "7       wiring         vram                         h0       argic   \n",
              "8      reminds          vlb                         v=    israelis   \n",
              "9      voltage         simm               ia522b1w165w  appressian   \n",
              "10       cycle        slots                         9f      ohanus   \n",
              "11    cylinder      adapter                         ez       sahak   \n",
              "12    diameter          fpu                    kubilay         'in   \n",
              "13       cents    connector                         8=   melkonian   \n",
              "14       wierd        nubus                         +t    villages   \n",
              "15      inches        cache                         ,4     lebanon   \n",
              "16      garage      jumpers                         =j    occupied   \n",
              "17       tight         5.25                         ,8  occupation   \n",
              "18       bucks        lciii                         qb       arabs   \n",
              "19        amps          mhz                         zm         cpr   \n",
              "\n",
              "       Topic 19       Topic 20  \n",
              "0           3.1     encryption  \n",
              "1     microsoft        privacy  \n",
              "2         motif         escrow  \n",
              "3         x11r5      encrypted  \n",
              "4        widget         crypto  \n",
              "5          font     classified  \n",
              "6          menu        denning  \n",
              "7     directory        wiretap  \n",
              "8     shareware   cryptography  \n",
              "9          xlib     sternlight  \n",
              "10        xterm        clipper  \n",
              "11      windows  cryptographic  \n",
              "12  openwindows       strnlght  \n",
              "13    animation        dorothy  \n",
              "14       bitmap            eff  \n",
              "15   ms-windows      sci.crypt  \n",
              "16        sunos        encrypt  \n",
              "17       ms-dos            nsa  \n",
              "18        sparc            des  \n",
              "19        linux       escrowed  "
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_words_per_topic = get_top_words(nkw, id_to_word, top_n=20, method=\"relative\")\n",
        "df_top_words = top_words_to_df(top_words_per_topic)\n",
        "df_top_words"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}