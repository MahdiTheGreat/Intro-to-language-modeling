{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparations"
      ],
      "metadata": {
        "id": "Cuobz5eyz1WA"
      },
      "id": "Cuobz5eyz1WA"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "ce702684-1a4f-4d39-b359-41466c77c9b1",
      "metadata": {
        "id": "ce702684-1a4f-4d39-b359-41466c77c9b1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5kG_vdeXzc4",
        "outputId": "e88c9af1-1de4-4417-fce8-1a9b609faddb"
      },
      "id": "S5kG_vdeXzc4",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Intro-to-language-modeling' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV9ofMztz-tx",
        "outputId": "2a107479-8ae2-4676-93d7-cac470bdb31b"
      },
      "id": "CV9ofMztz-tx",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIN-54j10AxS",
        "outputId": "fd8837c9-6e8c-4d2b-d7ba-8815b6161134"
      },
      "id": "UIN-54j10AxS",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "dataset='/content/Intro-to-language-modeling/pa4'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "! unzip -q {zip_file}\n",
        "! rm {zip_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJhIuW6G0GFq",
        "outputId": "c7c227c0-f0d7-4f18-e4c7-85cf632e1a33"
      },
      "id": "rJhIuW6G0GFq",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/Intro-to-language-modeling/pa4.zip, /content/Intro-to-language-modeling/pa4.zip.zip or /content/Intro-to-language-modeling/pa4.zip.ZIP.\n",
            "rm: cannot remove '/content/Intro-to-language-modeling/pa4.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ti_79mbgW1Z",
        "outputId": "eada8df5-b3fe-4018-cad0-80b418a8b818"
      },
      "id": "9Ti_79mbgW1Z",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "Tk_Ik7G6iBdB"
      },
      "id": "Tk_Ik7G6iBdB",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Full fine tuning"
      ],
      "metadata": {
        "id": "S6-24UXD2y_S"
      },
      "id": "S6-24UXD2y_S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "OE6Li7Uj22xT"
      },
      "id": "OE6Li7Uj22xT"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "imdb_dataset = load_dataset('csv', data_files = {'train': '/content/train.csv', 'eval': '/content/eval.csv'})"
      ],
      "metadata": {
        "id": "0-opqgZY2xgz"
      },
      "id": "0-opqgZY2xgz",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "def tokenize_helper(batch):\n",
        "    return tokenizer(batch['review'], padding=True, truncation=True)\n",
        "tokenized_imdb_dataset = imdb_dataset.map(tokenize_helper, batched=True)"
      ],
      "metadata": {
        "id": "_f5eoo9bVYI_"
      },
      "id": "_f5eoo9bVYI_",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating classifier"
      ],
      "metadata": {
        "id": "2NyPIBSEVpoS"
      },
      "id": "2NyPIBSEVpoS"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O_wRPg3WCU5",
        "outputId": "752fed70-f144-4f34-d950-5af2cdb96882"
      },
      "id": "1O_wRPg3WCU5",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Pn9o3ZWGyf",
        "outputId": "2bb5648a-4ea7-40d1-afff-98c443f20bbb"
      },
      "id": "D4Pn9o3ZWGyf",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting the number of trainable paramters"
      ],
      "metadata": {
        "id": "qGpaTeY4WMen"
      },
      "id": "qGpaTeY4WMen"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "qJ6dPDwx1JMj"
      },
      "id": "qJ6dPDwx1JMj",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "count_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A50bSGsnWVOj",
        "outputId": "10f4ca74-cd01-4cfe-85da-f963f9695258"
      },
      "id": "A50bSGsnWVOj",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66955010"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for training"
      ],
      "metadata": {
        "id": "JHjynotPWweC"
      },
      "id": "JHjynotPWweC"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_arguments = TrainingArguments(output_dir = 'out', num_train_epochs=10, eval_strategy='epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dil0DuAXGBI",
        "outputId": "7a48ee1f-2d38-45f4-c9f4-0cd3e82dae11"
      },
      "id": "-Dil0DuAXGBI",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_scorer = evaluate.load('accuracy')\n",
        "\n",
        "def evaluation_helper(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return accuracy_scorer.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "xqKGtVzCWyUq"
      },
      "id": "xqKGtVzCWyUq",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "ZpQfYjubWaph"
      },
      "id": "ZpQfYjubWaph"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "model.to(device)\n",
        "trainer = Trainer(model=model, args=training_arguments, train_dataset=tokenized_imdb_dataset['train'], eval_dataset=tokenized_imdb_dataset['eval'], compute_metrics=evaluation_helper)"
      ],
      "metadata": {
        "id": "rJ7noGfMWeR6"
      },
      "id": "rJ7noGfMWeR6",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pwvXJchrXchv",
        "outputId": "7a83615d-728c-47f7-879b-0f26c7a16802"
      },
      "id": "pwvXJchrXchv",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 19:07, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.348259</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.338300</td>\n",
              "      <td>0.482767</td>\n",
              "      <td>0.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.338300</td>\n",
              "      <td>0.460919</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.090200</td>\n",
              "      <td>0.612165</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.090200</td>\n",
              "      <td>0.742427</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.020500</td>\n",
              "      <td>0.646446</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.020500</td>\n",
              "      <td>0.726220</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>0.736991</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>0.751541</td>\n",
              "      <td>0.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.760752</td>\n",
              "      <td>0.904000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.09145046510696411, metrics={'train_runtime': 1147.6014, 'train_samples_per_second': 17.428, 'train_steps_per_second': 2.178, 'total_flos': 2649347973120000.0, 'train_loss': 0.09145046510696411, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to avoid rerunning\n",
        "trainer.save_model('fully-finetuned.model')"
      ],
      "metadata": {
        "id": "j0p4cYe9Xyo3"
      },
      "id": "j0p4cYe9Xyo3",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Fine tuning final layers only"
      ],
      "metadata": {
        "id": "bpbxjuvsYAH3"
      },
      "id": "bpbxjuvsYAH3"
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "# Remove gradient computation for all but classification layer\n",
        "for param in adapted_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in adapted_model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in adapted_model.pre_classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRtKnsxLX-b3",
        "outputId": "6458bc11-ef8d-4c20-b107-9f290868906e"
      },
      "id": "lRtKnsxLX-b3",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "count_trainable_parameters(adapted_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgnNUwaYaOxW",
        "outputId": "6fda54ea-d494-4fd0-fe58-23be8fd759b3"
      },
      "id": "KgnNUwaYaOxW",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "592130"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_model.to(device)\n",
        "adapted_trainer = Trainer(model=adapted_model, args=training_arguments, train_dataset=tokenized_imdb_dataset['train'], eval_dataset=tokenized_imdb_dataset['eval'], compute_metrics=evaluation_helper)"
      ],
      "metadata": {
        "id": "wTN9oJPPaff2"
      },
      "id": "wTN9oJPPaff2",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qYX_DtjPaluX",
        "outputId": "ecaaf834-dc2e-42a1-970c-6342854c4512"
      },
      "id": "qYX_DtjPaluX",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 06:50, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.596484</td>\n",
              "      <td>0.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.590600</td>\n",
              "      <td>0.500001</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.590600</td>\n",
              "      <td>0.434207</td>\n",
              "      <td>0.814000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.449100</td>\n",
              "      <td>0.402174</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.449100</td>\n",
              "      <td>0.401504</td>\n",
              "      <td>0.826000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.389323</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.377066</td>\n",
              "      <td>0.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.368555</td>\n",
              "      <td>0.842000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.366320</td>\n",
              "      <td>0.842000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.376200</td>\n",
              "      <td>0.364697</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.4389877624511719, metrics={'train_runtime': 410.7837, 'train_samples_per_second': 48.687, 'train_steps_per_second': 6.086, 'total_flos': 2649347973120000.0, 'train_loss': 0.4389877624511719, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_trainer.save_model('adapted.model')"
      ],
      "metadata": {
        "id": "JSzUu6UXaphV"
      },
      "id": "JSzUu6UXaphV",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fine tuning with LoRA"
      ],
      "metadata": {
        "id": "a1NflLY7athI"
      },
      "id": "a1NflLY7athI"
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0rv3txXatIz",
        "outputId": "67635d03-d564-444e-f6e4-126ed82e7ac0"
      },
      "id": "X0rv3txXatIz",
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities for modifying models"
      ],
      "metadata": {
        "id": "CqTZqEF3dWGd"
      },
      "id": "CqTZqEF3dWGd"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_qv_layers(model):\n",
        "  dictionary = {}\n",
        "  for name, module in model.named_modules():\n",
        "    if 'q_lin' in name or 'v_lin' in name:\n",
        "      dictionary[name] = module\n",
        "  return dictionary\n"
      ],
      "metadata": {
        "id": "BZS9_6vhdm_Y"
      },
      "id": "BZS9_6vhdm_Y",
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "len(extract_qv_layers(lora_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBKciio_CdWw",
        "outputId": "4c561994-bdfb-4eca-9c39-d4af4bd32191"
      },
      "id": "kBKciio_CdWw",
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 425
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_layers(model, named_layers):\n",
        "    for name, layer in named_layers.items():\n",
        "        components = name.split('.')\n",
        "        submodule = model\n",
        "        for component in components[:-1]:\n",
        "            submodule = getattr(submodule, component)\n",
        "        setattr(submodule, components[-1], layer)"
      ],
      "metadata": {
        "id": "_WqTeUdtdo0u"
      },
      "id": "_WqTeUdtdo0u",
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the LoRA layer"
      ],
      "metadata": {
        "id": "ZsYRBEe5m6jb"
      },
      "id": "ZsYRBEe5m6jb"
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "id": "14d538e5-8041-4efa-97d7-4b32532827c7",
      "metadata": {
        "id": "14d538e5-8041-4efa-97d7-4b32532827c7"
      },
      "outputs": [],
      "source": [
        "class LinearBlockWithLoRA(nn.Module):\n",
        "    def __init__(self, W, r, alpha = 0.01):\n",
        "        \"\"\"\n",
        "        Initializes the LinearBlockWithLoRA.\n",
        "\n",
        "        Args:\n",
        "            W (torch.Tensor): Pre-trained weight matrix.\n",
        "            r (int): Rank of the low-rank approximation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Store the pre-trained weight matrix\n",
        "        self.W = W  # Frozen pre-trained weights\n",
        "\n",
        "        # Get the dimensions of the pre-trained weight matrix\n",
        "        out_dim, in_dim  = W.shape\n",
        "\n",
        "        # Initialize the low-rank matrices A and B\n",
        "        self.A = nn.Linear(in_features=in_dim, out_features=r, bias=False)  # Low-rank adaptation A\n",
        "        self.B = nn.Linear(in_features=r, out_features=out_dim, bias=False)   # Low-rank adaptation B\n",
        "\n",
        "        # Initialize the weights of the low-rank matrices\n",
        "        nn.init.normal_(self.A.weight)\n",
        "        nn.init.zeros_(self.B.weight)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the LinearBlockWithLoRA.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying W and LoRA.\n",
        "        \"\"\"\n",
        "        # Compute the output with the pre-trained weight matrix\n",
        "        W_out = X @ self.W  # Using frozen weights\n",
        "\n",
        "        # Compute the low-rank adaptation\n",
        "        a_out = self.A(X) # (batch_size x in_dim) @ (in_dim x r) @ (r x out_dim)\n",
        "        b_out = self.B(a_out) # (batch_size x in_dim) @ (in_dim x r) @ (r x out_dim\n",
        "\n",
        "        # Add scaled adaptation to the pre-trained weights' output\n",
        "        scaled_b_out = (self.alpha/self.r)*b_out\n",
        "\n",
        "        return W_out + scaled_b_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning with LoRA"
      ],
      "metadata": {
        "id": "BsqR5tkdnHvv"
      },
      "id": "BsqR5tkdnHvv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap linear layers with LinearBlockWithLoRA\n",
        "relevant_layers = extract_qv_layers(lora_model)\n",
        "\n",
        "for name, layer in relevant_layers.items():\n",
        "    lora_layer = LinearBlockWithLoRA(layer.weight, r=8)\n",
        "    lora_layer.W.requires_grad = False\n",
        "    relevant_layers[name] = lora_layer\n",
        "\n",
        "replace_layers(lora_model, relevant_layers)"
      ],
      "metadata": {
        "id": "zpdqhAlPxrFz"
      },
      "id": "zpdqhAlPxrFz",
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "count_trainable_parameters(lora_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfF2ECDyo8It",
        "outputId": "d86167fb-8284-44d4-c155-0d9049ebd525"
      },
      "id": "WfF2ECDyo8It",
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60015362"
            ]
          },
          "metadata": {},
          "execution_count": 429
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.to(device)\n",
        "lora_trainer = Trainer(model=lora_model, args=training_arguments, train_dataset=tokenized_imdb_dataset['train'], eval_dataset=tokenized_imdb_dataset['eval'], compute_metrics=evaluation_helper)"
      ],
      "metadata": {
        "id": "X6imCL19n32j"
      },
      "id": "X6imCL19n32j",
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fo_2GWZlp08d",
        "outputId": "48f75320-ea7a-4316-e0ca-7cd7e05a73cb"
      },
      "id": "fo_2GWZlp08d",
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 18:22, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.712238</td>\n",
              "      <td>0.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.517300</td>\n",
              "      <td>0.384692</td>\n",
              "      <td>0.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.517300</td>\n",
              "      <td>0.477394</td>\n",
              "      <td>0.852000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.624631</td>\n",
              "      <td>0.856000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.776061</td>\n",
              "      <td>0.848000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.043700</td>\n",
              "      <td>0.857094</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.043700</td>\n",
              "      <td>0.836823</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>0.853546</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>0.861644</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>0.861726</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.15874468517303467, metrics={'train_runtime': 1103.0096, 'train_samples_per_second': 18.132, 'train_steps_per_second': 2.267, 'total_flos': 2657841438720000.0, 'train_loss': 0.15874468517303467, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 431
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_trainer.save_model('lora.model')"
      ],
      "metadata": {
        "id": "8f3sQ65JswvA"
      },
      "id": "8f3sQ65JswvA",
      "execution_count": 432,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}