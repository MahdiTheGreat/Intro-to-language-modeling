{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparations"
      ],
      "metadata": {
        "id": "Cuobz5eyz1WA"
      },
      "id": "Cuobz5eyz1WA"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "ce702684-1a4f-4d39-b359-41466c77c9b1",
      "metadata": {
        "id": "ce702684-1a4f-4d39-b359-41466c77c9b1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5kG_vdeXzc4",
        "outputId": "e88c9af1-1de4-4417-fce8-1a9b609faddb"
      },
      "id": "S5kG_vdeXzc4",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Intro-to-language-modeling' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV9ofMztz-tx",
        "outputId": "2a107479-8ae2-4676-93d7-cac470bdb31b"
      },
      "id": "CV9ofMztz-tx",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIN-54j10AxS",
        "outputId": "fd8837c9-6e8c-4d2b-d7ba-8815b6161134"
      },
      "id": "UIN-54j10AxS",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "dataset='/content/Intro-to-language-modeling/pa4'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "! unzip -q {zip_file}\n",
        "! rm {zip_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJhIuW6G0GFq",
        "outputId": "c7c227c0-f0d7-4f18-e4c7-85cf632e1a33"
      },
      "id": "rJhIuW6G0GFq",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/Intro-to-language-modeling/pa4.zip, /content/Intro-to-language-modeling/pa4.zip.zip or /content/Intro-to-language-modeling/pa4.zip.ZIP.\n",
            "rm: cannot remove '/content/Intro-to-language-modeling/pa4.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ti_79mbgW1Z",
        "outputId": "eada8df5-b3fe-4018-cad0-80b418a8b818"
      },
      "id": "9Ti_79mbgW1Z",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "Tk_Ik7G6iBdB"
      },
      "id": "Tk_Ik7G6iBdB",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Full fine tuning"
      ],
      "metadata": {
        "id": "S6-24UXD2y_S"
      },
      "id": "S6-24UXD2y_S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "OE6Li7Uj22xT"
      },
      "id": "OE6Li7Uj22xT"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "imdb_dataset = load_dataset('csv', data_files = {'train': '/content/train.csv', 'eval': '/content/eval.csv'})"
      ],
      "metadata": {
        "id": "0-opqgZY2xgz"
      },
      "id": "0-opqgZY2xgz",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "def tokenize_helper(batch):\n",
        "    return tokenizer(batch['review'], padding=True, truncation=True)\n",
        "tokenized_imdb_dataset = imdb_dataset.map(tokenize_helper, batched=True)"
      ],
      "metadata": {
        "id": "_f5eoo9bVYI_"
      },
      "id": "_f5eoo9bVYI_",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating classifier"
      ],
      "metadata": {
        "id": "2NyPIBSEVpoS"
      },
      "id": "2NyPIBSEVpoS"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O_wRPg3WCU5",
        "outputId": "752fed70-f144-4f34-d950-5af2cdb96882"
      },
      "id": "1O_wRPg3WCU5",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Pn9o3ZWGyf",
        "outputId": "2bb5648a-4ea7-40d1-afff-98c443f20bbb"
      },
      "id": "D4Pn9o3ZWGyf",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting the number of trainable paramters"
      ],
      "metadata": {
        "id": "qGpaTeY4WMen"
      },
      "id": "qGpaTeY4WMen"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "qJ6dPDwx1JMj"
      },
      "id": "qJ6dPDwx1JMj",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "count_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A50bSGsnWVOj",
        "outputId": "10f4ca74-cd01-4cfe-85da-f963f9695258"
      },
      "id": "A50bSGsnWVOj",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66955010"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for training"
      ],
      "metadata": {
        "id": "JHjynotPWweC"
      },
      "id": "JHjynotPWweC"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_arguments = TrainingArguments(output_dir = 'out', num_train_epochs=10, eval_strategy='epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dil0DuAXGBI",
        "outputId": "7a48ee1f-2d38-45f4-c9f4-0cd3e82dae11"
      },
      "id": "-Dil0DuAXGBI",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_scorer = evaluate.load('accuracy')\n",
        "\n",
        "def evaluation_helper(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return accuracy_scorer.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "xqKGtVzCWyUq"
      },
      "id": "xqKGtVzCWyUq",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "ZpQfYjubWaph"
      },
      "id": "ZpQfYjubWaph"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "model.to(device)\n",
        "trainer = Trainer(model=model, args=training_arguments, train_dataset=tokenized_imdb_dataset['train'], eval_dataset=tokenized_imdb_dataset['eval'], compute_metrics=evaluation_helper)"
      ],
      "metadata": {
        "id": "rJ7noGfMWeR6"
      },
      "id": "rJ7noGfMWeR6",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pwvXJchrXchv",
        "outputId": "7a83615d-728c-47f7-879b-0f26c7a16802"
      },
      "id": "pwvXJchrXchv",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 19:07, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.348259</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.338300</td>\n",
              "      <td>0.482767</td>\n",
              "      <td>0.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.338300</td>\n",
              "      <td>0.460919</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.090200</td>\n",
              "      <td>0.612165</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.090200</td>\n",
              "      <td>0.742427</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.020500</td>\n",
              "      <td>0.646446</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.020500</td>\n",
              "      <td>0.726220</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>0.736991</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>0.751541</td>\n",
              "      <td>0.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.760752</td>\n",
              "      <td>0.904000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.09145046510696411, metrics={'train_runtime': 1147.6014, 'train_samples_per_second': 17.428, 'train_steps_per_second': 2.178, 'total_flos': 2649347973120000.0, 'train_loss': 0.09145046510696411, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to avoid rerunning\n",
        "trainer.save_model('fully-finetuned.model')"
      ],
      "metadata": {
        "id": "j0p4cYe9Xyo3"
      },
      "id": "j0p4cYe9Xyo3",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Fine tuning final layers only"
      ],
      "metadata": {
        "id": "bpbxjuvsYAH3"
      },
      "id": "bpbxjuvsYAH3"
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "# Remove gradient computation for all but classification layer\n",
        "for param in adapted_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in adapted_model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in adapted_model.pre_classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRtKnsxLX-b3",
        "outputId": "6458bc11-ef8d-4c20-b107-9f290868906e"
      },
      "id": "lRtKnsxLX-b3",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "count_trainable_parameters(adapted_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgnNUwaYaOxW",
        "outputId": "6fda54ea-d494-4fd0-fe58-23be8fd759b3"
      },
      "id": "KgnNUwaYaOxW",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "592130"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_model.to(device)\n",
        "adapted_trainer = Trainer(model=adapted_model, args=training_arguments, train_dataset=tokenized_imdb_dataset['train'], eval_dataset=tokenized_imdb_dataset['eval'], compute_metrics=evaluation_helper)"
      ],
      "metadata": {
        "id": "wTN9oJPPaff2"
      },
      "id": "wTN9oJPPaff2",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qYX_DtjPaluX",
        "outputId": "ecaaf834-dc2e-42a1-970c-6342854c4512"
      },
      "id": "qYX_DtjPaluX",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 06:50, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.596484</td>\n",
              "      <td>0.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.590600</td>\n",
              "      <td>0.500001</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.590600</td>\n",
              "      <td>0.434207</td>\n",
              "      <td>0.814000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.449100</td>\n",
              "      <td>0.402174</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.449100</td>\n",
              "      <td>0.401504</td>\n",
              "      <td>0.826000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.389323</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.377066</td>\n",
              "      <td>0.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.368555</td>\n",
              "      <td>0.842000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.366320</td>\n",
              "      <td>0.842000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.376200</td>\n",
              "      <td>0.364697</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.4389877624511719, metrics={'train_runtime': 410.7837, 'train_samples_per_second': 48.687, 'train_steps_per_second': 6.086, 'total_flos': 2649347973120000.0, 'train_loss': 0.4389877624511719, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adapted_trainer.save_model('adapted.model')"
      ],
      "metadata": {
        "id": "JSzUu6UXaphV"
      },
      "id": "JSzUu6UXaphV",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fine tuning with LoRA"
      ],
      "metadata": {
        "id": "a1NflLY7athI"
      },
      "id": "a1NflLY7athI"
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0rv3txXatIz",
        "outputId": "a9017acd-2f25-43e9-b4f7-a585ebab3ade"
      },
      "id": "X0rv3txXatIz",
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities for modifying models"
      ],
      "metadata": {
        "id": "CqTZqEF3dWGd"
      },
      "id": "CqTZqEF3dWGd"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_qv_layers(model):\n",
        "  dictionary = {}\n",
        "  for name in model.state_dict():\n",
        "    if 'q' in name or 'v' in name:\n",
        "      dictionary[name] = model.state_dict()[name]\n",
        "  return dictionary\n"
      ],
      "metadata": {
        "id": "BZS9_6vhdm_Y"
      },
      "id": "BZS9_6vhdm_Y",
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_layers(model, named_layers):\n",
        "    for name, layer in named_layers.items():\n",
        "        components = name.split('.')\n",
        "        submodule = model\n",
        "        for component in components[:-1]:\n",
        "            submodule = getattr(submodule, component)\n",
        "        setattr(submodule, components[-1], layer)"
      ],
      "metadata": {
        "id": "_WqTeUdtdo0u"
      },
      "id": "_WqTeUdtdo0u",
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the LoRA layer"
      ],
      "metadata": {
        "id": "ZsYRBEe5m6jb"
      },
      "id": "ZsYRBEe5m6jb"
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "14d538e5-8041-4efa-97d7-4b32532827c7",
      "metadata": {
        "id": "14d538e5-8041-4efa-97d7-4b32532827c7"
      },
      "outputs": [],
      "source": [
        "class LinearBlockWithLoRA(nn.Module):\n",
        "    def __init__(self, W, r, alpha = 0.01):\n",
        "        \"\"\"\n",
        "        Initializes the LinearBlockWithLoRA.\n",
        "\n",
        "        Args:\n",
        "            W (torch.Tensor): Pre-trained weight matrix.\n",
        "            r (int): Rank of the low-rank approximation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Store the pre-trained weight matrix\n",
        "        self.W = W  # Frozen pre-trained weights\n",
        "\n",
        "        # Get the dimensions of the pre-trained weight matrix\n",
        "        out_dim, in_dim  = W.shape\n",
        "\n",
        "        # Initialize the low-rank matrices A and B\n",
        "        #self.A = nn.Linear(in_features=in_dim, out_features=r, bias=False)  # Low-rank adaptation A\n",
        "        #self.B = nn.Linear(in_features=r, out_features=out_dim, bias=False)   # Low-rank adaptation B\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, r))\n",
        "        self.B = nn.Parameter(torch.randn(r, out_dim))\n",
        "\n",
        "        # Initialize the weights of the low-rank matrices\n",
        "        nn.init.normal_(self.A)\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the LinearBlockWithLoRA.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying W and LoRA.\n",
        "        \"\"\"\n",
        "        # Compute the output with the pre-trained weight matrix\n",
        "        W_out = self.W(X)  # Using frozen weights\n",
        "\n",
        "        # Compute the low-rank adaptation\n",
        "        a_out = self.A(X) # (batch_size x in_dim) @ (in_dim x r) @ (r x out_dim)\n",
        "        b_out = self.B(a_out) # (batch_size x in_dim) @ (in_dim x r) @ (r x out_dim\n",
        "\n",
        "        # Add scaled adaptation to the pre-trained weights' output\n",
        "        scaled_b_out = (self.alpha/self.r)*b_out\n",
        "\n",
        "        return W_out + scaled_b_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning with LoRA"
      ],
      "metadata": {
        "id": "BsqR5tkdnHvv"
      },
      "id": "BsqR5tkdnHvv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap linear layers with LinearBlockWithLoRA\n",
        "relevant_layers = extract_qv_layers(lora_model)\n",
        "\n",
        "for name, layer in relevant_layers.items():\n",
        "    layer.requires_grad = False\n",
        "    if 'weight' in name:\n",
        "      relevant_layers[name] = LinearBlockWithLoRA(layer, r=8)\n",
        "\n",
        "\n",
        "#print(relevant_layers)\n",
        "replace_layers(lora_model, relevant_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "zpdqhAlPxrFz",
        "outputId": "d606127e-a700-46db-f246-a384945852c7"
      },
      "id": "zpdqhAlPxrFz",
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot assign '__main__.LinearBlockWithLoRA' as parameter 'weight' (torch.nn.Parameter or None expected)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-197-c43b8010efb2>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(relevant_layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mreplace_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-195-f6f1881c3cad>\u001b[0m in \u001b[0;36mreplace_layers\u001b[0;34m(model, named_layers)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m   1960\u001b[0m                     \u001b[0;34mf\"cannot assign '{torch.typename(value)}' as parameter '{name}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                     \u001b[0;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot assign '__main__.LinearBlockWithLoRA' as parameter 'weight' (torch.nn.Parameter or None expected)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap linear layers with LinearBlockWithLoRA\n",
        "relevant_layers = extract_qv_layers(lora_model)\n",
        "\n",
        "for name, layer in relevant_layers.items():\n",
        "    layer.requires_grad = False\n",
        "    if 'weight' in name:\n",
        "        # Create LinearBlockWithLoRA instance\n",
        "        lora_block = LinearBlockWithLoRA(layer, r=8)\n",
        "        # Assign the pre-trained weights directly to LinearBlockWithLoRA's W\n",
        "        lora_block.W = layer\n",
        "        relevant_layers[name] = lora_block  # Assign the entire lora_block instance to relevant_layers\n",
        "\n",
        "#print(relevant_layers)\n",
        "replace_layers(lora_model, relevant_layers) # Now relevant_layers contains entire modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "SUEgXZTg3WiT",
        "outputId": "9a825683-b1af-4992-e18f-a2335165ef14"
      },
      "id": "SUEgXZTg3WiT",
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot assign '__main__.LinearBlockWithLoRA' as parameter 'weight' (torch.nn.Parameter or None expected)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-190-9f85b5752b35>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print(relevant_layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mreplace_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Now relevant_layers contains entire modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-185-f6f1881c3cad>\u001b[0m in \u001b[0;36mreplace_layers\u001b[0;34m(model, named_layers)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m   1960\u001b[0m                     \u001b[0;34mf\"cannot assign '{torch.typename(value)}' as parameter '{name}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                     \u001b[0;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot assign '__main__.LinearBlockWithLoRA' as parameter 'weight' (torch.nn.Parameter or None expected)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "count_trainable_parameters(lora_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfF2ECDyo8It",
        "outputId": "3969465e-e5e4-40f1-d137-4aa659d6949f"
      },
      "id": "WfF2ECDyo8It",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66955010"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.to(device)\n",
        "lora_trainer = Trainer(model=lora_model, args=training_arguments, train_dataset=tokenized_imdb_dataset['train'], eval_dataset=tokenized_imdb_dataset['eval'], compute_metrics=evaluation_helper)"
      ],
      "metadata": {
        "id": "X6imCL19n32j"
      },
      "id": "X6imCL19n32j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_trainer.train()"
      ],
      "metadata": {
        "id": "fo_2GWZlp08d"
      },
      "id": "fo_2GWZlp08d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_trainer.save_model('lora.model')"
      ],
      "metadata": {
        "id": "8f3sQ65JswvA"
      },
      "id": "8f3sQ65JswvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From LoRA demo"
      ],
      "metadata": {
        "id": "flDD9EN3djX2"
      },
      "id": "flDD9EN3djX2"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "16e8f3c4-dd91-4166-93cd-dc66fc891267",
      "metadata": {
        "id": "16e8f3c4-dd91-4166-93cd-dc66fc891267",
        "outputId": "9a7a1573-b1f6-4c36-ce9e-44e3b660b682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-54180ca6f566>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained = torch.load('s7_pretrained.model')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 's7_pretrained.model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-54180ca6f566>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's7_pretrained.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 's7_pretrained.model'"
          ]
        }
      ],
      "source": [
        "pretrained = torch.load('s7_pretrained.model')\n",
        "pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c035ab-2229-4313-a48b-a7da8590c687",
      "metadata": {
        "id": "56c035ab-2229-4313-a48b-a7da8590c687"
      },
      "outputs": [],
      "source": [
        "def batcher(batch):\n",
        "    X = torch.as_tensor([x for x, _ in batch])\n",
        "    Y = 1.0*torch.as_tensor([y for _, y in batch])\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c38f361-4a51-41d4-bf52-554f8d9b1e2c",
      "metadata": {
        "id": "7c38f361-4a51-41d4-bf52-554f8d9b1e2c"
      },
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "    dl = DataLoader(list(zip(books_X_te, books_Y_te)), batch_size=32, shuffle=False, collate_fn=batcher)\n",
        "    n_corr = 0\n",
        "    for Xb, Yb in dl:\n",
        "        with torch.no_grad():\n",
        "            model_out = model(Xb)\n",
        "        preds = model_out[:, 0] > 0\n",
        "        gold = Yb > 0\n",
        "        n_corr += sum(preds == gold).item()\n",
        "    return n_corr / len(books_Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81753ec-3fca-40a2-b1af-3c484b4f3fac",
      "metadata": {
        "id": "f81753ec-3fca-40a2-b1af-3c484b4f3fac"
      },
      "outputs": [],
      "source": [
        "eval_model(pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c391fe32-19ac-44bf-98f7-18faf6666024",
      "metadata": {
        "id": "c391fe32-19ac-44bf-98f7-18faf6666024"
      },
      "source": [
        "# Basic fine-tuning\n",
        "\n",
        "We create a new model where we copy the weights from the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5535598-361e-46dd-9367-31e3e98021bf",
      "metadata": {
        "id": "f5535598-361e-46dd-9367-31e3e98021bf"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "finetuned = nn.Sequential(\n",
        "    nn.Linear(in_features=768, out_features=512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=1)\n",
        ")\n",
        "\n",
        "# pretrained = torch.load('s7_pretrained.model')\n",
        "\n",
        "finetuned[0].weight.data = pretrained[0].weight.data.clone()\n",
        "finetuned[0].bias.data = pretrained[0].bias.data.clone()\n",
        "finetuned[2].weight.data = pretrained[2].weight.data.clone()\n",
        "finetuned[2].bias.data = pretrained[2].bias.data.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd587f2-b291-4f46-b170-44b1f33c48b3",
      "metadata": {
        "id": "bdd587f2-b291-4f46-b170-44b1f33c48b3"
      },
      "outputs": [],
      "source": [
        "eval_model(finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a038088a-0123-4d91-894c-cc1dcb8f5473",
      "metadata": {
        "id": "a038088a-0123-4d91-894c-cc1dcb8f5473"
      },
      "outputs": [],
      "source": [
        "def train(model, n_epochs=10):\n",
        "    dl = DataLoader(list(zip(books_X_tr, books_Y_tr)), batch_size=32, shuffle=True, collate_fn=batcher)\n",
        "\n",
        "    # NOTE!\n",
        "    params = [ p for p in model.parameters() if p.requires_grad_ ]\n",
        "\n",
        "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        for Xb, Yb in dl:\n",
        "            model_out = model(Xb)[:, 0]\n",
        "            loss = loss_fn(model_out, Yb)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        mean_loss = total_loss / len(dl)\n",
        "        acc = eval_model(model)\n",
        "        print(f'loss = {mean_loss:.4f}, acc = {acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- Complete `count_trainable_parameters` below.\n",
        "- Count the total number of trainable parameters in the model you fine-tuned.\n",
        "- Use the function `train` to fine-tune the cloned model."
      ],
      "metadata": {
        "id": "kP4ym2hY2vDI"
      },
      "id": "kP4ym2hY2vDI"
    },
    {
      "cell_type": "markdown",
      "id": "913197b3-6eda-4a5e-9c23-23e4c345ea18",
      "metadata": {
        "id": "913197b3-6eda-4a5e-9c23-23e4c345ea18"
      },
      "source": [
        "# Implementing LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- Complete `LinearBlockWithLoRA` above\n",
        "- Set up a model using this new block to replace the first linear layer. Initialize parameters from the pre-trained model. (Don't forget to switch off gradient computation for `W`.)\n",
        "- Count the parameters in the new model.\n",
        "- Train the new model."
      ],
      "metadata": {
        "id": "jTKFM5xk1-P_"
      },
      "id": "jTKFM5xk1-P_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "lora_model = nn.Sequential(\n",
        "    LinearBlockWithLoRA(pretrained[0], r=8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=1)\n",
        ")\n",
        "\n",
        "lora_model[0].W.weight.data = pretrained[0].weight.data.clone()\n",
        "lora_model[0].W.bias.data = pretrained[0].bias.data.clone()\n",
        "lora_model[2].weight.data = pretrained[2].weight.data.clone()\n",
        "lora_model[2].bias.data = pretrained[2].bias.data.clone()\n",
        "\n",
        "lora_model[0].W.requires_grad = False\n",
        "\n",
        "train(lora_model, n_epochs=10)\n",
        "\n",
        "print(count_trainable_parameters(lora_model))"
      ],
      "metadata": {
        "id": "UmNHV75Bs9E_"
      },
      "id": "UmNHV75Bs9E_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}