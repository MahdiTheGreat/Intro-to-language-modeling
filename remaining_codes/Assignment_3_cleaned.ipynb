{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/Assignment_3_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgzPiclr4Cun"
      },
      "source": [
        "# Step 0: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S471E7Q86cPH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.datasets import fetch_20newsgroups # We use the 20 news groups text dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW53Mx7p3LqE"
      },
      "source": [
        "# Step 1: Fetching data and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZHLDZSB2iOq"
      },
      "outputs": [],
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nLT1si_CqT4",
        "outputId": "62cf3f43-1505-4e80-feb0-5af29ab84904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "print(len(newsgroups_train.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejgOpsbANmyk"
      },
      "outputs": [],
      "source": [
        "# Split into smaller training sets in percentage\n",
        "percentage = 0.8\n",
        "split_index = int(len(newsgroups_train.data) * percentage)\n",
        "train_data_small = newsgroups_train.data[:split_index]\n",
        "train_targets_small = newsgroups_train.target[:split_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv0CKmXAOKnx",
        "outputId": "59f8e694-9e7d-40fa-af0e-a28a29b55e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_data_small[445])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh1KuX0703Ve",
        "outputId": "156c6026-7ce1-4033-c4a1-b09027eccf63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From mliggett silver ucs indiana edu matthew liggett Subject Re Opel owners Nntp Posting Host silver ucs indiana edu Organization Indiana University Lines In DG news cso uiuc edu uxa cso uiuc edu OrioleFan uiuc writes boyle cactus org Craig Boyle writes In article news cso uiuc edu uxa cso uiuc edu OrioleFan uiuc writes gibbonsa fraser sfu ca Darren Gibbons writes I m looking for information on Opel cars Now you ask which model Well the sad truth is I m not entirely sure but it s a two seater with roll over headlights hard top and really sporty looking My friend has one sitting in his yard in really nice condition body wise but he transmission has seized up on him so it hasn t run for a while Does anyone have any info on these cars The engine compartment looks really tight to work on but it is in fine shape and I am quite interested in it Thanks Darren Gibbons gibbonsa sfu ca This would be the manta would it not Sold through Buick dealers in the mid s as the price leader Sounds a lot more like an Opel GT to me I d guess that this is on the same chassis as the Kadett rather than the bigger Manta but I could easily be wrong I think the later Kadett s were sold here as Buick Opels Craig I think the Manta is the European name for the GT I m pretty sure that the only Kadett s sold here were are the Pontiac LeMans I think the GT is just an early to mid Manta Chintan Amin The University of Illinois UrbanaChampaign mail llama uiuc edu SIG UNDER CONSTRUCTION HARD HAT AREA Bzzt The manta was a two door sedan in the US It had a engine Was sometimes referred to as an Opel Manta s are also ve hot and fun cars too mliggett silver ucs indiana edu mliggett iugold bitnet junk collector toys R us kid antiauthoritarian and fan of frogs iguanas and other herps\n"
          ]
        }
      ],
      "source": [
        "def extract_body(text):\n",
        "    # Extract only words using regex\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "\n",
        "    # Join the words with spaces (optional)\n",
        "    cleaned_text = \" \".join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "removed_headers = extract_body(train_data_small[445])\n",
        "print(removed_headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHCCZWe3b9X",
        "outputId": "b6f10aa3-9c59-4c07-f3e7-e62a4e92bd53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\ANv\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Articles\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9051/9051 [00:10<00:00, 843.86it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique words: 109209\n",
            "Word-to-ID mapping example: {'\\x03': 0, '\\x03\\x03\\x1b': 1, '\\x1a': 2, '!': 3, '#': 4}\n",
            "Integer Corpus example (first article): []\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize structures for the preprocessed corpus\n",
        "filtered_train = [[] for _ in range(len(train_data_small))]  # Preprocessed articles\n",
        "flattened_train = []  # A single list of all words in the corpus\n",
        "\n",
        "# Tokenizing and removing stopwords\n",
        "print(\"Processing Articles\")\n",
        "for i, article in tqdm(enumerate(train_data_small), total=len(train_data_small)):\n",
        "    article_body = extract_body(article) # Only use body and remove headers and footers\n",
        "    word_tokens = word_tokenize(article_body)  # Tokenize article\n",
        "    # Remove stop words and add to both filtered_train and flattened_train\n",
        "    filtered_words = [w.lower() for w in word_tokens if w.lower() not in stop_words]\n",
        "    filtered_train[i] = filtered_words\n",
        "    flattened_train.extend(filtered_words)\n",
        "\n",
        "# Create a vocabulary mapping\n",
        "unique_words = sorted(set(flattened_train))  # Get unique words\n",
        "word_to_id = {word: idx for idx, word in enumerate(unique_words)}  # Map word to ID\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}  # Reverse mapping\n",
        "\n",
        "# Map the filtered articles to integer IDs\n",
        "int_corpus = [[word_to_id[word] for word in article] for article in filtered_train]\n",
        "\n",
        "# Display mappings and a small example\n",
        "print(f\"Total unique words: {len(unique_words)}\")\n",
        "print(\"Word-to-ID mapping example:\", {k: word_to_id[k] for k in list(word_to_id)[:5]})\n",
        "print(\"Integer Corpus example (first article):\", int_corpus[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku3E7_9eW93G"
      },
      "outputs": [],
      "source": [
        "word_count = Counter(flattened_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3wP-jEL1u1"
      },
      "outputs": [],
      "source": [
        "# Extract low-frequency words (occurrence <= 10) into a set\n",
        "low_frequency_words = {word for word, count in word_count.items() if count <= 10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q7WUkGCFsIX",
        "outputId": "77a8de04-68af-4ef2-9fbc-d3b320f561b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Removing LF words: 100%|██████████| 9051/9051 [00:00<00:00, 80738.78it/s]\n"
          ]
        }
      ],
      "source": [
        "# Filter articles efficiently using set operations\n",
        "corpus_hf = []\n",
        "for article in tqdm(int_corpus, desc=\"Removing LF words\"):\n",
        "    article_set = set(article)\n",
        "    filtered_article = list(article_set - low_frequency_words)\n",
        "    corpus_hf.append(filtered_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLmxtbMSQNjs",
        "outputId": "c81e98d4-4400-4051-e797-5a650e0995a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in corpus 1550053\n",
            "Vocabulary size after removing LF words 11537\n"
          ]
        }
      ],
      "source": [
        "flattened_train = [word for word in flattened_train if word not in low_frequency_words]\n",
        "voc_size = len(sorted(set(flattened_train)))\n",
        "print(\"Number of words in corpus\", len(flattened_train))\n",
        "print(\"Vocabulary size after removing LF words\", voc_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4pGSix73wx"
      },
      "source": [
        "# Step 2: Gibbs sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U85ycesL76R8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def lda_gibbs_sampling(corpus, K, alpha, beta, iterations):\n",
        "    \"\"\"\n",
        "    Implements Collapsed Gibbs Sampling for LDA.\n",
        "\n",
        "    :param corpus: List of lists, where each inner list contains word IDs in a document.\n",
        "    :param K: Number of topics.\n",
        "    :param alpha: Dirichlet prior for document-topic distribution.\n",
        "    :param beta: Dirichlet prior for topic-word distribution.\n",
        "    :param iterations: Number of Gibbs sampling iterations.\n",
        "    :return: Topic assignments, document-topic counts, topic-word counts, topic totals.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    D = len(corpus)  # Number of documents\n",
        "    V = len(list(set(word for doc in corpus for word in doc)))\n",
        "\n",
        "    # Count matrices\n",
        "    ndk = np.zeros((D, K))  # Document-topic counts\n",
        "    nkw = np.zeros((K, V))  # Topic-word counts\n",
        "    nk = np.zeros(K)        # Total words in each topic\n",
        "\n",
        "    # Topic assignments for each word\n",
        "    z = []  # Topic assignment for each word in corpus\n",
        "    for d, doc in enumerate(corpus):\n",
        "        doc_topics = []\n",
        "        for word in doc:\n",
        "            topic = np.random.randint(K)  # Randomly assign a topic\n",
        "            doc_topics.append(topic)\n",
        "            ndk[d, topic] += 1\n",
        "            nkw[topic, word] += 1\n",
        "            nk[topic] += 1\n",
        "        z.append(doc_topics)\n",
        "\n",
        "    # Gibbs sampling\n",
        "    for _ in tqdm(range(iterations)):\n",
        "        for d, doc in enumerate(corpus):\n",
        "            for i, word in enumerate(doc):\n",
        "                current_topic = z[d][i]\n",
        "\n",
        "                # Decrement counts\n",
        "                ndk[d, current_topic] -= 1\n",
        "                nkw[current_topic, word] -= 1\n",
        "                nk[current_topic] -= 1\n",
        "\n",
        "                # Compute topic probabilities (Maybe do a for loop here instead)\n",
        "                topic_probs = (ndk[d] + alpha) * (nkw[:, word] + beta) / (nk + beta * V)\n",
        "                topic_probs /= np.sum(topic_probs)  # Normalize\n",
        "\n",
        "                # Sample new topic\n",
        "                new_topic = np.random.choice(K, p=topic_probs)\n",
        "                z[d][i] = new_topic\n",
        "\n",
        "                # Increment counts\n",
        "                ndk[d, new_topic] += 1\n",
        "                nkw[new_topic, word] += 1\n",
        "                nk[new_topic] += 1\n",
        "\n",
        "    return z, ndk, nkw, nk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMPOEWe8FB1",
        "outputId": "6c6cade7-0f60-44c0-828d-b3c7b352286c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [1:03:52<00:00, 19.16s/it]\n"
          ]
        }
      ],
      "source": [
        "# Initialize data\n",
        "corpus = corpus_hf.copy()\n",
        "targets = train_targets_small.copy()\n",
        "topics = newsgroups_train.target_names.copy()\n",
        "\n",
        "# First parameter combo\n",
        "z, ndk, nkw, nk = lda_gibbs_sampling(corpus, alpha = 0.1, beta = 0.1, K=len(topics), iterations=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKX8MdD3cGWr",
        "outputId": "9b4b5b68-783d-499c-b916-733578d424c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9051, 20) (20, 109209) (20,)\n"
          ]
        }
      ],
      "source": [
        "print(ndk.shape, nkw.shape, nk.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AxH9Ge6fQIv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def get_top_words(nkw, id_to_word, top_n=20, method=\"raw\", beta=0.1):\n",
        "    \"\"\"\n",
        "    Get the top words for each topic.\n",
        "\n",
        "    :param nkw: Topic-word counts (K x V matrix).\n",
        "    :param id_to_word: Dictionary mapping word IDs to their original words.\n",
        "    :param top_n: Number of top words to retrieve per topic.\n",
        "    :param method: \"raw\" for raw counts, \"relative\" for relative frequencies.\n",
        "    :param beta: Dirichlet prior for smoothing (used in relative frequency).\n",
        "    :return: Dictionary of top words for each topic.\n",
        "    \"\"\"\n",
        "    K, V = nkw.shape\n",
        "    top_words_per_topic = {}\n",
        "\n",
        "    if method == \"raw\":\n",
        "        # Use raw counts\n",
        "        for k in range(K):\n",
        "            top_word_indices = np.argsort(nkw[k, :])[::-1][:top_n]  # Top N words by count\n",
        "            top_words_per_topic[k] = [f\"{id_to_word[idx]}, {int(nkw[k, idx])}\" for idx in top_word_indices]\n",
        "\n",
        "    elif method == \"relative\":\n",
        "        # Compute relative frequencies\n",
        "        word_totals = np.sum(nkw, axis=0)  # Total count of each word across all topics\n",
        "        for k in range(K): # for k in topics\n",
        "            relative_freqs = (nkw[k, :] + beta) / (word_totals + beta * K)  # Smoothed relative frequency\n",
        "            top_word_indices = np.argsort(relative_freqs)[::-1][:top_n]  # Top N words by relative frequency\n",
        "            top_words_per_topic[k] = [\n",
        "                f\"{id_to_word[idx]}, {int(nkw[k,idx])}\" for idx in top_word_indices\n",
        "            ]\n",
        "\n",
        "    return top_words_per_topic\n",
        "\n",
        "\n",
        "def top_words_to_df(top_words_per_topic):\n",
        "    \"\"\"\n",
        "    Display the top words for each topic in a table format.\n",
        "\n",
        "    :param top_words_per_topic: Dictionary of top words for each topic.\n",
        "    :param method: Description of the method used (\"raw\" or \"relative\").\n",
        "    \"\"\"\n",
        "    df_top_words = pd.DataFrame.from_dict(top_words_per_topic)\n",
        "    df_top_words.columns = [f\"Topic {i+1}\" for i in range(df_top_words.shape[1])]\n",
        "    return df_top_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBiSOQR0cZ0g",
        "outputId": "81fb0b94-0fab-4665-92c6-8abfc7bc7283"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "      <th>Topic 4</th>\n",
              "      <th>Topic 5</th>\n",
              "      <th>Topic 6</th>\n",
              "      <th>Topic 7</th>\n",
              "      <th>Topic 8</th>\n",
              "      <th>Topic 9</th>\n",
              "      <th>Topic 10</th>\n",
              "      <th>Topic 11</th>\n",
              "      <th>Topic 12</th>\n",
              "      <th>Topic 13</th>\n",
              "      <th>Topic 14</th>\n",
              "      <th>Topic 15</th>\n",
              "      <th>Topic 16</th>\n",
              "      <th>Topic 17</th>\n",
              "      <th>Topic 18</th>\n",
              "      <th>Topic 19</th>\n",
              "      <th>Topic 20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>disease, 80</td>\n",
              "      <td>bike, 196</td>\n",
              "      <td>., 546</td>\n",
              "      <td>de, 15</td>\n",
              "      <td>georgia, 7</td>\n",
              "      <td>., 1549</td>\n",
              "      <td>., 805</td>\n",
              "      <td>viking, 18</td>\n",
              "      <td>\\/, 10</td>\n",
              "      <td>+, 9</td>\n",
              "      <td>11, 47</td>\n",
              "      <td>apr, 172</td>\n",
              "      <td>car, 236</td>\n",
              "      <td>space, 188</td>\n",
              "      <td>., 3400</td>\n",
              "      <td>$, 407</td>\n",
              "      <td>curve, 10</td>\n",
              "      <td>israel, 160</td>\n",
              "      <td>., 1941</td>\n",
              "      <td>key, 224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>medical, 78</td>\n",
              "      <td>dod, 131</td>\n",
              "      <td>,, 498</td>\n",
              "      <td>comp, 12</td>\n",
              "      <td>huey, 7</td>\n",
              "      <td>,, 1537</td>\n",
              "      <td>,, 792</td>\n",
              "      <td>1066, 18</td>\n",
              "      <td>o\\, 9</td>\n",
              "      <td>#, 9</td>\n",
              "      <td>16, 44</td>\n",
              "      <td>1993, 111</td>\n",
              "      <td>cars, 147</td>\n",
              "      <td>earth, 92</td>\n",
              "      <td>,, 3316</td>\n",
              "      <td>., 405</td>\n",
              "      <td>bezier, 9</td>\n",
              "      <td>israeli, 129</td>\n",
              "      <td>,, 1779</td>\n",
              "      <td>encryption, 168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>doctor, 61</td>\n",
              "      <td>ride, 109</td>\n",
              "      <td>), 412</td>\n",
              "      <td>van, 9</td>\n",
              "      <td>athens, 7</td>\n",
              "      <td>:, 1369</td>\n",
              "      <td>:, 735</td>\n",
              "      <td>sorenson, 17</td>\n",
              "      <td>q, 9</td>\n",
              "      <td>u, 9</td>\n",
              "      <td>6, 42</td>\n",
              "      <td>gmt, 109</td>\n",
              "      <td>engine, 62</td>\n",
              "      <td>nasa, 88</td>\n",
              "      <td>(, 3032</td>\n",
              "      <td>,, 341</td>\n",
              "      <td>calculating, 7</td>\n",
              "      <td>turkish, 110</td>\n",
              "      <td>), 1343</td>\n",
              "      <td>chip, 162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>food, 61</td>\n",
              "      <td>riding, 90</td>\n",
              "      <td>(, 411</td>\n",
              "      <td>howard, 8</td>\n",
              "      <td>mcovingt, 7</td>\n",
              "      <td>), 1354</td>\n",
              "      <td>(, 709</td>\n",
              "      <td>distant, 17</td>\n",
              "      <td>x, 8</td>\n",
              "      <td>g, 9</td>\n",
              "      <td>12, 41</td>\n",
              "      <td>date, 77</td>\n",
              "      <td>miles, 61</td>\n",
              "      <td>launch, 84</td>\n",
              "      <td>), 3019</td>\n",
              "      <td>), 281</td>\n",
              "      <td>curves, 7</td>\n",
              "      <td>jews, 103</td>\n",
              "      <td>(, 1295</td>\n",
              "      <td>clipper, 161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>treatment, 58</td>\n",
              "      <td>#, 88</td>\n",
              "      <td>:, 401</td>\n",
              "      <td>marc, 8</td>\n",
              "      <td>aisun3.ai.uga.edu, 7</td>\n",
              "      <td>(, 1302</td>\n",
              "      <td>), 708</td>\n",
              "      <td>isu, 17</td>\n",
              "      <td>'', 7</td>\n",
              "      <td>k, 8</td>\n",
              "      <td>19, 38</td>\n",
              "      <td>93, 72</td>\n",
              "      <td>driver, 48</td>\n",
              "      <td>orbit, 77</td>\n",
              "      <td>:, 2795</td>\n",
              "      <td>(, 277</td>\n",
              "      <td>cubic, 7</td>\n",
              "      <td>turks, 101</td>\n",
              "      <td>?, 1260</td>\n",
              "      <td>information, 151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>medicine, 56</td>\n",
              "      <td>motorcycle, 72</td>\n",
              "      <td>@, 351</td>\n",
              "      <td>south, 8</td>\n",
              "      <td>todamhyp, 7</td>\n",
              "      <td>&gt;, 1225</td>\n",
              "      <td>&gt;, 648</td>\n",
              "      <td>machines, 17</td>\n",
              "      <td>jd, 7</td>\n",
              "      <td>mr, 8</td>\n",
              "      <td>33, 37</td>\n",
              "      <td>organization, 28</td>\n",
              "      <td>driving, 46</td>\n",
              "      <td>moon, 76</td>\n",
              "      <td>@, 2706</td>\n",
              "      <td>drive, 247</td>\n",
              "      <td>3-, 7</td>\n",
              "      <td>policy, 95</td>\n",
              "      <td>:, 1133</td>\n",
              "      <td>system, 149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cause, 55</td>\n",
              "      <td>bikes, 69</td>\n",
              "      <td>'s, 333</td>\n",
              "      <td>le, 8</td>\n",
              "      <td>charles.unlv.edu, 7</td>\n",
              "      <td>@, 1217</td>\n",
              "      <td>@, 618</td>\n",
              "      <td>exnet.iastate.edu, 16</td>\n",
              "      <td>/, 7</td>\n",
              "      <td>e, 8</td>\n",
              "      <td>8, 37</td>\n",
              "      <td>newsgroups, 28</td>\n",
              "      <td>ford, 45</td>\n",
              "      <td>shuttle, 59</td>\n",
              "      <td>&gt;, 2571</td>\n",
              "      <td>card, 212</td>\n",
              "      <td>2-, 7</td>\n",
              "      <td>armenia, 95</td>\n",
              "      <td>@, 1042</td>\n",
              "      <td>government, 139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>patients, 54</td>\n",
              "      <td>road, 45</td>\n",
              "      <td>game, 318</td>\n",
              "      <td>=, 8</td>\n",
              "      <td>40, 6</td>\n",
              "      <td>``, 1146</td>\n",
              "      <td>'', 613</td>\n",
              "      <td>promiscuous, 15</td>\n",
              "      <td>//, 7</td>\n",
              "      <td>l, 8</td>\n",
              "      <td>13, 36</td>\n",
              "      <td>subject, 27</td>\n",
              "      <td>drive, 43</td>\n",
              "      <td>flight, 59</td>\n",
              "      <td>?, 2362</td>\n",
              "      <td>@, 207</td>\n",
              "      <td>cusp, 6</td>\n",
              "      <td>armenians, 93</td>\n",
              "      <td>thanks, 790</td>\n",
              "      <td>public, 132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>symptoms, 46</td>\n",
              "      <td>front, 45</td>\n",
              "      <td>!, 314</td>\n",
              "      <td>il, 7</td>\n",
              "      <td>photography, 6</td>\n",
              "      <td>'', 1116</td>\n",
              "      <td>?, 562</td>\n",
              "      <td>exotic, 15</td>\n",
              "      <td>ms, 7</td>\n",
              "      <td>aj, 8</td>\n",
              "      <td>14, 35</td>\n",
              "      <td>wilson, 24</td>\n",
              "      <td>driven, 42</td>\n",
              "      <td>pat, 58</td>\n",
              "      <td>writes, 2214</td>\n",
              "      <td>:, 201</td>\n",
              "      <td>bj, 6</td>\n",
              "      <td>arab, 91</td>\n",
              "      <td>&gt;, 655</td>\n",
              "      <td>keys, 129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>diet, 44</td>\n",
              "      <td>bmw, 44</td>\n",
              "      <td>team, 309</td>\n",
              "      <td>smtp, 7</td>\n",
              "      <td>&gt;, 6</td>\n",
              "      <td>writes, 1109</td>\n",
              "      <td>writes, 549</td>\n",
              "      <td>z1dan, 14</td>\n",
              "      <td>-, 7</td>\n",
              "      <td>mp, 8</td>\n",
              "      <td>26, 34</td>\n",
              "      <td>fri, 22</td>\n",
              "      <td>honda, 41</td>\n",
              "      <td>science, 44</td>\n",
              "      <td>n't, 2075</td>\n",
              "      <td>sale, 201</td>\n",
              "      <td>detecting, 6</td>\n",
              "      <td>war, 90</td>\n",
              "      <td>anyone, 627</td>\n",
              "      <td>security, 124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>blood, 41</td>\n",
              "      <td>rider, 42</td>\n",
              "      <td>&gt;, 305</td>\n",
              "      <td>220, 7</td>\n",
              "      <td>ii, 6</td>\n",
              "      <td>?, 1058</td>\n",
              "      <td>``, 541</td>\n",
              "      <td>dod, 14</td>\n",
              "      <td>\\x/, 7</td>\n",
              "      <td>sl, 8</td>\n",
              "      <td>18, 34</td>\n",
              "      <td>thu, 21</td>\n",
              "      <td>wheel, 38</td>\n",
              "      <td>lunar, 44</td>\n",
              "      <td>&lt;, 1985</td>\n",
              "      <td>2, 168</td>\n",
              "      <td>su, 6</td>\n",
              "      <td>jewish, 89</td>\n",
              "      <td>'s, 583</td>\n",
              "      <td>secure, 116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>gordon, 41</td>\n",
              "      <td>seat, 40</td>\n",
              "      <td>?, 269</td>\n",
              "      <td>m., 7</td>\n",
              "      <td>tm, 6</td>\n",
              "      <td>n't, 1058</td>\n",
              "      <td>god, 506</td>\n",
              "      <td>exciting, 14</td>\n",
              "      <td>], 7</td>\n",
              "      <td>1, 7</td>\n",
              "      <td>41, 33</td>\n",
              "      <td>tony, 19</td>\n",
              "      <td>toyota, 38</td>\n",
              "      <td>cost, 43</td>\n",
              "      <td>article, 1911</td>\n",
              "      <td>price, 160</td>\n",
              "      <td>pp, 6</td>\n",
              "      <td>', 89</td>\n",
              "      <td>would, 582</td>\n",
              "      <td>phone, 115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>foods, 39</td>\n",
              "      <td>rear, 38</td>\n",
              "      <td>writes, 267</td>\n",
              "      <td>sombody, 6</td>\n",
              "      <td>ip, 6</td>\n",
              "      <td>&lt;, 1009</td>\n",
              "      <td>n't, 506</td>\n",
              "      <td>unusual, 13</td>\n",
              "      <td>mm, 6</td>\n",
              "      <td>h, 7</td>\n",
              "      <td>25, 32</td>\n",
              "      <td>tue, 17</td>\n",
              "      <td>tires, 36</td>\n",
              "      <td>mission, 40</td>\n",
              "      <td>'s, 1774</td>\n",
              "      <td>apple, 160</td>\n",
              "      <td>4-, 6</td>\n",
              "      <td>armenian, 84</td>\n",
              "      <td>know, 574</td>\n",
              "      <td>secret, 109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>doctors, 39</td>\n",
              "      <td>helmet, 38</td>\n",
              "      <td>&lt;, 262</td>\n",
              "      <td>en, 6</td>\n",
              "      <td>1., 6</td>\n",
              "      <td>'s, 1001</td>\n",
              "      <td>'s, 495</td>\n",
              "      <td>p., 12</td>\n",
              "      <td>mo, 6</td>\n",
              "      <td>c, 7</td>\n",
              "      <td>9, 32</td>\n",
              "      <td>message-id, 16</td>\n",
              "      <td>model, 36</td>\n",
              "      <td>station, 38</td>\n",
              "      <td>one, 1511</td>\n",
              "      <td>mac, 158</td>\n",
              "      <td>kk, 6</td>\n",
              "      <td>soviet, 83</td>\n",
              "      <td>n't, 555</td>\n",
              "      <td>data, 108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>msg, 38</td>\n",
              "      <td>honda, 36</td>\n",
              "      <td>year, 243</td>\n",
              "      <td>jurgen, 6</td>\n",
              "      <td>proves, 6</td>\n",
              "      <td>article, 998</td>\n",
              "      <td>one, 448</td>\n",
              "      <td>+, 12</td>\n",
              "      <td>mu, 6</td>\n",
              "      <td>,, 7</td>\n",
              "      <td>28, 31</td>\n",
              "      <td>mon, 16</td>\n",
              "      <td>rear, 36</td>\n",
              "      <td>development, 37</td>\n",
              "      <td>'', 1435</td>\n",
              "      <td>computer, 149</td>\n",
              "      <td>ferdinand, 6</td>\n",
              "      <td>soldiers, 82</td>\n",
              "      <td>please, 539</td>\n",
              "      <td>1993, 99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>patient, 37</td>\n",
              "      <td>yamaha, 30</td>\n",
              "      <td>games, 235</td>\n",
              "      <td>snooping, 6</td>\n",
              "      <td>sq, 5</td>\n",
              "      <td>people, 941</td>\n",
              "      <td>would, 421</td>\n",
              "      <td>}, 11</td>\n",
              "      <td>\\\\, 6</td>\n",
              "      <td>0, 7</td>\n",
              "      <td>29, 30</td>\n",
              "      <td>undefined, 16</td>\n",
              "      <td>road, 35</td>\n",
              "      <td>spacecraft, 36</td>\n",
              "      <td>would, 1416</td>\n",
              "      <td>please, 142</td>\n",
              "      <td>ez, 5</td>\n",
              "      <td>escape, 79</td>\n",
              "      <td>use, 527</td>\n",
              "      <td>nsa, 92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>banks, 37</td>\n",
              "      <td>cage, 30</td>\n",
              "      <td>play, 230</td>\n",
              "      <td>larry, 6</td>\n",
              "      <td>krillean, 5</td>\n",
              "      <td>would, 915</td>\n",
              "      <td>article, 414</td>\n",
              "      <td>null, 11</td>\n",
              "      <td>$, 6</td>\n",
              "      <td>mw, 7</td>\n",
              "      <td>23, 30</td>\n",
              "      <td>mark, 15</td>\n",
              "      <td>transmission, 34</td>\n",
              "      <td>solar, 35</td>\n",
              "      <td>``, 1274</td>\n",
              "      <td>scsi, 141</td>\n",
              "      <td>characterization, 5</td>\n",
              "      <td>work, 79</td>\n",
              "      <td>get, 495</td>\n",
              "      <td>technology, 91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>eat, 37</td>\n",
              "      <td>|, 28</td>\n",
              "      <td>last, 211</td>\n",
              "      <td>wave, 5</td>\n",
              "      <td>1993apr19.205615.1013, 5</td>\n",
              "      <td>one, 713</td>\n",
              "      <td>&lt;, 390</td>\n",
              "      <td>printf, 11</td>\n",
              "      <td>ot, 6</td>\n",
              "      <td>r, 7</td>\n",
              "      <td>5, 29</td>\n",
              "      <td>ncratl.atlantaga.ncr.com, 14</td>\n",
              "      <td>vehicle, 32</td>\n",
              "      <td>project, 34</td>\n",
              "      <td>like, 1170</td>\n",
              "      <td>offer, 141</td>\n",
              "      <td>maureen, 5</td>\n",
              "      <td>single, 75</td>\n",
              "      <td>!, 490</td>\n",
              "      <td>systems, 89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>health, 37</td>\n",
              "      <td>ama, 28</td>\n",
              "      <td>n't, 200</td>\n",
              "      <td>anthony, 5</td>\n",
              "      <td>1m, 5</td>\n",
              "      <td>think, 600</td>\n",
              "      <td>people, 385</td>\n",
              "      <td>int, 10</td>\n",
              "      <td>um, 6</td>\n",
              "      <td>%, 7</td>\n",
              "      <td>36, 28</td>\n",
              "      <td>mwilson, 14</td>\n",
              "      <td>mileage, 31</td>\n",
              "      <td>rocket, 33</td>\n",
              "      <td>..., 1113</td>\n",
              "      <td>new, 139</td>\n",
              "      <td>5-, 5</td>\n",
              "      <td>closed, 74</td>\n",
              "      <td>like, 489</td>\n",
              "      <td>privacy, 88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>causes, 37</td>\n",
              "      <td>gear, 27</td>\n",
              "      <td>season, 200</td>\n",
              "      <td>botz, 5</td>\n",
              "      <td>unlv.edu, 5</td>\n",
              "      <td>..., 587</td>\n",
              "      <td>say, 343</td>\n",
              "      <td>contains, 10</td>\n",
              "      <td>nf, 6</td>\n",
              "      <td>j, 7</td>\n",
              "      <td>32, 28</td>\n",
              "      <td>21, 14</td>\n",
              "      <td>dealer, 28</td>\n",
              "      <td>satellite, 32</td>\n",
              "      <td>know, 1105</td>\n",
              "      <td>controller, 139</td>\n",
              "      <td>mk, 5</td>\n",
              "      <td>longer, 74</td>\n",
              "      <td>windows, 483</td>\n",
              "      <td>escrow, 87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Topic 1         Topic 2      Topic 3      Topic 4   \n",
              "0     disease, 80       bike, 196       ., 546       de, 15  \\\n",
              "1     medical, 78        dod, 131       ,, 498     comp, 12   \n",
              "2      doctor, 61       ride, 109       ), 412       van, 9   \n",
              "3        food, 61      riding, 90       (, 411    howard, 8   \n",
              "4   treatment, 58           #, 88       :, 401      marc, 8   \n",
              "5    medicine, 56  motorcycle, 72       @, 351     south, 8   \n",
              "6       cause, 55       bikes, 69      's, 333        le, 8   \n",
              "7    patients, 54        road, 45    game, 318         =, 8   \n",
              "8    symptoms, 46       front, 45       !, 314        il, 7   \n",
              "9        diet, 44         bmw, 44    team, 309      smtp, 7   \n",
              "10      blood, 41       rider, 42       >, 305       220, 7   \n",
              "11     gordon, 41        seat, 40       ?, 269        m., 7   \n",
              "12      foods, 39        rear, 38  writes, 267   sombody, 6   \n",
              "13    doctors, 39      helmet, 38       <, 262        en, 6   \n",
              "14        msg, 38       honda, 36    year, 243    jurgen, 6   \n",
              "15    patient, 37      yamaha, 30   games, 235  snooping, 6   \n",
              "16      banks, 37        cage, 30    play, 230     larry, 6   \n",
              "17        eat, 37           |, 28    last, 211      wave, 5   \n",
              "18     health, 37         ama, 28     n't, 200   anthony, 5   \n",
              "19     causes, 37        gear, 27  season, 200      botz, 5   \n",
              "\n",
              "                     Topic 5       Topic 6       Topic 7   \n",
              "0                 georgia, 7       ., 1549        ., 805  \\\n",
              "1                    huey, 7       ,, 1537        ,, 792   \n",
              "2                  athens, 7       :, 1369        :, 735   \n",
              "3                mcovingt, 7       ), 1354        (, 709   \n",
              "4       aisun3.ai.uga.edu, 7       (, 1302        ), 708   \n",
              "5                todamhyp, 7       >, 1225        >, 648   \n",
              "6        charles.unlv.edu, 7       @, 1217        @, 618   \n",
              "7                      40, 6      ``, 1146       '', 613   \n",
              "8             photography, 6      '', 1116        ?, 562   \n",
              "9                       >, 6  writes, 1109   writes, 549   \n",
              "10                     ii, 6       ?, 1058       ``, 541   \n",
              "11                     tm, 6     n't, 1058      god, 506   \n",
              "12                     ip, 6       <, 1009      n't, 506   \n",
              "13                     1., 6      's, 1001       's, 495   \n",
              "14                 proves, 6  article, 998      one, 448   \n",
              "15                     sq, 5   people, 941    would, 421   \n",
              "16               krillean, 5    would, 915  article, 414   \n",
              "17  1993apr19.205615.1013, 5      one, 713        <, 390   \n",
              "18                     1m, 5    think, 600   people, 385   \n",
              "19               unlv.edu, 5      ..., 587      say, 343   \n",
              "\n",
              "                  Topic 8 Topic 9 Topic 10 Topic 11   \n",
              "0              viking, 18  \\/, 10     +, 9   11, 47  \\\n",
              "1                1066, 18   o\\, 9     #, 9   16, 44   \n",
              "2            sorenson, 17    q, 9     u, 9    6, 42   \n",
              "3             distant, 17    x, 8     g, 9   12, 41   \n",
              "4                 isu, 17   '', 7     k, 8   19, 38   \n",
              "5            machines, 17   jd, 7    mr, 8   33, 37   \n",
              "6   exnet.iastate.edu, 16    /, 7     e, 8    8, 37   \n",
              "7         promiscuous, 15   //, 7     l, 8   13, 36   \n",
              "8              exotic, 15   ms, 7    aj, 8   14, 35   \n",
              "9               z1dan, 14    -, 7    mp, 8   26, 34   \n",
              "10                dod, 14  \\x/, 7    sl, 8   18, 34   \n",
              "11           exciting, 14    ], 7     1, 7   41, 33   \n",
              "12            unusual, 13   mm, 6     h, 7   25, 32   \n",
              "13                 p., 12   mo, 6     c, 7    9, 32   \n",
              "14                  +, 12   mu, 6     ,, 7   28, 31   \n",
              "15                  }, 11   \\\\, 6     0, 7   29, 30   \n",
              "16               null, 11    $, 6    mw, 7   23, 30   \n",
              "17             printf, 11   ot, 6     r, 7    5, 29   \n",
              "18                int, 10   um, 6     %, 7   36, 28   \n",
              "19           contains, 10   nf, 6     j, 7   32, 28   \n",
              "\n",
              "                        Topic 12          Topic 13         Topic 14   \n",
              "0                       apr, 172          car, 236       space, 188  \\\n",
              "1                      1993, 111         cars, 147        earth, 92   \n",
              "2                       gmt, 109        engine, 62         nasa, 88   \n",
              "3                       date, 77         miles, 61       launch, 84   \n",
              "4                         93, 72        driver, 48        orbit, 77   \n",
              "5               organization, 28       driving, 46         moon, 76   \n",
              "6                 newsgroups, 28          ford, 45      shuttle, 59   \n",
              "7                    subject, 27         drive, 43       flight, 59   \n",
              "8                     wilson, 24        driven, 42          pat, 58   \n",
              "9                        fri, 22         honda, 41      science, 44   \n",
              "10                       thu, 21         wheel, 38        lunar, 44   \n",
              "11                      tony, 19        toyota, 38         cost, 43   \n",
              "12                       tue, 17         tires, 36      mission, 40   \n",
              "13                message-id, 16         model, 36      station, 38   \n",
              "14                       mon, 16          rear, 36  development, 37   \n",
              "15                 undefined, 16          road, 35   spacecraft, 36   \n",
              "16                      mark, 15  transmission, 34        solar, 35   \n",
              "17  ncratl.atlantaga.ncr.com, 14       vehicle, 32      project, 34   \n",
              "18                   mwilson, 14       mileage, 31       rocket, 33   \n",
              "19                        21, 14        dealer, 28    satellite, 32   \n",
              "\n",
              "         Topic 15         Topic 16             Topic 17       Topic 18   \n",
              "0         ., 3400           $, 407            curve, 10    israel, 160  \\\n",
              "1         ,, 3316           ., 405            bezier, 9   israeli, 129   \n",
              "2         (, 3032           ,, 341       calculating, 7   turkish, 110   \n",
              "3         ), 3019           ), 281            curves, 7      jews, 103   \n",
              "4         :, 2795           (, 277             cubic, 7     turks, 101   \n",
              "5         @, 2706       drive, 247                3-, 7     policy, 95   \n",
              "6         >, 2571        card, 212                2-, 7    armenia, 95   \n",
              "7         ?, 2362           @, 207              cusp, 6  armenians, 93   \n",
              "8    writes, 2214           :, 201                bj, 6       arab, 91   \n",
              "9       n't, 2075        sale, 201         detecting, 6        war, 90   \n",
              "10        <, 1985           2, 168                su, 6     jewish, 89   \n",
              "11  article, 1911       price, 160                pp, 6          ', 89   \n",
              "12       's, 1774       apple, 160                4-, 6   armenian, 84   \n",
              "13      one, 1511         mac, 158                kk, 6     soviet, 83   \n",
              "14       '', 1435    computer, 149         ferdinand, 6   soldiers, 82   \n",
              "15    would, 1416      please, 142                ez, 5     escape, 79   \n",
              "16       ``, 1274        scsi, 141  characterization, 5       work, 79   \n",
              "17     like, 1170       offer, 141           maureen, 5     single, 75   \n",
              "18      ..., 1113         new, 139                5-, 5     closed, 74   \n",
              "19     know, 1105  controller, 139                mk, 5     longer, 74   \n",
              "\n",
              "        Topic 19          Topic 20  \n",
              "0        ., 1941          key, 224  \n",
              "1        ,, 1779   encryption, 168  \n",
              "2        ), 1343         chip, 162  \n",
              "3        (, 1295      clipper, 161  \n",
              "4        ?, 1260  information, 151  \n",
              "5        :, 1133       system, 149  \n",
              "6        @, 1042   government, 139  \n",
              "7    thanks, 790       public, 132  \n",
              "8         >, 655         keys, 129  \n",
              "9    anyone, 627     security, 124  \n",
              "10       's, 583       secure, 116  \n",
              "11    would, 582        phone, 115  \n",
              "12     know, 574       secret, 109  \n",
              "13      n't, 555         data, 108  \n",
              "14   please, 539          1993, 99  \n",
              "15      use, 527           nsa, 92  \n",
              "16      get, 495    technology, 91  \n",
              "17        !, 490       systems, 89  \n",
              "18     like, 489       privacy, 88  \n",
              "19  windows, 483        escrow, 87  "
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_words_per_topic = get_top_words(nkw, id_to_word, top_n=20, method=\"raw\")\n",
        "df_top_words = top_words_to_df(top_words_per_topic)\n",
        "df_top_words"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}