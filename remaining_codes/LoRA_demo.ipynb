{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/LoRA_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce702684-1a4f-4d39-b359-41466c77c9b1",
      "metadata": {
        "id": "ce702684-1a4f-4d39-b359-41466c77c9b1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cse.chalmers.se/~richajo/diverse/l7/books.data\n",
        "!wget https://www.cse.chalmers.se/~richajo/diverse/l7/s7_pretrained.model"
      ],
      "metadata": {
        "id": "L3RDZnC30TeX",
        "outputId": "47fac2bc-1df8-4cfb-f704-19d018f61af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "L3RDZnC30TeX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-25 10:00:52--  https://www.cse.chalmers.se/~richajo/diverse/l7/books.data\n",
            "Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\n",
            "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6192712 (5.9M)\n",
            "Saving to: ‘books.data.1’\n",
            "\n",
            "books.data.1        100%[===================>]   5.91M  6.36MB/s    in 0.9s    \n",
            "\n",
            "2024-11-25 10:00:54 (6.36 MB/s) - ‘books.data.1’ saved [6192712/6192712]\n",
            "\n",
            "--2024-11-25 10:00:54--  https://www.cse.chalmers.se/~richajo/diverse/l7/s7_pretrained.model\n",
            "Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\n",
            "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1580288 (1.5M)\n",
            "Saving to: ‘s7_pretrained.model.1’\n",
            "\n",
            "s7_pretrained.model 100%[===================>]   1.51M  2.12MB/s    in 0.7s    \n",
            "\n",
            "2024-11-25 10:00:55 (2.12 MB/s) - ‘s7_pretrained.model.1’ saved [1580288/1580288]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b00774a-d0fc-4f64-b7ad-c1c40c9413cc",
      "metadata": {
        "id": "6b00774a-d0fc-4f64-b7ad-c1c40c9413cc",
        "outputId": "07d9014c-0583-43c9-bb7e-2b22b8ef7a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (2000, 768)\n",
            "Y length: 2000\n"
          ]
        }
      ],
      "source": [
        "with open('books.data', 'rb') as f:\n",
        "    books_X, books_Y = pickle.load(f)\n",
        "\n",
        "print('X shape:', books_X.shape)\n",
        "print('Y length:', len(books_Y))\n",
        "\n",
        "split_ix = 1500\n",
        "books_X_tr = books_X[:split_ix]\n",
        "books_Y_tr = books_Y[:split_ix]\n",
        "books_X_te = books_X[split_ix:]\n",
        "books_Y_te = books_Y[split_ix:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e8f3c4-dd91-4166-93cd-dc66fc891267",
      "metadata": {
        "id": "16e8f3c4-dd91-4166-93cd-dc66fc891267",
        "outputId": "d99ea0e6-0a46-4f96-e60a-5321c4741b9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-54180ca6f566>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained = torch.load('s7_pretrained.model')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "pretrained = torch.load('s7_pretrained.model')\n",
        "pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c035ab-2229-4313-a48b-a7da8590c687",
      "metadata": {
        "id": "56c035ab-2229-4313-a48b-a7da8590c687"
      },
      "outputs": [],
      "source": [
        "def batcher(batch):\n",
        "    X = torch.as_tensor([x for x, _ in batch])\n",
        "    Y = 1.0*torch.as_tensor([y for _, y in batch])\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c38f361-4a51-41d4-bf52-554f8d9b1e2c",
      "metadata": {
        "id": "7c38f361-4a51-41d4-bf52-554f8d9b1e2c"
      },
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "    dl = DataLoader(list(zip(books_X_te, books_Y_te)), batch_size=32, shuffle=False, collate_fn=batcher)\n",
        "    n_corr = 0\n",
        "    for Xb, Yb in dl:\n",
        "        with torch.no_grad():\n",
        "            model_out = model(Xb)\n",
        "        preds = model_out[:, 0] > 0\n",
        "        gold = Yb > 0\n",
        "        n_corr += sum(preds == gold).item()\n",
        "    return n_corr / len(books_Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81753ec-3fca-40a2-b1af-3c484b4f3fac",
      "metadata": {
        "id": "f81753ec-3fca-40a2-b1af-3c484b4f3fac",
        "outputId": "505e038b-38cf-4c71-8978-c6d5848ea42b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.794"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "eval_model(pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c391fe32-19ac-44bf-98f7-18faf6666024",
      "metadata": {
        "id": "c391fe32-19ac-44bf-98f7-18faf6666024"
      },
      "source": [
        "# Basic fine-tuning\n",
        "\n",
        "We create a new model where we copy the weights from the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5535598-361e-46dd-9367-31e3e98021bf",
      "metadata": {
        "id": "f5535598-361e-46dd-9367-31e3e98021bf"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "finetuned = nn.Sequential(\n",
        "    nn.Linear(in_features=768, out_features=512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=1)\n",
        ")\n",
        "\n",
        "# pretrained = torch.load('s7_pretrained.model')\n",
        "\n",
        "finetuned[0].weight.data = pretrained[0].weight.data.clone()\n",
        "finetuned[0].bias.data = pretrained[0].bias.data.clone()\n",
        "finetuned[2].weight.data = pretrained[2].weight.data.clone()\n",
        "finetuned[2].bias.data = pretrained[2].bias.data.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd587f2-b291-4f46-b170-44b1f33c48b3",
      "metadata": {
        "id": "bdd587f2-b291-4f46-b170-44b1f33c48b3",
        "outputId": "c1b1ad34-cd90-4ca2-fa65-3418ebc05fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.794"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "eval_model(finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a038088a-0123-4d91-894c-cc1dcb8f5473",
      "metadata": {
        "id": "a038088a-0123-4d91-894c-cc1dcb8f5473"
      },
      "outputs": [],
      "source": [
        "def train(model, n_epochs=10):\n",
        "    dl = DataLoader(list(zip(books_X_tr, books_Y_tr)), batch_size=32, shuffle=True, collate_fn=batcher)\n",
        "\n",
        "    # NOTE!\n",
        "    params = [ p for p in model.parameters() if p.requires_grad_ ]\n",
        "\n",
        "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        for Xb, Yb in dl:\n",
        "            model_out = model(Xb)[:, 0]\n",
        "            loss = loss_fn(model_out, Yb)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        mean_loss = total_loss / len(dl)\n",
        "        acc = eval_model(model)\n",
        "        print(f'loss = {mean_loss:.4f}, acc = {acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- Complete `count_trainable_parameters` below.\n",
        "- Count the total number of trainable parameters in the model you fine-tuned.\n",
        "- Use the function `train` to fine-tune the cloned model."
      ],
      "metadata": {
        "id": "kP4ym2hY2vDI"
      },
      "id": "kP4ym2hY2vDI"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "  # TODO\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "qJ6dPDwx1JMj"
      },
      "id": "qJ6dPDwx1JMj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "913197b3-6eda-4a5e-9c23-23e4c345ea18",
      "metadata": {
        "id": "913197b3-6eda-4a5e-9c23-23e4c345ea18"
      },
      "source": [
        "# Implementing LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d538e5-8041-4efa-97d7-4b32532827c7",
      "metadata": {
        "id": "14d538e5-8041-4efa-97d7-4b32532827c7"
      },
      "outputs": [],
      "source": [
        "class LinearBlockWithLoRA(nn.Module):\n",
        "    def __init__(self, W, r):\n",
        "        \"\"\"\n",
        "        Initializes the LinearBlockWithLoRA.\n",
        "\n",
        "        Args:\n",
        "            W (torch.Tensor): Pre-trained weight matrix.\n",
        "            r (int): Rank of the low-rank approximation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the pre-trained weight matrix\n",
        "        self.W = W  # Frozen pre-trained weights\n",
        "\n",
        "        # Get the dimensions of the pre-trained weight matrix\n",
        "        out_dim, in_dim  = W.weight.shape\n",
        "\n",
        "        # Initialize the low-rank matrices A and B\n",
        "        self.A = nn.Linear(in_features=in_dim, out_features=r, bias=False)  # Low-rank adaptation A\n",
        "        self.B = nn.Linear(in_features=r, out_features=out_dim, bias=False)   # Low-rank adaptation B\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the LinearBlockWithLoRA.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying W and LoRA.\n",
        "        \"\"\"\n",
        "        # Compute the output with the pre-trained weight matrix\n",
        "        W_out = self.W(X)  # Using frozen weights\n",
        "\n",
        "        # Compute the low-rank adaptation\n",
        "        a_out = self.A(X) # (batch_size x in_dim) @ (in_dim x r) @ (r x out_dim)\n",
        "        b_out = self.B(a_out) # (batch_size x in_dim) @ (in_dim x r) @ (r x out_dim\n",
        "\n",
        "        # Add scaled adaptation to the pre-trained weights' output\n",
        "\n",
        "        return W_out + b_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- Complete `LinearBlockWithLoRA` above\n",
        "- Set up a model using this new block to replace the first linear layer. Initialize parameters from the pre-trained model. (Don't forget to switch off gradient computation for `W`.)\n",
        "- Count the parameters in the new model.\n",
        "- Train the new model."
      ],
      "metadata": {
        "id": "jTKFM5xk1-P_"
      },
      "id": "jTKFM5xk1-P_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "lora_model = nn.Sequential(\n",
        "    LinearBlockWithLoRA(pretrained[0], r=8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=1)\n",
        ")\n",
        "\n",
        "lora_model[0].W.weight.data = pretrained[0].weight.data.clone()\n",
        "lora_model[0].W.bias.data = pretrained[0].bias.data.clone()\n",
        "lora_model[2].weight.data = pretrained[2].weight.data.clone()\n",
        "lora_model[2].bias.data = pretrained[2].bias.data.clone()\n",
        "\n",
        "lora_model[0].W.requires_grad = False\n",
        "\n",
        "train(lora_model, n_epochs=10)\n",
        "\n",
        "print(count_trainable_parameters(lora_model))"
      ],
      "metadata": {
        "id": "UmNHV75Bs9E_",
        "outputId": "e2e08177-d5f9-493d-c11b-431a685e5414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UmNHV75Bs9E_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 0.3777, acc = 0.8060\n",
            "loss = 0.3460, acc = 0.8100\n",
            "loss = 0.3201, acc = 0.8180\n",
            "loss = 0.3003, acc = 0.8120\n",
            "loss = 0.2878, acc = 0.8220\n",
            "loss = 0.2665, acc = 0.8220\n",
            "loss = 0.2608, acc = 0.8280\n",
            "loss = 0.2496, acc = 0.8180\n",
            "loss = 0.2358, acc = 0.8140\n",
            "loss = 0.2304, acc = 0.8220\n",
            "404481\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}