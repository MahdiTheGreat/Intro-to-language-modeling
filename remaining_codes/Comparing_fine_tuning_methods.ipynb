{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/Comparing_fine_tuning_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeKkvvISk7-J"
      },
      "source": [
        "# Using BERT models\n",
        "\n",
        "This example shows how to use BERT models from the HuggingFace library. We will first take a look at the basic usage and how we apply the model, and then see how we can fine-tune the BERT model to classify documents.\n",
        "\n",
        "To run this example, you need to make sure that the library `transformers` is installed. Please refer to [this page](https://huggingface.co/transformers/installation.html) for installation instructions, but in short you just need to install a package with `pip`. If you use Colab, the library is already installed.\n",
        "\n",
        "**Note**: If you are using Colab, please check that the runtime type is set to GPU after copying this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke4t_1NJk7-m"
      },
      "source": [
        "### Using the BERT tokenizer\n",
        "\n",
        "BERT has a built-in tokenizer. It uses WordPiece tokenization, which means that it will split rare words into pieces.\n",
        "\n",
        "There are many models included in the `transformers` library, each one identified by a model name string. For a full list, see [here](https://huggingface.co/transformers/pretrained_models.html). We will mainly consider the *BERT* model, originally described in the [paper by Devlin et al. (2019)](https://aclanthology.org/N19-1423.pdf).\n",
        "\n",
        "Each model in the `transformers` library comes with its own corresponding tokenizer: `BertTokenizer`, `DistilBertTokenizer`, `RobertaTokenizer`, etc. You need to make sure you use the right tokenizer for your model. Fortunately, in the latest version of the library, there is a utility called `AutoTokenizer` that will load the appropriate tokenizer for the model you have selected. Let's see how to use it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "GZU5nru6TcXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2pH-ahok7-o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tokenization in Transformer models\n",
        "\n",
        "Please look at the BERT notebook for more discussions about tokenizers."
      ],
      "metadata": {
        "id": "MHq3lZN9frk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Qnjk352afnoX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "8a657f705a41483e918f827de1e182c8",
            "2691110767914279a2db15de2e481065",
            "73c7626f06c945b095d77c316d4253cc",
            "76143834564b4e3da4b6b0ddf8fb2bf7",
            "34916ee81e67499e802261d92e98072e",
            "59043f35c43c426a8bd4a74b99f07546",
            "e2ed7844eac94196b4023b19b520eb73",
            "e6758f2c5bc04a10940bd009931d2cb9",
            "277e4f9fe0ba4ae8972b3bda4e260b2c",
            "1e0cb7203c6e4d9480bc0ddb5cfb60dc",
            "3b6093b4bf0b46a3b0d85f18d71d0c30",
            "f0328b55162a4c018f46b5adce9182fb",
            "c59ac1c3f10a47009876b52bdd7a9b1b",
            "ce61dbcbdf864198bbb3ba7995ff47c0",
            "5ea838d1f98343dcb69da3a2d85b3b0c",
            "11ad1160c7a04f299a4d00e22c9aa9cc",
            "d0de8c26a0b24a03948d08d4e660b19b",
            "39caeb9480b94a5c88ab9fd658101d73",
            "f44de5a0e38a4a14bcd4059be8ce6351",
            "fdd28fbeb8fe4ada99b67c7da9102775",
            "3051c9c2c66f4669bcf7cb3a3d1e200c",
            "31f448dd9b044bac82b235906bdd2df4",
            "e3324eee64c149a6a87cf33434227194",
            "2c47c569f65647a19bfc835d14d9bc8e",
            "a08c21039cd24974b17af626cfaf36c7",
            "ebf5120f57684fb3a3b881af2a8c610d",
            "ae7172cd43b64e829357f6da2fb64eb0",
            "79e087eefc3843fdba70927f9a5e5037",
            "224acd6831d742fc9a65a3a3104d3125",
            "2dd9bfd5f6dc4bf3ab66c70a8b913f0a",
            "7fc62cfde8904ef2a5c89550535f2bee",
            "1ac77e6bee8d4dc1823dfb385d3b5046",
            "ce478d6d7fe6443dbc7deab2e15d6f96",
            "988c8d466ae549cc9e71ca4fcd3fb6aa",
            "89e4897c2ba244e6922a3cdce775cf08",
            "6133dee3f69e44c49c902999b649e37a",
            "6aa94832043a447fa9fd88decccbb14d",
            "5e0356d39a8b430ba016cb74cb647aa4",
            "5eeaa46ffdb04ea2a576cc5f0ece3471",
            "bc4618dd89dd4bf39c8b01a882a0a907",
            "2acd466994b64798a13a00b5d668c63a",
            "4cd794d4c5c2446ea7c7014e7cb73e45",
            "7c5a9f79025248fda125f2f67b878644",
            "52823944843147cb93ca16ab58163123"
          ]
        },
        "outputId": "456fb65b-6eb1-4775-f051-67f6ed9dd1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a657f705a41483e918f827de1e182c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0328b55162a4c018f46b5adce9182fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3324eee64c149a6a87cf33434227194"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "988c8d466ae549cc9e71ca4fcd3fb6aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1lbHUpPk7-4"
      },
      "source": [
        "To exemplify how the tokenizer works, let's see an example. Note that we have added the special dummy tokens BERT uses: `[CLS]` to mark the beginning of the text, `[SEP]` at the end, and some padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkoQ5_0Ik7-6"
      },
      "outputs": [],
      "source": [
        "tokenizer.tokenize('[CLS] Rolf lives in Gothenburg. [SEP] [PAD] [PAD]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rccHszNGk7_F"
      },
      "source": [
        "The following example shows how rare words are split into word pieces by the BERT tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60r74xqDk7_G"
      },
      "outputs": [],
      "source": [
        "tokenizer.tokenize('Suzanna has broken the teapots.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X8lXW68k7_O"
      },
      "source": [
        "We can also encode the tokens as integers. (Each BERT tokenizer comes with a vocabulary that carries out this mapping.) The encoding step automatically adds the `[CLS]` and `[SEP]` dummy tokens at the beginning and end, which is why we get 7 integers for 5 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu7uIr9lk7_P"
      },
      "outputs": [],
      "source": [
        "tokenizer(['Rolf lives in Gothenburg.', 'Shorter example'],\n",
        "         truncation=True, max_length=128,\n",
        "         return_tensors='pt', padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the tokenizer:"
      ],
      "metadata": {
        "id": "UcZEmJl8kotE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer('tokenization')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP56TeOMkrLG",
        "outputId": "bbcaeb82-51cf-46bd-a5ce-c4a38b86e0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [30001, 1634], 'attention_mask': [1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer('Tokenizations and miscalculations.').input_ids\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RpsN6aUkxRO",
        "outputId": "1fca6fc9-4ef4-4467-f89b-187f48627f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[30642, 4582, 290, 285, 7860, 3129, 602, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.unk_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-jJpl25LDqT",
        "outputId": "7ec7b89b-e382-41fe-c931-6410138ff27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lt8EUHsUk9Q4",
        "outputId": "050e315e-461d-42ba-add6-6ed1ac77585c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tokenizations and miscalculations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in encoded:\n",
        "  print(f'{i}\\t|{tokenizer.decode(i)}|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pdjhmx3lEjG",
        "outputId": "b9704e98-dab9-493c-e562-a5865d19133a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30642\t|Token|\n",
            "4582\t|izations|\n",
            "290\t| and|\n",
            "285\t| m|\n",
            "7860\t|iscal|\n",
            "3129\t|cul|\n",
            "602\t|ations|\n",
            "13\t|.|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer('Testing PyTorch.', return_tensors='pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWm15zZXlu35",
        "outputId": "2d48148a-681c-4d0e-a4c3-e199a1ee9515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[44154,  9485, 15884,   354,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLin5VfxAZ0T"
      },
      "source": [
        "# Loading the GPT-2 model from the library\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "As we saw in the other notebook, we generally use the 'Auto' classes in Transformers to load models, and provide a model name. In this case, we specity `gpt2` to load the GPT-2 model.\n",
        "\n",
        "One small detail: in this case, we want to use GPT-2 as a *language model* -- that is, we want to apply it in a left-to-right fashion, in each step computing a distribution over the possible next words. For this type of application, we use `AutoModelForCausalLM` instead of `AutoModel` which is a pure language model. (Terminological note: The term *causal* LM simply means autoregressive LM in this context. It is unrelated to causality-based machine learning models.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCUD7SQQk7_u"
      },
      "source": [
        "### Trying out a BERT model from the Transformers library\n",
        "\n",
        "For the tokenizer above, we used `AutoTokenizer` to load the right tokenizer, given a model name string. In a similar fashion, we use `AutoModel` here to load the right model. If you want to use another model, just change the name string in the hyperparameter settings below. (As mentioned above, see [this page](https://huggingface.co/transformers/pretrained_models.html) for a list of pre-trained models.) We'll typically get a warning message here, which we can ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaOX-PD2ALIn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoModelForCausalLM\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "example_bert_model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For illustration, we can also print the layers of the model."
      ],
      "metadata": {
        "id": "GCkk4r6Nms7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQK3NAjZmqJ_",
        "outputId": "42d47a00-9714-4af5-fbc2-f5ab5f2a94d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing next-word probabilities\n",
        "\n",
        "Let's exemplify how we can compute the next-word (unnormalized log) probabilities.\n",
        "\n",
        "We tokenize and integer-encode an example text and see how the model predicts that this sentence will continue."
      ],
      "metadata": {
        "id": "zAp18ZfXmcAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer('I went to the bar to buy some', return_tensors='pt').input_ids\n",
        "\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKPdG3mpmjaJ",
        "outputId": "e6c70e51-6421-42e5-9c4d-f352dd605b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  40, 1816,  284,  262, 2318,  284, 2822,  617]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the model and it returns logits (unnormalized log probabilities). This tensor has the shape `(batch_size, n_words, voc_size)`."
      ],
      "metadata": {
        "id": "2bvY06hHqc-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = example_bert_model(input_ids).logits"
      ],
      "metadata": {
        "id": "Y3AGQv4Om7DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pick out the distribution for the last position and then find the index of the highest-scoring next word. This happens to be index 6099."
      ],
      "metadata": {
        "id": "b7JP_5bkqu46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_probable_word_id = outputs[0, -1].argmax()\n",
        "\n",
        "most_probable_word_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRSVjFkGm-jG",
        "outputId": "c664b3c2-6fad-417b-9093-2b0e2c3a8694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6099)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and index 6099 corresponds to the word \"beer\"."
      ],
      "metadata": {
        "id": "hQ3DxbEorG1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(most_probable_word_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nUzXTq-InFjT",
        "outputId": "8c961108-484a-461b-b7e0-cd640e01b13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' beer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can *sample* from the distribution instead of finding the highest-scoring word. We can adjust the temperature to control the \"randomness\" of this sampling."
      ],
      "metadata": {
        "id": "3cA8v1mhrKCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "temperature = 0.5\n",
        "\n",
        "dist = Categorical(logits=outputs[0, -1, :] / temperature)\n",
        "\n",
        "sampled = dist.sample()\n",
        "\n",
        "tokenizer.decode(sampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KyJ2nvZEnVX7",
        "outputId": "ebe545b8-dad3-4e24-a2ea-362f7a77e8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' drinks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5MLnRvsBtkW"
      },
      "source": [
        "# Using the built-in generation function\n",
        "\n",
        "However, in most applications we are not going to do the generation step-by-step as above, but simply use the built-in function `generate`.\n",
        "\n",
        "If we simply call the `generate` method without any options, we will use *greedy* autoregressive decoding. For each step, we select the word (piece) that looks most likely, given the previous context.\n",
        "\n",
        "The tensor `input_ids` corresponds to the *prompt* or prefix used to initialize the generation. By default, `generate` simply returns a tensor corresponding to the generated output. In our case, it will have the shape `(1, n)` where `n` is the length of the generated text. This tensor has one row because greedy decoding returns a single text.\n",
        "\n",
        "Let's see what happens with a simple example! What is your impression? Do you think the GPT-2 model is close to [strong AI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) or do we have a [*stochastic parrot*](https://dl.acm.org/doi/10.1145/3442188.3445922)? Any particular properties that you think are \"odd\" here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buGq1x6a73qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f039787e-d0a1-4957-b990-434d8aaafe81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I told him that I was going to be a part of the team. He said, 'I'm going to be a part of the team.' I said, 'I'm going to be a part of the team.' He said, 'I\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "input_ids = tokenizer('I told him that', return_tensors='pt').input_ids\n",
        "\n",
        "outputs = example_bert_model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    pad_token_id=0\n",
        ")\n",
        "\n",
        "tokenizer.decode(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn2YaG0brsy9"
      },
      "source": [
        "### Improving the generated output\n",
        "\n",
        "**Beam search.** Greedy decoding will not necessarily find the most probable sequence according to the LM. *Beam search* can mitigate some of this problem. In beam search, we keep several alternative hypotheses and update each of them in each step.\n",
        "\n",
        "To use beam search here, simply add the option `num_beams` and set it to an integer greater than 1. Since the beam search generates several texts, you may also add the option `num_return_sequences` and set it to something greater than 1 (but at most equal to the number of beams).\n",
        "\n",
        "Does beam search seem to give you better generated texts than before?\n",
        "\n",
        "**Avoiding repetition.** Probably, you previously encountered the problem that the decoder goes into an infinite loop and generates repetetive phrases. (Do you have an intuition for why this could be the case?) We may avoid this simply by forcing the model not to generate subsequences we have already seen. If you add the option `no_repeat_ngram_size` and set it to e.g. 2, then we prevent the model from generating previously seen bigrams (word pairs).\n",
        "\n",
        "**Sampling.** Another way to improve the diversity of generated texts and to make them less bland is to introduce some randomness when generating. If set the option `do_sample` to `True`, we *sample* from the distribution at each step instead of selecting the most likely next word.\n",
        "\n",
        "The `temperature` option (a floating-point value greater than 0) can be used to control the distribution used when sampling. When computing the probabilities over the vocabulary, the logits are divided by the `temperature` before applying the softmax. **Self-check:** What happens if `temperature` is close to 0 and what happens if it is greater than 1?\n",
        "\n",
        "Other options that control the sampling distribution include `top_k` and `top_p`. You can read about them in [this notebook](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb) if you scroll down to the section *Top-K Sampling*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfQoqoqbC8DM"
      },
      "source": [
        "We create an example input. This batch consists of a single sentence. We use the tokenizer above to encode the words as integers. (As above, special dummy tokens have been added at the beginning and end.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nU6QOy0JlHi"
      },
      "outputs": [],
      "source": [
        "test_input = tokenizer(['The sun is shining .'], return_tensors='pt')\n",
        "test_input.input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TINr4-tJlHj"
      },
      "source": [
        "We can now apply the BERT model and get the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y2V01q5JlHj"
      },
      "outputs": [],
      "source": [
        "bert_output = example_bert_model(input_ids=test_input.input_ids,\n",
        "                                 attention_mask=test_input.attention_mask,\n",
        "                                 output_attentions=True, output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROvKxduiJlHl"
      },
      "source": [
        "You can read about the output data structure here: https://huggingface.co/docs/transformers/main_classes/output\n",
        "\n",
        "The parts of this data structure that are most interesting for us are:\n",
        "\n",
        "- `last_hidden_state` (always returned)\n",
        "- `hidden_states` (returned because we set `output_hidden_states` to `True`)\n",
        "- `attentions` (returned because we set `attentions` to `True`)\n",
        "\n",
        "Take a look at these objects and try to understand why they have the shapes they have. (Note: `hidden_states` and `attentions` are tuples.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFEp3CqkJlHl"
      },
      "source": [
        "### Visualizing attention\n",
        "\n",
        "The paper [*What does BERT look at? An Analysis of BERT's Attention*](https://aclanthology.org/W19-4828.pdf) by Clark et al (2019) describes an analysis of BERT's attention heads using visualization.\n",
        "\n",
        "There is [an online tool](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional) that can help you create visualizations similar to the paper.\n",
        "\n",
        "You can also create visualizations in this notebook directly. The code below visualizes the attention scores using heatmaps.\n",
        "**Note**: keep in mind that in the paper and in the visualization tool, the layers and attention heads are numbered starting from 1. So if you want to visualize the head 11-6 as in Figure 1 in the paper, you will have to write\n",
        "```\n",
        "bert_outputs.attentions[10][0, 5]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1LuR979JlHm"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'svg'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "def plot_attention(input_ids, tokenizer, scores):\n",
        "    scores = scores.detach().numpy()\n",
        "\n",
        "    words = [tokenizer.decode(i) for i in input_ids]\n",
        "\n",
        "    if tokenizer.pad_token in words:\n",
        "        ix = words.index(tokenizer.pad_token)\n",
        "        words = words[:ix]\n",
        "        scores = scores[:ix, :ix]\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "        fig, ax = plt.subplots()\n",
        "        heatmap = ax.pcolor(scores, cmap='Blues_r')\n",
        "\n",
        "        ax.set_xticklabels(words, minor=False, rotation='vertical')\n",
        "        ax.set_yticklabels(words, minor=False)\n",
        "\n",
        "        ax.xaxis.tick_top()\n",
        "        ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
        "        ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        plt.colorbar(heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frpbILR1JlHn"
      },
      "source": [
        "For instance, this code shows how head 1 in layer 3 attends to the next token as exemplified in Figure 1 in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP5AxmhHJlHn"
      },
      "outputs": [],
      "source": [
        "layer = 3\n",
        "head = 1\n",
        "sentence = 0\n",
        "plot_attention(test_input.input_ids[sentence], tokenizer, bert_output.attentions[layer-1][sentence, head-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHYdGiGyJlHo"
      },
      "source": [
        "Try out the visualization tool or the heatmap visualization function in this notebook and try to see if you can see similar effects as described in the paper.\n",
        "\n",
        "Do you think we can learn anything about a model by this kind of inspection? There have been debates in the research community, for instance in the following papers:\n",
        "- Jain and Wallace (2019) [*Attention is not explanation*](https://aclanthology.org/N19-1357.pdf)\n",
        "- Wiegreffe and Pinter (2019) [*Attention is not not explanation*](https://aclanthology.org/D19-1002.pdf)\n",
        "- Bastings and Filippova (2020) [*The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?*](https://aclanthology.org/2020.blackboxnlp-1.14.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhHdx2vMJlHp"
      },
      "source": [
        "### Masked language modeling\n",
        "\n",
        "The BERT model was trained in a multitask fashion where one of the two training tasks is *masked language modeling*: for some tokens in the sentence, we substitute the dummy symbol `[MASK]` and then train the model to guess the missing words.\n",
        "\n",
        "This masked language model can also be loaded via the library: we just need to use `AutoModelForMaskedLM` in this case. This loads the BERT model again, but also includes the \"head\" that predicts the missing tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfA7Be5YJlHq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "mlm = AutoModelForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zknY4c1FJlHq"
      },
      "source": [
        "The output of applying `mlm` is a data structure where the relevant part is a tensor called `logits`. What is the shape of this tensor?\n",
        "\n",
        "Can you see what the BERT model predicts for the masked token in the sentence below?\n",
        "\n",
        "**Hint:** In this case, the masked token is at position 5 (because a `[CLS]` token is added at position 0). In general, you can write something like the following to find the position of the masked token:\n",
        "```\n",
        "mask_position = list(masked_sentence.input_ids[0]).index(tokenizer.mask_token_id)\n",
        "```\n",
        "\n",
        "**Hint:** You may found it useful to apply either [`argmax`](https://pytorch.org/docs/stable/generated/torch.argmax.html) or [`topk`](https://pytorch.org/docs/stable/generated/torch.topk.html).\n",
        "\n",
        "**Hint:** If you know the index `i` in the vocabulary for some token, you can find the corresponding word string by calling `tokenizer.decode(i)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhW-TziiJlHr"
      },
      "outputs": [],
      "source": [
        "masked_sentence = tokenizer('I went to the [MASK] to learn how to code.', return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIDzJwdZJlHr"
      },
      "source": [
        "In the paper by Petroni et al. (2019) called [*Language Models as Knowledge Bases?*](https://aclanthology.org/D19-1250.pdf), it is claimed that models such as BERT implicitly store quite a bit of encyclopedic knowledge that we can probe using the masked language model. Try to probe the model using sentences such as `[MASK] is the capital of Sweden.` and see if this idea seems reasonable. Can you think of any drawbacks of this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_UbPDrJlHr"
      },
      "source": [
        "Can you think of other experiments that would be interesting to carry out using the masked language model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3seLgk0Cg4M"
      },
      "source": [
        "### Fine-tuning a BERT  model for document classification\n",
        "\n",
        "For the classification task, we'll use our usual Amazon review benchmark. Some of the code here is identical to the previous notebook on document classifiction (with CBoW and BoW), with some small changes to adapt the code for the way that BERT deals with tokenization and vocabularies.\n",
        "\n",
        "**Note**: the library includes ready-made solutions for document classification based on BERT (see [`BertForSequenceClassification`](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForSequenceClassification)), and it may seem a bit redundant that we design our own classifier on top of BERT. But the point here is that we'd like to show how to use BERT in a modular way as a general representation component, so that you can use it in other types of applications. You may also read about the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) functionality as an alternative to the plain PyTorch solution we are using here to deal with preprocessing, training loop etc.\n",
        "\n",
        "Now, let's use a BERT model as we previously used other representation components (e.g. CBoW, RNN). There's only one small issue to keep in mind here: BERT gives *word* representations, but how do we get *document* representations? The recommended solution with BERT is to extract the representation at the dummy `[CLS]` token at position 0.\n",
        "\n",
        "<img src=\"http://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png\" alt=\"BERT\" style=\"width:30%;\"/>\n",
        "\n",
        "Now, please implement the code to build a document classifier on top of a BERT model. You will first need to declare the classification head (output unit) in `__init__`, and then implement the computation in `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoIgXJd5Cv4q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "from torch import nn\n",
        "\n",
        "class TextClassifierUsingBERT(nn.Module):\n",
        "\n",
        "    def __init__(self, clf):\n",
        "        super().__init__()\n",
        "\n",
        "        print('Loading pre-trained model...')\n",
        "        self.bert_model = AutoModel.from_pretrained(clf.params.bert_model_name)\n",
        "\n",
        "        # Size of the pre-trained model's word representations.\n",
        "        hidden_size = self.bert_model.config.hidden_size\n",
        "\n",
        "        # Number of classes for the classification task.\n",
        "        nbr_classes = clf.n_classes\n",
        "\n",
        "        # Also don't forget to create a classification \"head\" (output unit).\n",
        "\n",
        "    def forward(self, Xbatch, Xmask):\n",
        "\n",
        "        # Xbatch is the document tensor of shape (n_docs, max_length), where\n",
        "        # n_docs is the number of documents in the batch, and max_length is\n",
        "        # the maximal document length in the batch. Since we truncate the\n",
        "        # documents to length 128 in this example, max_length will be 128\n",
        "        # in most cases.\n",
        "        # Xmask is the attention mask: it is 0 for the positions corresponding to\n",
        "        # the padding tokens, and 1 elsewhere.\n",
        "\n",
        "        # Your code here...\n",
        "        # As usual, print the shapes of the tensors if you are confused!\n",
        "\n",
        "        # When you have implemented this, the output shape should be (n_docs, nbr_classes)\n",
        "        # as in our previous classifiers. This output contains the logits for the classes:\n",
        "        # you do not have to apply a softmax.\n",
        "        return SOMETHING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZZ45pTLk7_3"
      },
      "source": [
        "### Running the text classifier\n",
        "\n",
        "Now, let's build the complete text classification model and train it on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFjkEA3YJlHv"
      },
      "outputs": [],
      "source": [
        "!wget http://www.cse.chalmers.se/~richajo/waspnlp2024/dredze_amazon_reviews.zip\n",
        "!unzip dredze_amazon_reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks_TJ05eJlHv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "amazon_corpus = pd.read_csv('dredze_amazon_reviews.tsv', sep='\\t', header=None, names=['product', 'sentiment', 'text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j79H6CG4JlHw"
      },
      "source": [
        "**NOTE**: we use a lightly modified version of the classification code we used in previous lectures. The cell containing this code is available below and you need to exectute that cell before this one. (We put it below to improve readability of the notebook.)\n",
        "\n",
        "In my experiments, I got accuracies around 0.88 when I truncate the documents to 128 tokens and use the distilled BERT model (a \"compressed\" BERT model that runs somewhat faster). You can get higher accuracies if you use longer documents or larger BERT models, but you should be careful in that case so that you don't run out of GPU memory.\n",
        "\n",
        "This model is completely unregularized and the models seem to overfit quite quickly, with the accuracies sometimes peaking in the first epoch. Since each epoch takes about 2 minutes on my machine, so it's of course nice that we don't need to run it for many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkOndJSnk7_5"
      },
      "outputs": [],
      "source": [
        "class TextClassifierParametersBERT:\n",
        "    val_size = 0.2\n",
        "    device = 'cuda'\n",
        "    n_epochs = 4\n",
        "    batch_size = 32\n",
        "    decay = 0\n",
        "    max_len = 128\n",
        "    bert_model_name = 'distilbert-base-uncased'\n",
        "\n",
        "    # The BERT paper recommends learning rates 5e-5, 3e-5, or 2e-5\n",
        "    learning_rate = 5e-5\n",
        "\n",
        "#X, Y = read_data('dredze_amazon_reviews.txt', use_sentiment=True)\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "clf = NNClassifier(TextClassifierParametersBERT(), TextClassifierUsingBERT)\n",
        "\n",
        "clf.fit(list(amazon_corpus.text), list(amazon_corpus.sentiment))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9NM22QqKTLw"
      },
      "source": [
        "If we want, we can run the classifier interactively on some test documents. How would you think a bag-of-words classifier would have handled these cases?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crte-S_IHspz"
      },
      "outputs": [],
      "source": [
        "clf.predict(['A bad song!', 'A great song!', 'Not a bad song!', 'Not a great song!'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eAgsAC2HIK3"
      },
      "source": [
        "# Classification code\n",
        "\n",
        "This is almost the same code as we used for previous classifiers. The *major* difference is that we do not use our own vocabulary, but instead rely on BERT's built-in tokenizer. Apart from this, there are only some small cosmetic changes.\n",
        "\n",
        "Make sure you execute this cell before you run the classifier above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Nel-AQk8AN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DocumentBatcher:\n",
        "    \"\"\"A collator that builds a batch from a number of documents.\"\"\"\n",
        "\n",
        "    def __init__(self, pad_id):\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "    def make_batch_1(self, X):\n",
        "        \"\"\"Build a batch from a number of documents.\n",
        "        Returns a tensor of shape [n_docs, max_doc_length].\"\"\"\n",
        "\n",
        "        # How long is the longest document in this batch?\n",
        "        max_len = max(len(x) for x in X)\n",
        "\n",
        "        # Build the document tensor. We pad the shorter documents so that all documents\n",
        "        # have the same length.\n",
        "        Xpadded = torch.as_tensor([x + [self.pad_id]*(max_len-len(x)) for x in X])\n",
        "        return Xpadded\n",
        "\n",
        "\n",
        "    def make_batch_2(self, XY):\n",
        "        \"\"\"Build a batch from a number of documents and their labels.\n",
        "        Returns two tensors X and Y, where X is the document tensor,\n",
        "        of shape [n_docs, max_doc_length]\n",
        "\n",
        "        and\n",
        "\n",
        "        Y is the label tensor, of shape [n_docs].\n",
        "        \"\"\"\n",
        "\n",
        "        # How long is the longest document in this batch?\n",
        "        max_len = max(len(x) for x, _ in XY)\n",
        "\n",
        "        # Build the document tensor. We pad the shorter documents so that all documents\n",
        "        # have the same length.\n",
        "        Xpadded = torch.as_tensor([x + [self.pad_id]*(max_len-len(x)) for x, _ in XY])\n",
        "\n",
        "        # Build the label tensor.\n",
        "        Y = torch.as_tensor([y for _, y in XY])\n",
        "\n",
        "        return Xpadded, Y\n",
        "\n",
        "\n",
        "    def __call__(self, instances):\n",
        "        if isinstance(instances[0], tuple):\n",
        "            return self.make_batch_2(instances)\n",
        "        else:\n",
        "            return self.make_batch_1(instances)\n",
        "\n",
        "\n",
        "class NNClassifier:\n",
        "    \"\"\"A classifier based on a neural network.\"\"\"\n",
        "\n",
        "    def __init__(self, params, model_factory):\n",
        "        self.params = params\n",
        "        self.model_factory = model_factory\n",
        "        print('Loading tokenizer...')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(params.bert_model_name)\n",
        "\n",
        "    def preprocess(self, X, Y):\n",
        "\n",
        "        Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=self.params.val_size, random_state=0)\n",
        "\n",
        "        self.lbl_enc = LabelEncoder()\n",
        "        self.lbl_enc.fit(Ytrain)\n",
        "        self.n_classes = len(self.lbl_enc.classes_)\n",
        "\n",
        "        Xtrain_encoded = self.tokenizer(Xtrain, truncation=True, max_length=128).input_ids\n",
        "        Xval_encoded = self.tokenizer(Xval, truncation=True, max_length=128).input_ids\n",
        "\n",
        "        train_dataset = list(zip(Xtrain_encoded, self.lbl_enc.transform(Ytrain)))\n",
        "        val_dataset = list(zip(Xval_encoded, self.lbl_enc.transform(Yval)))\n",
        "\n",
        "        batcher = DocumentBatcher(self.tokenizer.pad_token_id)\n",
        "\n",
        "        self.train_loader = DataLoader(train_dataset, self.params.batch_size, shuffle=True, collate_fn=batcher)\n",
        "        self.val_loader = DataLoader(val_dataset, self.params.batch_size, shuffle=False, collate_fn=batcher)\n",
        "\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"Train the model. We assume that a dataset and a model have already been provided.\"\"\"\n",
        "        par = self.params\n",
        "\n",
        "        self.preprocess(X, Y)\n",
        "\n",
        "        self.model = self.model_factory(self)\n",
        "        self.model.to(par.device)\n",
        "\n",
        "        self.loss_func = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=par.learning_rate, weight_decay=par.decay)\n",
        "\n",
        "        self.history = defaultdict(list)\n",
        "\n",
        "        for epoch in range(par.n_epochs):\n",
        "            print(f'*** EPOCH {epoch+1} ***')\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            self.model.train()\n",
        "            train_loss, train_acc = self.epoch(self.train_loader, optimizer)\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_loss, val_acc = self.epoch(self.val_loader)\n",
        "\n",
        "            t1 = time.time()\n",
        "\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "            self.history['time'].append(t1-t0)\n",
        "\n",
        "            print(f'Time: {t1-t0:.4f}; train loss: {train_loss:.4f}; train acc: {train_acc:.4f}; val loss: {val_loss:.4f}; val acc: {val_acc:.4f}')\n",
        "\n",
        "    def epoch(self, batches, optimizer=None):\n",
        "        \"\"\"Runs the neural network for one epoch, using the given batches.\n",
        "        If an optimizer is provided, this is training data and we will update the model\n",
        "        after each batch. Otherwise, this is assumed to be validation data.\n",
        "\n",
        "        Returns the loss and accuracy over the epoch.\"\"\"\n",
        "        n_correct = 0\n",
        "        n_instances = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, (Xbatch, Ybatch) in enumerate(batches, 1):\n",
        "            Xbatch = Xbatch.to(self.params.device)\n",
        "            Xmask = (Xbatch != self.tokenizer.pad_token_id).long()\n",
        "            Ybatch = Ybatch.to(self.params.device)\n",
        "\n",
        "            scores = self.model(Xbatch, Xmask)\n",
        "\n",
        "            loss = self.loss_func(scores, Ybatch)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            guesses = scores.argmax(dim=1)\n",
        "            n_correct += (guesses == Ybatch).sum().item()\n",
        "            n_instances += Ybatch.shape[0]\n",
        "\n",
        "            if optimizer:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print('.', end='')\n",
        "            if step % 50 == 0:\n",
        "              print(f' ({step})')\n",
        "\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        print()\n",
        "\n",
        "        return total_loss/len(batches), n_correct/n_instances\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Run a trained classifier on a set of instances and return the predictions.\"\"\"\n",
        "\n",
        "        batcher = DocumentBatcher(self.tokenizer.pad_token_id)\n",
        "        encoded = self.tokenizer(X).input_ids\n",
        "        loader = DataLoader(encoded, self.params.batch_size, collate_fn=batcher)\n",
        "\n",
        "        self.model.eval()\n",
        "        outputs = []\n",
        "        with torch.no_grad():\n",
        "            for Xbatch in loader:\n",
        "                Xbatch = Xbatch.to(self.params.device)\n",
        "                Xmask = (Xbatch != self.tokenizer.pad_token_id).long()\n",
        "                scores = self.model(Xbatch, Xmask)\n",
        "                guesses = scores.argmax(dim=1)\n",
        "                outputs.append(guesses.cpu().numpy())\n",
        "\n",
        "        return self.lbl_enc.inverse_transform(np.hstack(outputs))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cse.chalmers.se/~richajo/diverse/l7/books.data\n",
        "!wget https://www.cse.chalmers.se/~richajo/diverse/l7/s7_pretrained.model"
      ],
      "metadata": {
        "id": "L3RDZnC30TeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b00774a-d0fc-4f64-b7ad-c1c40c9413cc"
      },
      "outputs": [],
      "source": [
        "with open('books.data', 'rb') as f:\n",
        "    books_X, books_Y = pickle.load(f)\n",
        "\n",
        "test_input = torch.as_tensor(books_X)\n",
        "print('X shape:', books_X.shape)\n",
        "print('Y length:', len(books_Y))\n",
        "\n",
        "split_ix = 1500\n",
        "books_X_tr = books_X[:split_ix]\n",
        "books_Y_tr = books_Y[:split_ix]\n",
        "books_X_te = books_X[split_ix:]\n",
        "books_Y_te = books_Y[split_ix:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16e8f3c4-dd91-4166-93cd-dc66fc891267"
      },
      "outputs": [],
      "source": [
        "model_fp32 = torch.load('s7_pretrained.model')\n",
        "model_bf16 = torch.load('s7_pretrained.model')\n",
        "model_bf16.bfloat16();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56c035ab-2229-4313-a48b-a7da8590c687"
      },
      "outputs": [],
      "source": [
        "def batcher(batch):\n",
        "    X = torch.as_tensor([x for x, _ in batch])\n",
        "    Y = 1.0*torch.as_tensor([y for _, y in batch])\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c38f361-4a51-41d4-bf52-554f8d9b1e2c"
      },
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "    dl = DataLoader(list(zip(books_X_te, books_Y_te)), batch_size=32, shuffle=False, collate_fn=batcher)\n",
        "    n_corr = 0\n",
        "    for Xb, Yb in dl:\n",
        "        with torch.no_grad():\n",
        "            model_out = model(Xb)\n",
        "        preds = model_out[:, 0] > 0\n",
        "        gold = Yb > 0\n",
        "        n_corr += sum(preds == gold).item()\n",
        "    return n_corr / len(books_Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f81753ec-3fca-40a2-b1af-3c484b4f3fac"
      },
      "outputs": [],
      "source": [
        "eval_model(model_fp32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c391fe32-19ac-44bf-98f7-18faf6666024"
      },
      "source": [
        "# Basic fine-tuning\n",
        "\n",
        "We create a new model where we copy the weights from the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5535598-361e-46dd-9367-31e3e98021bf"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "finetuned = nn.Sequential(\n",
        "    nn.Linear(in_features=768, out_features=512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=1)\n",
        ")\n",
        "\n",
        "# pretrained = torch.load('s7_pretrained.model')\n",
        "\n",
        "finetuned[0].weight.data = pretrained[0].weight.data.clone()\n",
        "finetuned[0].bias.data = pretrained[0].bias.data.clone()\n",
        "finetuned[2].weight.data = pretrained[2].weight.data.clone()\n",
        "finetuned[2].bias.data = pretrained[2].bias.data.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdd587f2-b291-4f46-b170-44b1f33c48b3"
      },
      "outputs": [],
      "source": [
        "eval_model(finetuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixrMmqvkZNKs"
      },
      "source": [
        "# Basic fine-tuning\n",
        "\n",
        "We create a new model where we copy the weights from the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_fp32 = model_fp32(test_input)"
      ],
      "metadata": {
        "id": "IPVTUWUf3blj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How much do the output values differ between FP32 and BF16?\n",
        "- How many binary predictions are \"flipped\" when we move to lower precision?"
      ],
      "metadata": {
        "id": "0D2HRoHIZaE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(model_fp32, {torch.nn.Linear}, dtype=torch.qint8)"
      ],
      "metadata": {
        "id": "E1ycVR_n_62o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How much do the output values differ between FP32 and 8-bit integer quantization?\n",
        "- How many binary predictions are \"flipped\" when we use 8-bit integer quantization?"
      ],
      "metadata": {
        "id": "Hw3t_OqBZy8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a038088a-0123-4d91-894c-cc1dcb8f5473"
      },
      "outputs": [],
      "source": [
        "def train(model, n_epochs=10):\n",
        "    dl = DataLoader(list(zip(books_X_tr, books_Y_tr)), batch_size=32, shuffle=True, collate_fn=batcher)\n",
        "\n",
        "    # NOTE!\n",
        "    params = [ p for p in model.parameters() if p.requires_grad_ ]\n",
        "\n",
        "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        for Xb, Yb in dl:\n",
        "            model_out = model(Xb)[:, 0]\n",
        "            loss = loss_fn(model_out, Yb)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        mean_loss = total_loss / len(dl)\n",
        "        acc = eval_model(model)\n",
        "        print(f'loss = {mean_loss:.4f}, acc = {acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- Complete `count_trainable_parameters` below.\n",
        "- Count the total number of trainable parameters in the model you fine-tuned.\n",
        "- Use the function `train` to fine-tune the cloned model."
      ],
      "metadata": {
        "id": "kP4ym2hY2vDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "  # TODO\n",
        "  return None"
      ],
      "metadata": {
        "id": "qJ6dPDwx1JMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "913197b3-6eda-4a5e-9c23-23e4c345ea18"
      },
      "source": [
        "# Implementing LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14d538e5-8041-4efa-97d7-4b32532827c7"
      },
      "outputs": [],
      "source": [
        "class LinearBlockWithLoRA(nn.Module):\n",
        "\n",
        "    def __init__(self, W, r):\n",
        "        super().__init__()\n",
        "        self.W = W\n",
        "        in_dimension, out_dimension = W.weight.shape\n",
        "\n",
        "        self.A = nn.Linear(in_features=in_dimension, out_features=r, bias=False)\n",
        "        self.B = nn.Linear(in_features=r, out_features=out_dimension, bias=False)\n",
        "\n",
        "    def forward(self, X):\n",
        "        W_out = self.W(X)\n",
        "\n",
        "        a_out = self.A(X)\n",
        "        b_out = self.B(a_out)\n",
        "\n",
        "        # TODO: add more code here\n",
        "        return W_out + b_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- Complete `LinearBlockWithLoRA` above\n",
        "- Set up a model using this new block to replace the first linear layer. Initialize parameters from the pre-trained model. (Don't forget to switch off gradient computation for `W`.)\n",
        "- Count the parameters in the new model.\n",
        "- Train the new model."
      ],
      "metadata": {
        "id": "jTKFM5xk1-P_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}