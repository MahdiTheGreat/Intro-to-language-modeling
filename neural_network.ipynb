{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipdb\n",
        "!pip install -U spacy\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhVjlxLwJhiu",
        "outputId": "8de79c08-9b7c-485a-d75b-ce0c580f7f13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
            "  Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
            "  Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m925.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.4 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.0.1 numpy-2.0.2 spacy-3.8.2 thinc-8.3.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import ipdb\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "# %pdb on"
      ],
      "metadata": {
        "id": "v6kGb9T7JfDL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to plot the training metrics\n",
        "\n",
        "def plot_training_metrics(train_acc, val_acc, train_loss, title, save_path):\n",
        "    # Ensure that all input lists have the same length\n",
        "    assert len(train_acc) == len(val_acc) == len(train_loss), \"All input histories must have the same length.\"\n",
        "\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    # Create the metrics DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Accuracy (%)': train_acc,\n",
        "        'Validation Accuracy (%)': val_acc,\n",
        "        'Training Loss': train_loss\n",
        "    })\n",
        "\n",
        "    # Initialize the plot\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Training and Validation Accuracy on ax1\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy (%)', color=color)\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Training Accuracy (%)'], label='Train Acc', color='tab:blue')\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Validation Accuracy (%)'], label='Val Acc', color='tab:cyan')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Create a second y-axis for Training Loss\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Train Loss', color='tab:red')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
        "\n",
        "    # Set plot title and layout\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save and display the plot\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "B2pWU-GXJb7U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# EarlyStopping class remains the same\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
        "        self.patience = patience  # Number of epochs to wait for improvement\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.path = path  # Path to save the best model\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss < self.best_score - self.delta:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Save model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class SimpleANN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, layer_sizes,activation=nn.ReLU,last_layer_activation=nn.Softmax,dropout=0):\n",
        "\n",
        "        super(SimpleANN, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes)-2):\n",
        "          self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "          self.layers.append(nn.Dropout(dropout))\n",
        "          self.layers.append(activation())\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "        if last_layer_activation is not None:\n",
        "         self.layers.append(nn.Dropout(dropout))\n",
        "         self.layers.append(last_layer_activation())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is a batch of word indices (e.g., [batch_size])\n",
        "        embeddings = self.embeddings(x)  # Get word embeddings for each word in the batch\n",
        "\n",
        "        # Flatten the input embeddings (if necessary, depending on your task)\n",
        "        x = embeddings.view(-1, np.prod(embeddings.shape[1:]))  # Flatten for fully connected layers\n",
        "\n",
        "        #x = x.view(-1, np.prod(x.shape[1:])) # Flatten the input\n",
        "        x = x.float()\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5dT0NfEsJQri"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "3im355-CJNEV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh2mGMOHJLVf",
        "outputId": "3e31d3f9-f8b1-4e31-c75e-5fb9856a7d82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "def dense_arch_builder(input_size,scale_factor=0,hidden_layers_num=0,repeat=0,output_size=1):\n",
        "  layer_sizes=[input_size]\n",
        "\n",
        "  if scale_factor!=0:\n",
        "\n",
        "   if scale_factor>1:\n",
        "    for i in range(hidden_layers_num):\n",
        "     layer_sizes.append(layer_sizes[-1]*scale_factor)\n",
        "    while layer_sizes[-1]<output_size:\n",
        "     layer_sizes.append(layer_sizes[-1]*scale_factor)\n",
        "\n",
        "   elif scale_factor==1:\n",
        "     for i in range(2,hidden_layers_num+2):\n",
        "      layer_sizes.append(layer_sizes[0]*i)\n",
        "     i+=1\n",
        "     while layer_sizes[-1]<output_size:\n",
        "      layer_sizes.append(layer_sizes[0]*i)\n",
        "      i+=1\n",
        "\n",
        "   mirrored_layer_sizes=deepcopy(layer_sizes)\n",
        "   mirrored_layer_sizes.reverse()\n",
        "   mirrored_layer_sizes=mirrored_layer_sizes[1:-1]\n",
        "\n",
        "   for i in range(repeat):\n",
        "    layer_sizes.append(layer_sizes[-1])\n",
        "\n",
        "   if output_size>0:\n",
        "    layer_sizes+=mirrored_layer_sizes\n",
        "    downscale_factor=scale_factor if scale_factor>1 else 2\n",
        "\n",
        "    while layer_sizes[-1]!=output_size:\n",
        "     if layer_sizes[-1]//downscale_factor>=output_size:\n",
        "      layer_sizes.append(layer_sizes[-1]//downscale_factor)\n",
        "     else:\n",
        "      layer_sizes.append(output_size)\n",
        "\n",
        "  else:\n",
        "    downscale_factor = (input_size / output_size) ** (1 / hidden_layers_num)\n",
        "    for i in range(hidden_layers_num):\n",
        "        layer_sizes.append(int(input_size / (downscale_factor ** i)))\n",
        "    layer_sizes.append(output_size)\n",
        "\n",
        "  return layer_sizes\n"
      ],
      "metadata": {
        "id": "AHggv8hkax0I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes=dense_arch_builder(input_size=3,scale_factor=2,hidden_layers_num=4,repeat=0,output_size=24)\n",
        "print(layer_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ELDJdA3a0DS",
        "outputId": "5da00b34-b64c-4c06-ed2f-2b6b17e52e0c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 6, 12, 24, 48, 24, 12, 6, 24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_rdFEcIZI2c2"
      },
      "outputs": [],
      "source": [
        "## Creating a tensor dataset ##\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def TorchDataLoader(training_sequences, batch_size):\n",
        "  context_words = [item[0] for item in training_sequences]  # List of [context]\n",
        "  target_words = [item[1] for item in training_sequences]   # List of target words\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  context_tensor = torch.tensor(context_words, dtype=torch.long)  # Shape: (num_samples, 3)\n",
        "  target_tensor = torch.tensor(target_words, dtype=torch.long)    # Shape: (num_samples,)\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(context_tensor, target_tensor)\n",
        "\n",
        "  # Create a DataLoader for batching\n",
        "  batch_size = 4\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Iterate over the DataLoader\n",
        "  #for batch_context, batch_target in dataloader:\n",
        "  #    print(\"Batch context:\", batch_context)\n",
        "  #    print(\"Batch target:\", batch_target)\n",
        "      # You can now use batch_context and batch_target for model training\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_text = [\n",
        "    ([2, 2, 2], 7), ([2, 2, 7], 8), ([2, 7, 8], 3), ([7, 8, 3], 9),\n",
        "    ([8, 3, 9], 4), ([3, 9, 4], 5), ([9, 4, 5], 2), ([2, 2, 2], 2),\n",
        "    ([2, 2, 2], 12), ([2, 2, 12], 4), ([2, 12, 4], 5), ([12, 4, 5], 13),\n",
        "    ([4, 5, 13], 3), ([5, 13, 3], 14), ([13, 3, 14], 15), ([3, 14, 15], 2),\n",
        "    ([2, 2, 2], 6), ([2, 2, 6], 16), ([2, 6, 16], 17), ([6, 16, 17], 18),\n",
        "    ([16, 17, 18], 19), ([17, 18, 19], 20), ([18, 19, 20], 6), ([19, 20, 6], 21),\n",
        "    ([20, 6, 21], 22), ([6, 21, 22], 2)\n",
        "]\n",
        "\n",
        "training_size = 0.8\n",
        "training_text = dummy_text[:int(len(dummy_text)*training_size)]\n",
        "val_text = dummy_text[int(len(dummy_text)*training_size):]\n",
        "\n",
        "trainloader = TorchDataLoader(training_text, 4)\n",
        "valloader = TorchDataLoader(val_text, 4)"
      ],
      "metadata": {
        "id": "xZU_rYRWKl4I"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes=dense_arch_builder(input_size=72,scale_factor=2,hidden_layers_num=4,repeat=0,output_size=24)\n",
        "print(layer_sizes)\n",
        "\n",
        "model = SimpleANN(layer_sizes=layer_sizes, vocab_size=24, embed_size=24)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "patience = 5\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "number_of_epochs = 30\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    for batch_context, batch_target in trainloader:\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context\n",
        "        Y = batch_target\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        outputs = model(X)  # Model output for X\n",
        "        loss = criterion(outputs, Y) # Compute the loss between model output and Y\n",
        "\n",
        "        #BACKWARD PASS (updating the model parameters):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{number_of_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradient computation for validation\n",
        "        for inputs, targets in valloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
        "    print(f\"Epoch {epoch+1}/{number_of_epochs} - Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    # Call early stopping after each epoch\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, load the best model after training\n",
        "model.load_state_dict(torch.load('checkpoint.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvWa59nZJCps",
        "outputId": "89a23da4-3e3b-4c8a-8569-dc1c36cd8719"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 144, 288, 576, 1152, 576, 288, 144, 72, 36, 24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 3.1813\n",
            "Epoch 1/30 - Validation Loss: 3.181040\n",
            "Validation loss decreased (inf --> 3.181040).  Saving model ...\n",
            "Epoch [2/30], Loss: 3.1784\n",
            "Epoch 2/30 - Validation Loss: 3.183803\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [3/30], Loss: 3.1586\n",
            "Epoch 3/30 - Validation Loss: 3.179778\n",
            "Validation loss decreased (3.181040 --> 3.179778).  Saving model ...\n",
            "Epoch [4/30], Loss: 3.2468\n",
            "Epoch 4/30 - Validation Loss: 3.247109\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [5/30], Loss: 2.9968\n",
            "Epoch 5/30 - Validation Loss: 3.234914\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [6/30], Loss: 3.2173\n",
            "Epoch 6/30 - Validation Loss: 3.208758\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [7/30], Loss: 3.0027\n",
            "Epoch 7/30 - Validation Loss: 3.169878\n",
            "Validation loss decreased (3.179778 --> 3.169878).  Saving model ...\n",
            "Epoch [8/30], Loss: 2.9398\n",
            "Epoch 8/30 - Validation Loss: 3.193126\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [9/30], Loss: 3.1645\n",
            "Epoch 9/30 - Validation Loss: 3.222477\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [10/30], Loss: 2.9263\n",
            "Epoch 10/30 - Validation Loss: 3.218733\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [11/30], Loss: 3.1376\n",
            "Epoch 11/30 - Validation Loss: 3.082569\n",
            "Validation loss decreased (3.169878 --> 3.082569).  Saving model ...\n",
            "Epoch [12/30], Loss: 3.1977\n",
            "Epoch 12/30 - Validation Loss: 3.104212\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [13/30], Loss: 2.9674\n",
            "Epoch 13/30 - Validation Loss: 2.992410\n",
            "Validation loss decreased (3.082569 --> 2.992410).  Saving model ...\n",
            "Epoch [14/30], Loss: 3.1562\n",
            "Epoch 14/30 - Validation Loss: 3.116909\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [15/30], Loss: 3.1289\n",
            "Epoch 15/30 - Validation Loss: 3.221128\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [16/30], Loss: 3.2209\n",
            "Epoch 16/30 - Validation Loss: 3.215916\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [17/30], Loss: 3.0572\n",
            "Epoch 17/30 - Validation Loss: 3.206187\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [18/30], Loss: 3.2447\n",
            "Epoch 18/30 - Validation Loss: 3.214449\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping triggered!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-c2b3ad6dad48>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('checkpoint.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "9vZmEKXlP7vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the next word for four sentences:"
      ],
      "metadata": {
        "id": "ak9bYCH7QEtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = torch.tensor([[0, 13, 12],\n",
        "                              [0, 8, 9],\n",
        "                              [8, 7, 6],\n",
        "                              [5, 4, 5]])\n",
        "output = model(test_sentences).detach().numpy()\n",
        "\n",
        "# Predict\n",
        "predictions = np.argmax(output, axis=1)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "plbd92dQP7d-",
        "outputId": "b1f2c90e-a755-44ba-deb8-7471ac0e966f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitative evaluation"
      ],
      "metadata": {
        "id": "JfO60jvqTjdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict validation data\n",
        "dummy_val = [\n",
        "    ([2, 2, 2], 7), ([2, 2, 7], 8), ([2, 7, 8], 3), ([7, 8, 3], 9),\n",
        "    ([8, 3, 9], 4), ([3, 9, 4], 5), ([9, 4, 5], 2), ([2, 2, 2], 2),\n",
        "    ([2, 2, 2], 12), ([2, 2, 12], 4), ([2, 12, 4], 5), ([12, 4, 5], 13),\n",
        "    ([4, 5, 13], 3), ([5, 13, 3], 14), ([13, 3, 14], 15), ([3, 14, 15], 2),\n",
        "    ([2, 2, 2], 6), ([2, 2, 6], 16), ([2, 6, 16], 17), ([6, 16, 17], 18),\n",
        "    ([16, 17, 18], 19), ([17, 18, 19], 20), ([18, 19, 20], 6), ([19, 20, 6], 21),\n",
        "    ([20, 6, 21], 22), ([6, 21, 22], 2)\n",
        "]\n",
        "val_dataloader = TorchDataLoader(dummy_val, 4)\n",
        "loss = []\n",
        "for batch_context, batch_target in val_dataloader:\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context\n",
        "        Y = batch_target\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        outputs = (model(X))  # Model output for X\n",
        "        loss.append((criterion(outputs, Y)).item()) # Compute the loss between model output and Y\n",
        "\n",
        "# Compute perplexity\n",
        "perplexity = np.exp(np.mean(loss))\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "5xfNYfZ7Ti5i",
        "outputId": "1a8687fb-4522-47a9-8058-a9599b9aa2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.860031353068354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the word embeddings"
      ],
      "metadata": {
        "id": "ggDBCb0fZMNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbors(emb, voc, inv_voc, word, n_neighbors=5):\n",
        "\n",
        "    # Look up the embedding for the test word.\n",
        "    test_emb = emb.weight[voc[word]]\n",
        "\n",
        "    # We'll use a cosine similarity function to find the most similar words.\n",
        "    sim_func = nn.CosineSimilarity(dim=1)\n",
        "    cosine_scores = sim_func(test_emb, emb.weight)\n",
        "\n",
        "    # Find the positions of the highest cosine values.\n",
        "    near_nbr = cosine_scores.topk(n_neighbors+1)\n",
        "    topk_cos = near_nbr.values[1:]\n",
        "    topk_indices = near_nbr.indices[1:]\n",
        "    # NB: the first word in the top-k list is the query word itself!\n",
        "    # That's why we skip the first position in the code above.\n",
        "\n",
        "    # Finally, map word indices back to strings, and put the result in a list.\n",
        "    return [ (inv_voc[ix.item()], cos.item()) for ix, cos in zip(topk_indices, topk_cos) ]\n",
        "word_embedding = model.embeddings(torch.tensor([5]))\n",
        "print(word_embedding)\n",
        "nearest_neighbors(word_embedding)\n",
        "nearest_neighbors(\"2005\")"
      ],
      "metadata": {
        "id": "lxQFbdXMZLmg",
        "outputId": "12727750-a482-45b7-f486-6eb0be2b553c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.8097,  0.6443, -0.6667,  0.2803, -0.7137, -0.7417,  0.5777, -0.0141,\n",
            "         -0.9880, -2.3758, -0.8092,  0.3889, -1.6978,  1.1951,  0.6678,  1.4581,\n",
            "          0.9677, -0.4329, -1.4000,  1.2658,  0.6585,  0.7534, -1.5279, -0.8618]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "nearest_neighbors() missing 3 required positional arguments: 'voc', 'inv_voc', and 'word'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-548ef294fc38>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mnearest_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mnearest_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2005\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: nearest_neighbors() missing 3 required positional arguments: 'voc', 'inv_voc', and 'word'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_embeddings_pca(emb, inv_voc, words):\n",
        "    vectors = np.vstack([emb.weight[inv_voc[w]].cpu().detach().numpy() for w in words])\n",
        "    vectors -= vectors.mean(axis=0)\n",
        "    twodim = TruncatedSVD(n_components=2).fit_transform(vectors)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.02, y, word)\n",
        "    plt.axis('off')\n",
        "\n",
        "plot_embeddings_pca(model[0], prepr, ['sweden', 'denmark', 'europe', 'africa', 'london', 'stockholm', 'large', 'small', 'great', 'black', '3', '7', '10', 'seven', 'three', 'ten', '1984', '2005', '2010'])"
      ],
      "metadata": {
        "id": "pYxSMQvnZTVE",
        "outputId": "1eaa1419-8bcd-465d-bbb3-893b9a978e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/_core/_dtype.py:106: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if dtype.type == np.bool:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-5d3504a9c753>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_embeddings_pca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_voc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv_voc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_preserve_dia_indices_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_isfinite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFiniteStatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy_isfinite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0m_NUMPY_NAMESPACE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array_api_compat.numpy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    604\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_continuous_distns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_discrete_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetaln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammaln\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgamln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog1p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_interpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fitpack_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_interpolate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fitpack_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdfitpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_polyint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Interpolator1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# These are in the API for fitpack even if not used in fitpack.py itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fitpack_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbisplrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbisplev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdblint\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fitpack_impl\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bsplines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBSpline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m _parcur_cache = {'t': array([], float), 'wrk': array([], float),\n\u001b[0;32m--> 103\u001b[0;31m                  \u001b[0;34m'iwrk'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfitpack_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'u'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                  'ub': 0, 'ue': 1}\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}