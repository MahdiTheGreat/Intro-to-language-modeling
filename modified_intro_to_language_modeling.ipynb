{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparations"
      ],
      "metadata": {
        "id": "Ash16YV-b6T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
        "%cd Intro-to-language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXuQ-VlzuRv",
        "outputId": "113320b0-369f-49d9-a678-9cc86605cfba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-language-modeling'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 43 (delta 22), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (43/43), 29.40 MiB | 13.85 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "/content/Intro-to-language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5fv9gQcVafW3"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import spacy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "Qjj4IdOi08ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to plot the training metrics\n",
        "\n",
        "def plot_training_metrics(train_acc, val_acc, train_loss, title, save_path):\n",
        "    # Ensure that all input lists have the same length\n",
        "    assert len(train_acc) == len(val_acc) == len(train_loss), \"All input histories must have the same length.\"\n",
        "\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    # Create the metrics DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Accuracy (%)': train_acc,\n",
        "        'Validation Accuracy (%)': val_acc,\n",
        "        'Training Loss': train_loss\n",
        "    })\n",
        "\n",
        "    # Initialize the plot\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Training and Validation Accuracy on ax1\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy (%)', color=color)\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Training Accuracy (%)'], label='Train Acc', color='tab:blue')\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Validation Accuracy (%)'], label='Val Acc', color='tab:cyan')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Create a second y-axis for Training Loss\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Train Loss', color='tab:red')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
        "\n",
        "    # Set plot title and layout\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save and display the plot\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TDDGQTI51AF2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDybdl906rv",
        "outputId": "604c97a6-6ea1-48de-e911-b192a12d9df7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 & 2"
      ],
      "metadata": {
        "id": "8VLcOGFvb-pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='lmdemo'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "!unzip -q $zip_file\n",
        "!rm $zip_file"
      ],
      "metadata": {
        "id": "IE8oAx8b3AWX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set=open(f'{dataset}/train.txt','r',encoding='utf-8').read()\n",
        "val_set=open(f'{dataset}/val.txt','r',encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "clFRaGPQ4Jc-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 64821808"
      ],
      "metadata": {
        "id": "HJ_uX3IDof2K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "class VocabularyBuilder:\n",
        "    def __init__(self, max_voc_size):\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.str_to_int = {}\n",
        "        self.int_to_str = {}\n",
        "        self.special_tokens = [\"BEGINNING\", \"END\", \"UNKNOWN\"]\n",
        "        self.token_counter = Counter()\n",
        "\n",
        "    def build_vocabulary(self, text):\n",
        "\n",
        "        sents=nlp(text)\n",
        "\n",
        "        for token in sents:\n",
        "            if not token.is_space and not token.is_punct:\n",
        "                self.token_counter[token.text.lower()] += 1\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "        # Start vocabulary with special tokens\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Select the most common tokens, considering max_voc_size - len(special_tokens)\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = self.token_counter.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def create_premade_vocabulary(self, c):\n",
        "        # Start vocabulary with special tokens\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Select the most common tokens, considering max_voc_size - len(special_tokens)\n",
        "        max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = c.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "    def get_token_id(self, token):\n",
        "        # Return the integer ID for a given token\n",
        "        return self.str_to_int.get(token.lower(), self.str_to_int[\"UNKNOWN\"])\n",
        "\n",
        "    def get_token_str(self, token_id):\n",
        "        # Return the original token string for a given integer ID\n",
        "        return self.int_to_str.get(token_id, \"UNKNOWN\")\n",
        "\n",
        "    def sanity_check(self):\n",
        "        # Check vocabulary size\n",
        "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
        "\n",
        "        # Check special tokens exist and are unique\n",
        "        for token in self.special_tokens:\n",
        "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
        "\n",
        "        # Check if highly frequent words are included and rare ones are not\n",
        "        common_words = [\"the\", \"and\"]\n",
        "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
        "\n",
        "        for word in common_words:\n",
        "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
        "\n",
        "        for word in rare_words:\n",
        "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
        "\n",
        "        # Check that mapping back and forth works for a test word\n",
        "        test_word = \"The\"\n",
        "        token_id = self.get_token_id(test_word)\n",
        "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
        "\n",
        "        print(\"Sanity check passed!\")\n",
        "\n",
        "vocab_builder = VocabularyBuilder(max_voc_size=5000)\n"
      ],
      "metadata": {
        "id": "CZJ4k7STz96H"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize VocabularyBuilder with a max vocabulary size\n",
        "for paragraph in training_set.split():\n",
        "  vocab_builder.build_vocabulary(paragraph)\n",
        "vocab_builder.create_vocabulary()\n",
        "\n",
        "# Example mappings\n",
        "print(\"str_to_int:\", vocab_builder.str_to_int)\n",
        "print(\"int_to_str:\", vocab_builder.int_to_str)\n",
        "\n",
        "# Convert a token to integer ID and back to string\n",
        "token_id = vocab_builder.get_token_id(\"example\")\n",
        "print(\"Token ID for 'example':\", token_id)\n",
        "print(\"Original token from ID:\", vocab_builder.get_token_str(token_id))"
      ],
      "metadata": {
        "id": "T1LhuLzfE2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d05e55af-20f2-42cd-a933-86b4665aac7c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-69cb0a52ec64>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize VocabularyBuilder with a max vocabulary size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mvocab_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvocab_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-800c2ffe9908>\u001b[0m in \u001b[0;36mbuild_vocabulary\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/ml/tb_framework.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     step_model = ParserStepModel(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/ml/parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUMPY_OPS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray1i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mYf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdYs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/residual.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/layers/layernorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mXhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_builder.token_counter)"
      ],
      "metadata": {
        "id": "LenZuLv1vyia",
        "outputId": "7cdc6fde-a4d5-45ad-e269-7e7902344a6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'the': 278, 'of': 178, 'and': 155, 'in': 97, 'is': 86, 'a': 70, 'are': 68, 'anatomy': 60, 'to': 48, 'which': 30, 'cells': 30, 'animals': 30, 'body': 29, 'organs': 28, 'muscle': 28, 'or': 27, 'as': 25, 'from': 25, 'an': 23, 'tissues': 23, 'by': 21, 'tissue': 21, 'with': 20, 'study': 20, 'be': 18, 'found': 17, 'epithelial': 16, 'into': 15, 'have': 15, 'structure': 14, 'their': 14, 'parts': 14, 'also': 14, 'structures': 14, 'can': 14, 'it': 13, 'animal': 13, 'including': 13, 'connective': 13, 'other': 12, 'there': 12, 'they': 11, 'formed': 11, 'nervous': 11, 'microscopic': 10, 'composed': 10, 'those': 10, 'cell': 10, 'types': 10, 'matrix': 10, 'derived': 9, 'blood': 9, 'such': 9, 'on': 9, 'internal': 9, 'that': 9, 'dissection': 8, 'organisms': 8, 'its': 8, 'gross': 8, 'includes': 8, 'functions': 8, 'methods': 8, 'studied': 8, 'up': 8, 'wall': 8, 'more': 8, 'germ': 8, 'layers': 8, 'extracellular': 8, 'form': 8, 'many': 8, 'but': 8, 'most': 7, 'two': 7, 'has': 7, 'all': 7, 'called': 7, 'three': 7, 'main': 7, 'smooth': 7, 'branch': 6, 'embryology': 6, 'human': 6, 'macroscopic': 6, 'using': 6, 'known': 6, 'make': 6, 'cut': 6, 'external': 6, 'used': 6, 'include': 6, 'term': 6, 'plant': 6, 'nerves': 6, 'skin': 6, 'developed': 6, 'layer': 6, 'movement': 6, 'skeletal': 6, 'sea': 6, 'neurons': 6, 'system': 6, 'fish': 6, 'one': 5, 'basic': 5, 'information': 5, 'some': 5, 'embryonic': 5, 'muscles': 5, 'ectoderm': 5, 'endoderm': 5, 'bone': 5, 'respiratory': 5, 'this': 5, 'no': 5, 'vertebrates': 5, 'greek': 4, 'anatomē': 4, 'biology': 4, 'concerned': 4, 'natural': 4, 'science': 4, 'organization': 4, 'these': 4, 'processes': 4, 'over': 4, 'discipline': 4, 'divided': 4, 'examination': 4, \"'s\": 4, 'superficial': 4, 'various': 4, 'histology': 4, 'imaging': 4, 'x': 4, 'magnetic': 4, 'resonance': 4, 'physiology': 4, 'respectively': 4, 'function': 4, 'often': 4, 'together': 4, 'i': 4, 'systems': 4, 'position': 4, 'shape': 4, 'liver': 4, 'number': 4, 'large': 4, 'surface': 4, 'organism': 4, 'invasive': 4, 'small': 4, 'angiography': 4, 'vessels': 4, 'refer': 4, 'rest': 4, 'kingdom': 4, 'contains': 4, 'multicellular': 4, 'not': 4, 'when': 4, 'present': 4, 'each': 4, 'typically': 4, 'membrane': 4, 'different': 4, 'triploblastic': 4, 'fibrous': 4, 'proteins': 4, 'collagen': 4, 'modified': 4, 'support': 4, 'cuticle': 4, 'squamous': 4, 'lamina': 4, 'epithelium': 4, 'type': 4, 'lining': 4, 'intestine': 4, 'simple': 4, 'contractile': 4, 'cause': 4, 'either': 4, 'filaments': 4, 'cardiac': 4, 'contracts': 4, 'slowly': 4, 'range': 4, 'striated': 4, 'higher': 4, 'bundles': 4, 'nerve': 4, 'receptor': 4, 'specialized': 4, 'sense': 4, 'at': 4, 'notochord': 4, 'tail': 4, 'evolution': 3, 'for': 3, 'features': 3, 'through': 3, 'rays': 3, 'however': 3, 'same': 3, 'although': 3, 'secondarily': 3, 'stage': 3, 'development': 3, 'nucleus': 3, 'mesoderm': 3, 'made': 3, 'material': 3, 'cartilage': 3, 'major': 3, 'part': 3, 'skeleton': 3, 'tract': 3, 'consists': 3, 'vertebrate': 3, 'may': 3, 'glands': 3, 'appendages': 3, 'between': 3, 'stomach': 3, 'only': 3, 'heart': 3, 'round': 3, 'transmit': 3, 'local': 3, 'along': 3, 'neural': 3, 'spinal': 3, 'cord': 3, 'vertebral': 3, 'column': 3, 'vertebrae': 3, 'fins': 3, 'dealing': 2, 'structural': 2, 'living': 2, 'things': 2, 'old': 2, 'having': 2, 'beginnings': 2, 'prehistoric': 2, 'times': 2, 'inherently': 2, 'tied': 2, 'comparative': 2, 'evolutionary': 2, 'phylogeny': 2, 'generated': 2, 'immediate': 2, 'long': 2, 'timescales': 2, 'essential': 2, 'sciences': 2, 'medicine': 2, 'unaided': 2, 'eyesight': 2, 'involves': 2, 'use': 2, 'optical': 2, 'instruments': 2, 'history': 2, 'characterized': 2, 'progressive': 2, 'understanding': 2, 'improved': 2, 'dramatically': 2, 'advancing': 2, 'carcasses': 2, 'cadavers': 2, 'corpses': 2, '20th': 2, 'century': 2, 'medical': 2, 'techniques': 2, 'ray': 2, 'ultrasound': 2, 'pair': 2, 'related': 2, 'disciplines': 2, 'ἀνατομή': 2, 'ἀνατέμνω': 2, 'anatémnō': 2, 'open': 2, 'ἀνά': 2, 'aná': 2, 'τέμνω': 2, 'témnō': 2, 'scientific': 2, 'appearance': 2, 'materials': 2, 'locations': 2, 'relationships': 2, 'quite': 2, 'distinct': 2, 'biochemistry': 2, 'deal': 2, 'chemical': 2, 'involved': 2, 'example': 2, 'anatomist': 2, 'size': 2, 'supply': 2, 'innervation': 2, 'organ': 2, 'while': 2, 'physiologist': 2, 'interested': 2, 'production': 2, 'bile': 2, 'role': 2, 'nutrition': 2, 'regulation': 2, 'bodily': 2, 'subdivided': 2, 'branches': 2, 'enough': 2, 'seen': 2, 'naked': 2, 'eye': 2, 'sight': 2, 'scale': 2, 'immature': 2, 'condition': 2, 'both': 2, 'non': 2, 'goal': 2, 'obtaining': 2, 'about': 2, 'opened': 2, 'endoscopy': 2, 'video': 2, 'camera': 2, 'equipped': 2, 'instrument': 2, 'inserted': 2, 'incision': 2, 'explore': 2, 'visualize': 2, 'commonly': 2, 'taken': 2, 'substantially': 2, 'throughout': 2, 'zootomy': 2, 'sometimes': 2, 'specifically': 2, 'plants': 2, 'dissimilar': 2, 'nature': 2, 'animalia': 2, 'metazoa': 2, 'heterotrophic': 2, 'motile': 2, 'adopted': 2, 'sessile': 2, 'lifestyle': 2, 'bodies': 2, 'differentiated': 2, 'separate': 2, 'eumetazoans': 2, 'digestive': 2, 'chamber': 2, 'openings': 2, 'gametes': 2, 'produced': 2, 'sex': 2, 'zygotes': 2, 'blastula': 2, 'metazoans': 2, 'do': 2, 'sponges': 2, 'undifferentiated': 2, 'unlike': 2, 'neither': 2, 'nor': 2, 'chloroplasts': 2, 'vacuoles': 2, 'much': 2, 'smaller': 2, 'than': 2, 'numerous': 2, 'phospholipids': 2, 'cytoplasm': 2, 'simpler': 2, 'invertebrates': 2, 'diploblastic': 2, 'whose': 2, 'embryo': 2, 'grouped': 2, 'four': 2, 'scattered': 2, 'among': 2, 'inorganic': 2, 'gives': 2, 'holds': 2, 'them': 2, 'place': 2, 'loose': 2, 'adipose': 2, 'chief': 2, 'abundant': 2, 'plays': 2, 'organizing': 2, 'maintaining': 2, 'protect': 2, 'exoskeleton': 2, 'thickened': 2, 'rigid': 2, 'stiffened': 2, 'mineralization': 2, 'crustaceans': 2, 'cross': 2, 'linking': 2, 'insects': 2, 'endoskeleton': 2, 'well': 2, 'less': 2, 'closely': 2, 'packed': 2, 'bound': 2, 'adhesion': 2, 'molecules': 2, 'little': 2, 'intercellular': 2, 'space': 2, 'flat': 2, 'cuboidal': 2, 'columnar': 2, 'basal': 2, 'upper': 2, 'basement': 2, 'lower': 2, 'reticular': 2, 'lying': 2, 'next': 2, 'secreted': 2, 'suit': 2, 'particular': 2, 'ciliated': 2, 'microvilli': 2, 'intestinal': 2, 'villi': 2, 'outer': 2, 'keratinized': 2, 'stratified': 2, 'covers': 2, 'exterior': 2, 'keratinocytes': 2, '95': 2, 'secrete': 2, 'just': 2, 'coat': 2, 'glycoproteins': 2, 'advanced': 2, 'myocytes': 2, 'active': 2, 'produce': 2, 'force': 2, 'motion': 2, 'locomotion': 2, 'within': 2, 'separated': 2, 'striations': 2, 'examined': 2, 'microscopically': 2, 'maintains': 2, 'contractibility': 2, 'wide': 2, 'stretch': 2, 'lengths': 2, 'anemone': 2, 'tentacles': 2, 'cucumbers': 2, 'rapidly': 2, 'limited': 2, 'extension': 2, 'jaws': 2, 'obliquely': 2, 'intermediate': 2, 'staggered': 2, 'earthworms': 2, 'extend': 2, 'rapid': 2, 'contractions': 2, 'occur': 2, 'attached': 2, 'provide': 2, 'arranged': 2, 'antagonistic': 2, 'sets': 2, 'walls': 2, 'uterus': 2, 'bladder': 2, 'intestines': 2, 'oesophagus': 2, 'airways': 2, 'allowing': 2, 'contract': 2, 'pump': 2, 'slow': 2, 'moving': 2, 'radially': 2, 'symmetrical': 2, 'marine': 2, 'ctenophores': 2, 'cnidarians': 2, 'anemones': 2, 'jellyfish': 2, 'net': 2, 'organized': 2, 'longitudinally': 2, 'reaction': 2, 'stimulus': 2, 'complex': 2, 'chemoreceptors': 2, 'photoreceptors': 2, 'groups': 2, 'send': 2, 'messages': 2, 'networks': 2, 'connected': 2, 'ganglia': 2, 'receptors': 2, 'basis': 2, 'peripheral': 2, 'controls': 2, 'tube': 2, 'posterior': 2, 'anus': 2, 'gut': 2, 'end': 2, 'limbs': 2, 'trunk': 2, 'bony': 2, 'spine': 2, 'ear': 2, 'central': 1, 'brain': 1, 'latter': 1, 'sensory': 1, 'motor': 1, 'influence': 1, 'target': 1, 'somatic': 1, 'conveys': 1, 'sensation': 1, 'voluntary': 1, 'autonomic': 1, 'involuntarily': 1, 'certain': 1, 'similar': 1, 'plan': 1, 'point': 1, 'lives': 1, 'mostly': 1, 'share': 1, 'chordate': 1, 'characteristics': 1, 'stiffening': 1, 'rod': 1, 'dorsal': 1, 'hollow': 1, 'pharyngeal': 1, 'arches': 1, 'protected': 1, 'above': 1, 'gastrointestinal': 1, 'below': 1, 'continues': 1, 'mouth': 1, 'anterior': 1, 'base': 1, 'defining': 1, 'characteristic': 1, 'segmented': 1, 'series': 1, 'becomes': 1, 'pulposus': 1, 'intervertebral': 1, 'discs': 1, 'few': 1, 'sturgeon': 1, 'coelacanth': 1, 'retain': 1, 'adulthood': 1, 'jawed': 1, 'typified': 1, 'paired': 1, 'legs': 1, 'lost': 1, 'considered': 1, 'homologous': 1, 'because': 1, 'underlying': 1, 'was': 1, 'inherited': 1, 'last': 1, 'common': 1, 'ancestor': 1, 'arguments': 1, 'put': 1, 'forward': 1, 'charles': 1, 'darwin': 1, 'his': 1, 'theory': 1, 'head': 1, 'divisions': 1, 'always': 1, 'externally': 1, 'visible': 1, 'forms': 1, 'inside': 1, 'cartilaginous': 1, 'element': 1, 'articulating': 1, 'lightweight': 1, 'yet': 1, 'strong': 1, 'ribs': 1, 'attach': 1, 'limb': 1, 'girdles': 1, 'soft': 1, 'spines': 1, 'exception': 1, 'caudal': 1, 'direct': 1, 'connection': 1, 'supported': 1, 'compose': 1, 'chambers': 1, 'pumps': 1, 'surfaces': 1, 'gills': 1, 'single': 1, 'circulatory': 1, 'loop': 1, 'eyes': 1, 'adapted': 1, 'seeing': 1, 'underwater': 1, 'vision': 1, 'inner': 1, 'middle': 1, 'low': 1, 'frequency': 1, 'vibrations': 1, 'detected': 1, 'lateral': 1, 'line': 1, 'run': 1, 'length': 1, 'sides': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save vocab so we don't have to rerun it\n",
        "counter= vocab_builder.token_counter\n",
        "with open(\"full_vocab\", 'w') as f:\n",
        "    for k,v in  counter.most_common():\n",
        "        f.write( \"{} {}\\n\".format(k,v) )"
      ],
      "metadata": {
        "id": "Z4cv1HQlQ0ya"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "premade_counter = Counter()\n",
        "\n",
        "# Read the file and populate the Counter\n",
        "with open(\"/content/Intro-to-language-modeling/full_vocab\", 'r') as file:\n",
        "    for line in file:\n",
        "        # Split the line into word and frequency\n",
        "        parts = line.split(\" \")\n",
        "        print(parts)\n",
        "        if len(parts) == 2:\n",
        "            word, freq = parts[0], int(parts[1])\n",
        "            premade_counter[word] = freq\n",
        "vocab_builder.create_premade_vocabulary(premade_counter)\n",
        "print(premade_counter)\n"
      ],
      "metadata": {
        "id": "aIiqpkWcE8sM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e93ee09-72c3-4b47-8427-44ff55e283bb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['o', '90\\n']\n",
            "['a', '89\\n']\n",
            "['e', '84\\n']\n",
            "['i', '84\\n']\n",
            "['t', '82\\n']\n",
            "['n', '78\\n']\n",
            "['s', '78\\n']\n",
            "['r', '49\\n']\n",
            "['c', '44\\n']\n",
            "['h', '39\\n']\n",
            "['m', '37\\n']\n",
            "['y', '36\\n']\n",
            "['d', '32\\n']\n",
            "['l', '30\\n']\n",
            "['u', '26\\n']\n",
            "['g', '24\\n']\n",
            "['f', '20\\n']\n",
            "['p', '15\\n']\n",
            "['b', '13\\n']\n",
            "['v', '13\\n']\n",
            "['w', '4\\n']\n",
            "['k', '2\\n']\n",
            "['z', '2\\n']\n",
            "['ē', '1\\n']\n",
            "['x', '1\\n']\n",
            "Counter({'o': 90, 'a': 89, 'e': 84, 'i': 84, 't': 82, 'n': 78, 's': 78, 'r': 49, 'c': 44, 'h': 39, 'm': 37, 'y': 36, 'd': 32, 'l': 30, 'u': 26, 'g': 24, 'f': 20, 'p': 15, 'b': 13, 'v': 13, 'w': 4, 'k': 2, 'z': 2, 'ē': 1, 'x': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sanity check\n",
        "vocab_builder.sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "g1sB4W8Q0Qo9",
        "outputId": "1235db86-0e23-45f7-e282-ec63e2be0796"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Common word 'the' not in vocabulary.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9a56e0a64e6a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-210cc0569b80>\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Common word '{word}' not in vocabulary.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrare_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Common word 'the' not in vocabulary."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingDataPreparer:\n",
        "    def __init__(self, vocab_builder, context_window_size):\n",
        "        self.vocab_builder = vocab_builder\n",
        "        self.N = context_window_size\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"Tokenizes and encodes a single string with special symbols.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to encode.\n",
        "\n",
        "        Returns:\n",
        "        - List[int]: A list of token IDs including BEGINNING and END tokens.\n",
        "        \"\"\"\n",
        "        # Tokenize the text\n",
        "        doc = nlp(text)\n",
        "\n",
        "        tokens = [token.text.lower() for token in doc]\n",
        "\n",
        "        # Map tokens to integer IDs, using \"UNKNOWN\" for out-of-vocabulary words\n",
        "        token_ids = [self.vocab_builder.get_token_id(token) for token in tokens]\n",
        "        modified_tokens = [0]*self.N\n",
        "        modified_tokens.extend(token_ids)\n",
        "        modified_tokens.append(1)\n",
        "\n",
        "        return modified_tokens\n",
        "\n",
        "    def create_training_sequences(self, text):\n",
        "        \"\"\"\n",
        "        Creates training sequences from a single string by generating sequences of length N+1.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input string to create sequences from.\n",
        "\n",
        "        Returns:\n",
        "        - List[Tuple[List[int], int]]: A list of (context, target) pairs.\n",
        "        \"\"\"\n",
        "        training_sequences = []\n",
        "\n",
        "        # Encode the text with BEGINNING, END, and UNKNOWN tokens\n",
        "        encoded_text = self.encode_text(text)\n",
        "\n",
        "        # Generate sequences of length N+1\n",
        "        for i in range(len(encoded_text) - self.N):\n",
        "            context = encoded_text[i : i + self.N]  # N tokens for context\n",
        "            target = encoded_text[i + self.N]       # Next token as the target\n",
        "            training_sequences.append((context, target))\n",
        "\n",
        "        return training_sequences\n"
      ],
      "metadata": {
        "id": "uXLrr6YeF0AF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_window_size = 3\n",
        "data_preparer = TrainingDataPreparer(vocab_builder, context_window_size)\n",
        "\n",
        "# Create training sequences\n",
        "training_sequences = data_preparer.create_training_sequences(training_set[:100])\n",
        "\n",
        "# Display some training sequences\n",
        "print(\"Training sequences (context, target):\")\n",
        "for context, target in training_sequences[:10]:  # Show the first few sequences\n",
        "    print([vocab_builder.get_token_str(id) for id in context], \"->\", vocab_builder.get_token_str(target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A5cX52Y73i1",
        "outputId": "f6b032a3-b159-4cd9-9066-2251704c3494"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sequences (context, target):\n",
            "['BEGINNING', 'BEGINNING', 'BEGINNING'] -> UNKNOWN\n",
            "['BEGINNING', 'BEGINNING', 'UNKNOWN'] -> UNKNOWN\n",
            "['BEGINNING', 'UNKNOWN', 'UNKNOWN'] -> UNKNOWN\n",
            "['UNKNOWN', 'UNKNOWN', 'UNKNOWN'] -> UNKNOWN\n",
            "['UNKNOWN', 'UNKNOWN', 'UNKNOWN'] -> greek\n",
            "['UNKNOWN', 'UNKNOWN', 'greek'] -> UNKNOWN\n",
            "['UNKNOWN', 'greek', 'UNKNOWN'] -> UNKNOWN\n",
            "['greek', 'UNKNOWN', 'UNKNOWN'] -> UNKNOWN\n",
            "['UNKNOWN', 'UNKNOWN', 'UNKNOWN'] -> UNKNOWN\n",
            "['UNKNOWN', 'UNKNOWN', 'UNKNOWN'] -> UNKNOWN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a tensor dataset ##\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def TorchDataLoader(training_sequences, batch_size):\n",
        "  context_words = [item[0] for item in training_sequences]  # List of [context]\n",
        "  target_words = [item[1] for item in training_sequences]   # List of target words\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  context_tensor = torch.tensor(context_words, dtype=torch.long)  # Shape: (num_samples, 3)\n",
        "  target_tensor = torch.tensor(target_words, dtype=torch.long)    # Shape: (num_samples,)\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(context_tensor, target_tensor)\n",
        "\n",
        "  # Create a DataLoader for batching\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "DqKicZuza81v"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preparer = TrainingDataPreparer(vocab_builder, 3)\n",
        "\n",
        "training_sequences = []\n",
        "split_training_set = training_set.splitlines()\n",
        "for paragraph in split_training_set:\n",
        "  training_sequences.append(preparer.create_training_sequences(paragraph))\n",
        "flattened_training_sequences =  [\n",
        "    x\n",
        "    for xs in training_sequences\n",
        "    for x in xs\n",
        "]"
      ],
      "metadata": {
        "id": "dRXFfX7Zf4Hw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training sequences so we don't have to rerun\n",
        "with open(\"/content/sample_data/training_sequences_5k\", 'w') as f:\n",
        "    for x in  flattened_training_sequences:\n",
        "        f.write(\"{}\\n\".format(str(x)))"
      ],
      "metadata": {
        "id": "D3r2Nbg7J54Y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preparer = TrainingDataPreparer(vocab_builder, 3)\n",
        "\n",
        "val_sequences = []\n",
        "split_val_set = val_set.splitlines()\n",
        "for paragraph in split_val_set:\n",
        "  val_sequences.append(preparer.create_training_sequences(paragraph))\n",
        "flattened_val_sequences =  [\n",
        "    x\n",
        "    for xs in val_sequences\n",
        "    for x in xs\n",
        "]"
      ],
      "metadata": {
        "id": "2iMBAWK0VWxP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save val sequences so we don't have to rerun\n",
        "with open(\"/content/sample_data/val_sequences_5k\", 'w') as f:\n",
        "    for x in  flattened_training_sequences:\n",
        "        f.write(\"{}\\n\".format(str(x)))"
      ],
      "metadata": {
        "id": "9p2ZtFutVrDs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = TorchDataLoader(flattened_training_sequences, 64)"
      ],
      "metadata": {
        "id": "5nNMuktjU1IC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valloader = TorchDataLoader(flattened_val_sequences, 64)"
      ],
      "metadata": {
        "id": "9FpFFCWqVl2j"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3"
      ],
      "metadata": {
        "id": "6cxG6iHlclgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# EarlyStopping class remains the same\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
        "        self.patience = patience  # Number of epochs to wait for improvement\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.path = path  # Path to save the best model\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss < self.best_score - self.delta:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Save model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class SimpleANN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, layer_sizes,activation=nn.ReLU,last_layer_activation=nn.Softmax,dropout=0):\n",
        "\n",
        "        super(SimpleANN, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes)-2):\n",
        "          self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "          self.layers.append(nn.Dropout(dropout))\n",
        "          self.layers.append(activation())\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "        if last_layer_activation is not None:\n",
        "         self.layers.append(nn.Dropout(dropout))\n",
        "         self.layers.append(last_layer_activation())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is a batch of word indices (e.g., [batch_size])\n",
        "        embeddings = self.embeddings(x)  # Get word embeddings for each word in the batch\n",
        "\n",
        "        # Flatten the input embeddings (if necessary, depending on your task)\n",
        "        x = embeddings.view(-1, np.prod(embeddings.shape[1:]))  # Flatten for fully connected layers\n",
        "\n",
        "        #x = x.view(-1, np.prod(x.shape[1:])) # Flatten the input\n",
        "        x = x.float()\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Q2CkGhOV1JpZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleANN(layer_sizes=[72, 96, 512, 5000], vocab_size=5000, embed_size=24)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "patience = 5\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "number_of_epochs = 30\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    for batch_context, batch_target in trainloader:\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context\n",
        "        Y = batch_target\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        outputs = model(X)  # Model output for X\n",
        "        loss = criterion(outputs, Y) # Compute the loss between model output and Y\n",
        "\n",
        "        #BACKWARD PASS (updating the model parameters):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{number_of_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradient computation for validation\n",
        "        for inputs, targets in valloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
        "    print(f\"Epoch {epoch+1}/{number_of_epochs} - Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    # Call early stopping after each epoch\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, load the best model after training\n",
        "model.load_state_dict(torch.load('checkpoint.pth'))"
      ],
      "metadata": {
        "id": "7iUtwI8nbBO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "699a54d7-8495-4040-b97f-97fd59541f5d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c7acb0b70403>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Model output for X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute the loss between model output and Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-93178da26cb6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "Hsm3CLoWgub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\"This is very\",\n",
        "                  \"A tall building\",\n",
        "                  \"The next sentence\",\n",
        "                  \"Not a big\"]\n",
        "\n",
        "encoded_sentences = []\n",
        "for sentence in test_sentences:\n",
        "  encoded_sentences.append([vocab_builder.get_token_id(sentence) for word in sentence.split(\" \")])\n",
        "\n",
        "test_sentences = torch.tensor([[0, 13, 12],\n",
        "                              [0, 8, 9],\n",
        "                              [8, 7, 6],\n",
        "                              [5, 4, 5]])\n",
        "output = model(torch.tensor(encoded_sentences)).detach().numpy()\n",
        "\n",
        "# Predict\n",
        "predictions = np.argmax(output, axis=1)\n",
        "\n",
        "for prediction in predictions:\n",
        "  print(vocab_builder.get_token_str(prediction))"
      ],
      "metadata": {
        "id": "HOp6TdEPgtmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_sequences = preparer.create_training_sequences(val_set)\n",
        "val_dataloader = TorchDataLoader(val_sequences, 4)\n",
        "\n",
        "loss = []\n",
        "for batch_context, batch_target in val_dataloader:\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context\n",
        "        Y = batch_target\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        outputs = (model(X))  # Model output for X\n",
        "        loss.append((criterion(outputs, Y)).item()) # Compute the loss between model output and Y\n",
        "\n",
        "# Compute perplexity\n",
        "perplexity = np.exp(np.mean(loss))\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "9c_IdNVZiR7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbors(emb, voc, inv_voc, word, n_neighbors=5):\n",
        "\n",
        "    # Look up the embedding for the test word.\n",
        "    test_emb = emb.weight[voc[word]]\n",
        "\n",
        "    # We'll use a cosine similarity function to find the most similar words.\n",
        "    sim_func = nn.CosineSimilarity(dim=1)\n",
        "    cosine_scores = sim_func(test_emb, emb.weight)\n",
        "\n",
        "    # Find the positions of the highest cosine values.\n",
        "    near_nbr = cosine_scores.topk(n_neighbors+1)\n",
        "    topk_cos = near_nbr.values[1:]\n",
        "    topk_indices = near_nbr.indices[1:]\n",
        "    # NB: the first word in the top-k list is the query word itself!\n",
        "    # That's why we skip the first position in the code above.\n",
        "\n",
        "    # Finally, map word indices back to strings, and put the result in a list.\n",
        "    return [ (inv_voc[ix.item()], cos.item()) for ix, cos in zip(topk_indices, topk_cos) ]\n",
        "\n",
        "nearest_neighbors(vocab_builder)\n",
        "nearest_neighbors(\"2005\")"
      ],
      "metadata": {
        "id": "Vm5IGathjhMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_embeddings_pca(emb, inv_voc, words):\n",
        "    vectors = np.vstack([emb.weight[inv_voc[w]].cpu().detach().numpy() for w in words])\n",
        "    vectors -= vectors.mean(axis=0)\n",
        "    twodim = TruncatedSVD(n_components=2).fit_transform(vectors)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.02, y, word)\n",
        "    plt.axis('off')\n",
        "\n",
        "plot_embeddings_pca(model, prepr, ['sweden', 'denmark', 'europe', 'africa', 'london', 'stockholm', 'large', 'small', 'great', 'black', '3', '7', '10', 'seven', 'three', 'ten', '1984', '2005', '2010'])\n"
      ],
      "metadata": {
        "id": "RpQwos6mj8di"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}