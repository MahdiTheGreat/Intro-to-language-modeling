{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiTheGreat/Intro-to-language-modeling/blob/main/modified_intro_to_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparations"
      ],
      "metadata": {
        "id": "Ash16YV-b6T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MahdiTheGreat/Intro-to-language-modeling.git\n",
        "%cd Intro-to-language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXuQ-VlzuRv",
        "outputId": "09c6e1d9-8e2e-445f-bccd-7a896b20d7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-language-modeling'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 64 (delta 34), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (64/64), 30.34 MiB | 14.13 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/content/Intro-to-language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "vnqYc-ovemSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipdb\n",
        "!pip install -U spacy\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOhhHoBLSQyw",
        "outputId": "0ff25f20-02fc-44e6-cffb-eeaf384f2223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
            "  Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
            "  Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.0.1 numpy-2.0.2 spacy-3.8.2 thinc-8.3.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fv9gQcVafW3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=2024):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(1998)"
      ],
      "metadata": {
        "id": "Qjj4IdOi08ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to plot the training metrics\n",
        "\n",
        "def plot_training_metrics(train_acc, val_acc, train_loss, title, save_path):\n",
        "    # Ensure that all input lists have the same length\n",
        "    assert len(train_acc) == len(val_acc) == len(train_loss), \"All input histories must have the same length.\"\n",
        "\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    # Create the metrics DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Accuracy (%)': train_acc,\n",
        "        'Validation Accuracy (%)': val_acc,\n",
        "        'Training Loss': train_loss\n",
        "    })\n",
        "\n",
        "    # Initialize the plot\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Training and Validation Accuracy on ax1\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy (%)', color=color)\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Training Accuracy (%)'], label='Train Acc', color='tab:blue')\n",
        "    ax1.plot(df_metrics['Epoch'], df_metrics['Validation Accuracy (%)'], label='Val Acc', color='tab:cyan')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Create a second y-axis for Training Loss\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Train Loss', color='tab:red')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
        "\n",
        "    # Set plot title and layout\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save and display the plot\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TDDGQTI51AF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDybdl906rv",
        "outputId": "3eef2953-38a2-4622-9136-2c9f1870a5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 & 2"
      ],
      "metadata": {
        "id": "8VLcOGFvb-pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='lmdemo'\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "!unzip -q $zip_file\n",
        "!rm $zip_file"
      ],
      "metadata": {
        "id": "IE8oAx8b3AWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set=open(f'{dataset}/train.txt','r',encoding='utf-8').read()\n",
        "val_set=open(f'{dataset}/val.txt','r',encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "clFRaGPQ4Jc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "HJ_uX3IDof2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7503f07f-70e7-4b22-e9f3-513194b35e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_filepath=\"example.txt\""
      ],
      "metadata": {
        "id": "0YlBHofdlJsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "from enum import Enum, auto\n",
        "\n",
        "class Special_tokens(Enum):\n",
        "    BEGINNING = \"BEGINNING\"\n",
        "    END = \"END\"\n",
        "    UNKNOWN = \"UNKOWN\"\n",
        "\n",
        "# Load spaCy model for tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class VocabularyBuilder:\n",
        "    def __init__(self, max_voc_size=None):\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.str_to_int = {}\n",
        "        self.int_to_str = {}\n",
        "        self.special_tokens = [token.value for token in Special_tokens]\n",
        "        self.token_counter = None\n",
        "\n",
        "    def get_token_counter(self,filepath,nlp,encoding):\n",
        "\n",
        "     buffer = \"\"  # Buffer to store partial sentences between lines\n",
        "     token_counter = Counter()\n",
        "\n",
        "     def sent_processor(sent,complete=True):\n",
        "      tokens=[]\n",
        "      if complete: tokens.append(nlp(Special_tokens.BEGINNING.value)[0])  # Add \"BEGINNING\" at the start of each sentence\n",
        "      tokens.extend([token for token in sent])  # Add sentence tokens\n",
        "      if complete: tokens.append(nlp(Special_tokens.END.value)[0])  # Add \"END\" at the end of each sentence\n",
        "      for token in tokens:\n",
        "       if not token.is_space:\n",
        "        if token.text not in self.special_tokens:\n",
        "         token_counter[token.text] += 1\n",
        "        else:\n",
        "         token_counter[token.text.lower()] += 1\n",
        "\n",
        "\n",
        "     with open(filepath, 'r') as file:\n",
        "      lines = [line for line in file]\n",
        "      for line in tqdm(lines, desc=\"Processing Lines for tokens\", unit=\"Lines\"):\n",
        "          # Add line to buffer and process with spaCy\n",
        "          buffer += \" \" + line.strip()\n",
        "          doc = nlp(buffer)\n",
        "          # Extract complete sentences\n",
        "          sentences = list(doc.sents)\n",
        "          for i, sent in enumerate(sentences):\n",
        "              # If it's not the last sentence, we print it as it's complete\n",
        "              if i < len(sentences) - 1:\n",
        "                  print(sent)\n",
        "                  sent_processor(sent)\n",
        "              else:\n",
        "                  # If it's the last sentence, store it in the buffer in case it's incomplete\n",
        "                  buffer = sent.text\n",
        "                  # Process sentences and identify complete sentences\n",
        "          for sent in doc.sents:\n",
        "              if sent.end_char < len(buffer):\n",
        "                  print(sent)\n",
        "                  sent_processor(sent)\n",
        "      # Process any remaining content in the buffer\n",
        "      doc = nlp(buffer)\n",
        "      for sent in doc.sents:\n",
        "       print(sent)\n",
        "       sent_processor(sent)\n",
        "     return token_counter\n",
        "\n",
        "    def build_vocabulary(self, filepath, nlp,token_counter_savepath=None,token_counter_loadpath=None,encoding=\"utf-8\"):\n",
        "\n",
        "        # Tokenize text and count tokens\n",
        "        if token_counter_loadpath is not None:\n",
        "         with open(token_counter_loadpath, \"r\") as file:\n",
        "            self.token_counter = Counter(json.load(file))\n",
        "        else:\n",
        "         self.token_counter =self.get_token_counter(filepath=filepath,nlp=nlp,encoding=encoding)\n",
        "\n",
        "        # Start vocabulary with special tokens\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Select the most common tokens, considering max_voc_size - len(special_tokens)\n",
        "        if self.max_voc_size is None:\n",
        "            max_words = len(self.token_counter) - len(self.special_tokens)\n",
        "            self.max_voc_size = max_words + len(self.special_tokens)\n",
        "        else:\n",
        "         max_words = self.max_voc_size - len(self.special_tokens)\n",
        "        most_common_tokens = self.token_counter.most_common(max_words)\n",
        "\n",
        "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.special_tokens)):\n",
        "            self.str_to_int[token] = idx\n",
        "            self.int_to_str[idx] = token\n",
        "\n",
        "        # Save to a JSON file\n",
        "        if token_counter_savepath is not None:\n",
        "         with open(token_counter_savepath, \"w\") as file:\n",
        "             json.dump(self.token_counter, file)\n",
        "\n",
        "\n",
        "    def get_token_id(self, token):\n",
        "        # Return the integer ID for a given token\n",
        "        token=token.lower() if token not in self.special_tokens else token\n",
        "        return self.str_to_int.get(token, self.str_to_int[Special_tokens.UNKNOWN.value])\n",
        "\n",
        "    def get_token_str(self, token_id):\n",
        "        # Return the original token string for a given integer ID\n",
        "        return self.int_to_str.get(token_id, Special_tokens.UNKNOWN.value)\n",
        "\n",
        "    def sanity_check(self):\n",
        "        # Check vocabulary size\n",
        "        assert len(self.str_to_int) <= self.max_voc_size, \"Vocabulary size exceeds max_voc_size.\"\n",
        "\n",
        "        # Check special tokens exist and are unique\n",
        "        for token in self.special_tokens:\n",
        "            assert token in self.str_to_int, f\"Missing special token: {token}\"\n",
        "\n",
        "        # Check if highly frequent words are included and rare ones are not\n",
        "        common_words = [\"the\", \"and\"]\n",
        "        rare_words = [\"cuboidal\", \"epiglottis\"]\n",
        "\n",
        "        for word in common_words:\n",
        "            assert word in self.str_to_int, f\"Common word '{word}' not in vocabulary.\"\n",
        "\n",
        "        for word in rare_words:\n",
        "            assert word not in self.str_to_int, f\"Rare word '{word}' should not be in vocabulary.\"\n",
        "\n",
        "        # Check that mapping back and forth works for a test word\n",
        "        test_word = \"The\"\n",
        "        token_id = self.get_token_id(test_word)\n",
        "        assert self.get_token_str(token_id) == test_word.lower(), \"Round-trip token mapping failed.\"\n",
        "\n",
        "        print(\"Sanity check passed!\")\n",
        "\n",
        "token_counter_filepath=\"token_counter.json\"\n",
        "vocab_builder = VocabularyBuilder()\n",
        "vocab_builder.build_vocabulary(filepath=example_filepath, nlp=nlp,token_counter_savepath=token_counter_filepath)\n",
        "\n",
        "# Example mappings\n",
        "print(\"str_to_int:\", vocab_builder.str_to_int)\n",
        "print(\"int_to_str:\", vocab_builder.int_to_str)\n",
        "print(\"vocabulary size: \",len(vocab_builder.token_counter))\n",
        "\n",
        "# Convert a token to integer ID and back to string\n",
        "token_id = vocab_builder.get_token_id(\"example\")\n",
        "print(\"Token ID for 'example':\", token_id)\n",
        "print(\"Original token from ID:\", vocab_builder.get_token_str(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzu9MwAtlMHS",
        "outputId": "7e0024c3-b13e-422e-b197-6009969330a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Lines for tokens: 100%|██████████| 5/5 [00:00<00:00, 31.69Lines/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Anatomy Anatomy (Greek anatomē, “dissection”) is the branch of biology concerned with the study of the structure of organisms and their parts.  \n",
            "Anatomy is a branch of natural science dealing with the structural organization of living things.  \n",
            "It is an old science, having its beginnings in prehistoric times.  \n",
            "Anatomy is inherently tied to embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and long (evolution) timescales.  \n",
            "Human anatomy is one of the basic essential sciences of medicine.\n",
            "The discipline of anatomy is divided into macroscopic and microscopic anatomy.  \n",
            "Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight.  \n",
            "Gross anatomy also includes the branch of superficial anatomy.  \n",
            "Human anatomy is one of the basic essential sciences of medicine.\n",
            "The discipline of anatomy is divided into macroscopic and microscopic anatomy.  \n",
            "Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.\n",
            "str_to_int: {'BEGINNING': 0, 'END': 1, 'UNKOWN': 2, 'of': 3, 'the': 4, 'anatomy': 5, 'beginning': 6, '.': 7, 'end': 8, ',': 9, 'is': 10, 'and': 11, 'Anatomy': 12, '(': 13, ')': 14, 'branch': 15, 'study': 16, 'in': 17, 'biology': 18, 'with': 19, 'parts': 20, 'science': 21, 'an': 22, 'embryology': 23, 'as': 24, 'Human': 25, 'one': 26, 'basic': 27, 'essential': 28, 'sciences': 29, 'medicine': 30, 'The': 31, 'discipline': 32, 'divided': 33, 'into': 34, 'macroscopic': 35, 'microscopic': 36, 'also': 37, 'Greek': 38, 'anatomē': 39, '“': 40, 'dissection': 41, '”': 42, 'concerned': 43, 'structure': 44, 'organisms': 45, 'their': 46, 'a': 47, 'natural': 48, 'dealing': 49, 'structural': 50, 'organization': 51, 'living': 52, 'things': 53, 'It': 54, 'old': 55, 'having': 56, 'its': 57, 'beginnings': 58, 'prehistoric': 59, 'times': 60, 'inherently': 61, 'tied': 62, 'to': 63, 'comparative': 64, 'evolutionary': 65, 'phylogeny': 66, 'these': 67, 'are': 68, 'processes': 69, 'by': 70, 'which': 71, 'generated': 72, 'over': 73, 'immediate': 74, 'long': 75, 'evolution': 76, 'timescales': 77, 'Macroscopic': 78, 'or': 79, 'gross': 80, 'examination': 81, 'animal': 82, \"'s\": 83, 'body': 84, 'using': 85, 'unaided': 86, 'eyesight': 87, 'Gross': 88, 'includes': 89, 'superficial': 90, 'Microscopic': 91, 'involves': 92, 'use': 93, 'optical': 94, 'instruments': 95, 'tissues': 96, 'various': 97, 'structures': 98}\n",
            "int_to_str: {0: 'BEGINNING', 1: 'END', 2: 'UNKOWN', 3: 'of', 4: 'the', 5: 'anatomy', 6: 'beginning', 7: '.', 8: 'end', 9: ',', 10: 'is', 11: 'and', 12: 'Anatomy', 13: '(', 14: ')', 15: 'branch', 16: 'study', 17: 'in', 18: 'biology', 19: 'with', 20: 'parts', 21: 'science', 22: 'an', 23: 'embryology', 24: 'as', 25: 'Human', 26: 'one', 27: 'basic', 28: 'essential', 29: 'sciences', 30: 'medicine', 31: 'The', 32: 'discipline', 33: 'divided', 34: 'into', 35: 'macroscopic', 36: 'microscopic', 37: 'also', 38: 'Greek', 39: 'anatomē', 40: '“', 41: 'dissection', 42: '”', 43: 'concerned', 44: 'structure', 45: 'organisms', 46: 'their', 47: 'a', 48: 'natural', 49: 'dealing', 50: 'structural', 51: 'organization', 52: 'living', 53: 'things', 54: 'It', 55: 'old', 56: 'having', 57: 'its', 58: 'beginnings', 59: 'prehistoric', 60: 'times', 61: 'inherently', 62: 'tied', 63: 'to', 64: 'comparative', 65: 'evolutionary', 66: 'phylogeny', 67: 'these', 68: 'are', 69: 'processes', 70: 'by', 71: 'which', 72: 'generated', 73: 'over', 74: 'immediate', 75: 'long', 76: 'evolution', 77: 'timescales', 78: 'Macroscopic', 79: 'or', 80: 'gross', 81: 'examination', 82: 'animal', 83: \"'s\", 84: 'body', 85: 'using', 86: 'unaided', 87: 'eyesight', 88: 'Gross', 89: 'includes', 90: 'superficial', 91: 'Microscopic', 92: 'involves', 93: 'use', 94: 'optical', 95: 'instruments', 96: 'tissues', 97: 'various', 98: 'structures'}\n",
            "vocabulary size:  99\n",
            "Token ID for 'example': 2\n",
            "Original token from ID: UNKOWN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_counter_filepath=\"token_counter.json\"\n",
        "vocab_builder = VocabularyBuilder()\n",
        "vocab_builder.build_vocabulary(filepath=example_filepath, nlp=nlp,token_counter_loadpath=token_counter_filepath)\n",
        "\n",
        "# Example mappings\n",
        "print(\"str_to_int:\", vocab_builder.str_to_int)\n",
        "print(\"int_to_str:\", vocab_builder.int_to_str)\n",
        "print(\"vocabulary size: \",len(vocab_builder.token_counter))\n",
        "\n",
        "\n",
        "# Convert a token to integer ID and back to string\n",
        "token_id = vocab_builder.get_token_id(\"example\")\n",
        "print(\"Token ID for 'example':\", token_id)\n",
        "print(\"Original token from ID:\", vocab_builder.get_token_str(token_id))"
      ],
      "metadata": {
        "id": "T1LhuLzfE2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157431a7-e355-489c-8dd3-a8e18b01440d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "str_to_int: {'BEGINNING': 0, 'END': 1, 'UNKOWN': 2, 'of': 3, 'the': 4, 'anatomy': 5, 'beginning': 6, '.': 7, 'end': 8, ',': 9, 'is': 10, 'and': 11, 'Anatomy': 12, '(': 13, ')': 14, 'branch': 15, 'study': 16, 'in': 17, 'biology': 18, 'with': 19, 'parts': 20, 'science': 21, 'an': 22, 'embryology': 23, 'as': 24, 'Human': 25, 'one': 26, 'basic': 27, 'essential': 28, 'sciences': 29, 'medicine': 30, 'The': 31, 'discipline': 32, 'divided': 33, 'into': 34, 'macroscopic': 35, 'microscopic': 36, 'also': 37, 'Greek': 38, 'anatomē': 39, '“': 40, 'dissection': 41, '”': 42, 'concerned': 43, 'structure': 44, 'organisms': 45, 'their': 46, 'a': 47, 'natural': 48, 'dealing': 49, 'structural': 50, 'organization': 51, 'living': 52, 'things': 53, 'It': 54, 'old': 55, 'having': 56, 'its': 57, 'beginnings': 58, 'prehistoric': 59, 'times': 60, 'inherently': 61, 'tied': 62, 'to': 63, 'comparative': 64, 'evolutionary': 65, 'phylogeny': 66, 'these': 67, 'are': 68, 'processes': 69, 'by': 70, 'which': 71, 'generated': 72, 'over': 73, 'immediate': 74, 'long': 75, 'evolution': 76, 'timescales': 77, 'Macroscopic': 78, 'or': 79, 'gross': 80, 'examination': 81, 'animal': 82, \"'s\": 83, 'body': 84, 'using': 85, 'unaided': 86, 'eyesight': 87, 'Gross': 88, 'includes': 89, 'superficial': 90, 'Microscopic': 91, 'involves': 92, 'use': 93, 'optical': 94, 'instruments': 95, 'tissues': 96, 'various': 97, 'structures': 98}\n",
            "int_to_str: {0: 'BEGINNING', 1: 'END', 2: 'UNKOWN', 3: 'of', 4: 'the', 5: 'anatomy', 6: 'beginning', 7: '.', 8: 'end', 9: ',', 10: 'is', 11: 'and', 12: 'Anatomy', 13: '(', 14: ')', 15: 'branch', 16: 'study', 17: 'in', 18: 'biology', 19: 'with', 20: 'parts', 21: 'science', 22: 'an', 23: 'embryology', 24: 'as', 25: 'Human', 26: 'one', 27: 'basic', 28: 'essential', 29: 'sciences', 30: 'medicine', 31: 'The', 32: 'discipline', 33: 'divided', 34: 'into', 35: 'macroscopic', 36: 'microscopic', 37: 'also', 38: 'Greek', 39: 'anatomē', 40: '“', 41: 'dissection', 42: '”', 43: 'concerned', 44: 'structure', 45: 'organisms', 46: 'their', 47: 'a', 48: 'natural', 49: 'dealing', 50: 'structural', 51: 'organization', 52: 'living', 53: 'things', 54: 'It', 55: 'old', 56: 'having', 57: 'its', 58: 'beginnings', 59: 'prehistoric', 60: 'times', 61: 'inherently', 62: 'tied', 63: 'to', 64: 'comparative', 65: 'evolutionary', 66: 'phylogeny', 67: 'these', 68: 'are', 69: 'processes', 70: 'by', 71: 'which', 72: 'generated', 73: 'over', 74: 'immediate', 75: 'long', 76: 'evolution', 77: 'timescales', 78: 'Macroscopic', 79: 'or', 80: 'gross', 81: 'examination', 82: 'animal', 83: \"'s\", 84: 'body', 85: 'using', 86: 'unaided', 87: 'eyesight', 88: 'Gross', 89: 'includes', 90: 'superficial', 91: 'Microscopic', 92: 'involves', 93: 'use', 94: 'optical', 95: 'instruments', 96: 'tissues', 97: 'various', 98: 'structures'}\n",
            "vocabulary size:  99\n",
            "Token ID for 'example': 2\n",
            "Original token from ID: UNKOWN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sanity check\n",
        "vocab_builder.sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sB4W8Q0Qo9",
        "outputId": "aa2221ea-40ef-4508-f489-2204e8cdb0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "class TrainingDataPreparer:\n",
        "    def __init__(self, vocab_builder, nlp, context_window_size=3, chunk_size=1024):\n",
        "        self.vocab_builder = vocab_builder\n",
        "        self.context_window_size = context_window_size\n",
        "        self.chunk_size = chunk_size\n",
        "        self.nlp = nlp\n",
        "\n",
        "    def encode_token(self, token):\n",
        "        token_id = self.vocab_builder.get_token_id(token.text)\n",
        "        return token_id if token_id != self.vocab_builder.get_token_id(Special_tokens.UNKNOWN.value) else None\n",
        "\n",
        "    def prepare_training_data(self, input_file, output_file):\n",
        "        with open(input_file, \"r\") as infile, open(output_file, \"w\", newline=\"\") as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([f\"Token_{i+1}\" for i in range(self.context_window_size)] + [\"Target\"])\n",
        "\n",
        "            # Initialize the beginning padding tokens\n",
        "            padded_tokens = [self.vocab_builder.get_token_id(Special_tokens.BEGINNING.value)] * self.context_window_size\n",
        "            first_chunk = True\n",
        "            file_size = len(infile.read())  # Get the size of the file to calculate progress\n",
        "            infile.seek(0)  # Reset the file pointer to the beginning after reading the file size\n",
        "\n",
        "            # Use tqdm for reading chunks\n",
        "            with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=\"Processing file\") as pbar:\n",
        "             while True:\n",
        "              chunk = infile.read(self.chunk_size)\n",
        "              if not chunk:\n",
        "                  break\n",
        "\n",
        "              # Tokenize chunk into sentences\n",
        "              doc = self.nlp(chunk)\n",
        "              sentences = list(doc.sents)\n",
        "\n",
        "              for sentence in sentences:\n",
        "                  # Process sentence and convert to token IDs, skipping unknowns and spaces\n",
        "                  sentence_token_ids = [\n",
        "                      self.encode_token(token) for token in sentence if self.encode_token(token) is not None\n",
        "                  ]\n",
        "\n",
        "                  if first_chunk and sentence_token_ids:\n",
        "                      padded_tokens += sentence_token_ids\n",
        "                      first_chunk = False\n",
        "                  else:\n",
        "                      # Add only the sentence tokens from subsequent sentences\n",
        "                      padded_tokens.extend(sentence_token_ids)\n",
        "\n",
        "                  # Add END token at the end of each sentence\n",
        "                  padded_tokens.append(self.vocab_builder.get_token_id(Special_tokens.END.value))\n",
        "\n",
        "                  # Generate context-target sequences\n",
        "                  for i in range(len(padded_tokens) - self.context_window_size):\n",
        "                      context = padded_tokens[i:i + self.context_window_size]\n",
        "                      target = padded_tokens[i + self.context_window_size]\n",
        "                      writer.writerow(context + [target])\n",
        "\n",
        "              # Retain only the last context window tokens for the next chunk\n",
        "              padded_tokens = padded_tokens[-self.context_window_size:]\n",
        "\n",
        "        print(\"Training data preparation complete.\")\n",
        "\n",
        "    def print_csv_as_words(self, csv_file):\n",
        "           \"\"\"\n",
        "           Reads a CSV file with token IDs, decodes them to words, and prints each sequence.\n",
        "           \"\"\"\n",
        "           with open(csv_file, \"r\") as file:\n",
        "               reader = csv.reader(file)\n",
        "               headers = next(reader)  # Skip the header\n",
        "\n",
        "               for row in reader:\n",
        "                   context_ids = row[:-1]  # All columns except the last one are context\n",
        "                   target_id = row[-1]  # Last column is the target\n",
        "\n",
        "                   # Convert token IDs to words\n",
        "                   context_words = [self.vocab_builder.get_token_str(int(token_id)) for token_id in context_ids]\n",
        "                   target_word = self.vocab_builder.get_token_str(int(target_id))\n",
        "\n",
        "                   # Print context and target as words\n",
        "                   print(\"Context:\", context_words, \"-> Target:\", target_word)\n",
        "\n",
        "\n",
        "data_preparer = TrainingDataPreparer(vocab_builder=vocab_builder,nlp=nlp, context_window_size=3)\n",
        "\n",
        "input_file = \"example.txt\"\n",
        "output_file = \"training_sequences.csv\"\n",
        "\n",
        "# Prepare training data\n",
        "data_preparer.prepare_training_data(input_file, output_file)\n"
      ],
      "metadata": {
        "id": "uXLrr6YeF0AF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced43bae-2ee3-4c0a-ea84-21f9a41e7eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing file:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data preparation complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=pd.read_csv(\"training_sequences.csv\")\n",
        "print(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIZFPXu6OacK",
        "outputId": "42065b04-b2c9-43bc-85ea-e72203b3fe4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Token_1  Token_2  Token_3  Target\n",
            "0          0        0        0       5\n",
            "1          0        0        5       5\n",
            "2          0        5        5      13\n",
            "3          5        5       13      39\n",
            "4          5       13       39       9\n",
            "..       ...      ...      ...     ...\n",
            "933       11       37       17       4\n",
            "934       37       17        4      16\n",
            "935       17        4       16       3\n",
            "936        4       16        3       7\n",
            "937       16        3        7       1\n",
            "\n",
            "[938 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_preparer.print_csv_as_words(\"training_sequences.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj9ZV2AgOdwC",
        "outputId": "1b50edec-9540-4c9f-eb32-1baad722d83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['times', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: inherently\n",
            "Context: ['anatomy', 'is', 'inherently'] -> Target: tied\n",
            "Context: ['is', 'inherently', 'tied'] -> Target: to\n",
            "Context: ['inherently', 'tied', 'to'] -> Target: embryology\n",
            "Context: ['tied', 'to', 'embryology'] -> Target: ,\n",
            "Context: ['to', 'embryology', ','] -> Target: comparative\n",
            "Context: ['embryology', ',', 'comparative'] -> Target: anatomy\n",
            "Context: [',', 'comparative', 'anatomy'] -> Target: ,\n",
            "Context: ['comparative', 'anatomy', ','] -> Target: evolutionary\n",
            "Context: ['anatomy', ',', 'evolutionary'] -> Target: biology\n",
            "Context: [',', 'evolutionary', 'biology'] -> Target: ,\n",
            "Context: ['evolutionary', 'biology', ','] -> Target: and\n",
            "Context: ['biology', ',', 'and'] -> Target: phylogeny\n",
            "Context: [',', 'and', 'phylogeny'] -> Target: ,\n",
            "Context: ['and', 'phylogeny', ','] -> Target: as\n",
            "Context: ['phylogeny', ',', 'as'] -> Target: these\n",
            "Context: [',', 'as', 'these'] -> Target: are\n",
            "Context: ['as', 'these', 'are'] -> Target: the\n",
            "Context: ['these', 'are', 'the'] -> Target: processes\n",
            "Context: ['are', 'the', 'processes'] -> Target: by\n",
            "Context: ['the', 'processes', 'by'] -> Target: which\n",
            "Context: ['processes', 'by', 'which'] -> Target: anatomy\n",
            "Context: ['by', 'which', 'anatomy'] -> Target: is\n",
            "Context: ['which', 'anatomy', 'is'] -> Target: generated\n",
            "Context: ['anatomy', 'is', 'generated'] -> Target: over\n",
            "Context: ['is', 'generated', 'over'] -> Target: immediate\n",
            "Context: ['generated', 'over', 'immediate'] -> Target: (\n",
            "Context: ['over', 'immediate', '('] -> Target: embryology\n",
            "Context: ['immediate', '(', 'embryology'] -> Target: )\n",
            "Context: ['(', 'embryology', ')'] -> Target: and\n",
            "Context: ['embryology', ')', 'and'] -> Target: long\n",
            "Context: [')', 'and', 'long'] -> Target: (\n",
            "Context: ['and', 'long', '('] -> Target: evolution\n",
            "Context: ['long', '(', 'evolution'] -> Target: )\n",
            "Context: ['(', 'evolution', ')'] -> Target: timescales\n",
            "Context: ['evolution', ')', 'timescales'] -> Target: .\n",
            "Context: [')', 'timescales', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['times', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: inherently\n",
            "Context: ['anatomy', 'is', 'inherently'] -> Target: tied\n",
            "Context: ['is', 'inherently', 'tied'] -> Target: to\n",
            "Context: ['inherently', 'tied', 'to'] -> Target: embryology\n",
            "Context: ['tied', 'to', 'embryology'] -> Target: ,\n",
            "Context: ['to', 'embryology', ','] -> Target: comparative\n",
            "Context: ['embryology', ',', 'comparative'] -> Target: anatomy\n",
            "Context: [',', 'comparative', 'anatomy'] -> Target: ,\n",
            "Context: ['comparative', 'anatomy', ','] -> Target: evolutionary\n",
            "Context: ['anatomy', ',', 'evolutionary'] -> Target: biology\n",
            "Context: [',', 'evolutionary', 'biology'] -> Target: ,\n",
            "Context: ['evolutionary', 'biology', ','] -> Target: and\n",
            "Context: ['biology', ',', 'and'] -> Target: phylogeny\n",
            "Context: [',', 'and', 'phylogeny'] -> Target: ,\n",
            "Context: ['and', 'phylogeny', ','] -> Target: as\n",
            "Context: ['phylogeny', ',', 'as'] -> Target: these\n",
            "Context: [',', 'as', 'these'] -> Target: are\n",
            "Context: ['as', 'these', 'are'] -> Target: the\n",
            "Context: ['these', 'are', 'the'] -> Target: processes\n",
            "Context: ['are', 'the', 'processes'] -> Target: by\n",
            "Context: ['the', 'processes', 'by'] -> Target: which\n",
            "Context: ['processes', 'by', 'which'] -> Target: anatomy\n",
            "Context: ['by', 'which', 'anatomy'] -> Target: is\n",
            "Context: ['which', 'anatomy', 'is'] -> Target: generated\n",
            "Context: ['anatomy', 'is', 'generated'] -> Target: over\n",
            "Context: ['is', 'generated', 'over'] -> Target: immediate\n",
            "Context: ['generated', 'over', 'immediate'] -> Target: (\n",
            "Context: ['over', 'immediate', '('] -> Target: embryology\n",
            "Context: ['immediate', '(', 'embryology'] -> Target: )\n",
            "Context: ['(', 'embryology', ')'] -> Target: and\n",
            "Context: ['embryology', ')', 'and'] -> Target: long\n",
            "Context: [')', 'and', 'long'] -> Target: (\n",
            "Context: ['and', 'long', '('] -> Target: evolution\n",
            "Context: ['long', '(', 'evolution'] -> Target: )\n",
            "Context: ['(', 'evolution', ')'] -> Target: timescales\n",
            "Context: ['evolution', ')', 'timescales'] -> Target: .\n",
            "Context: [')', 'timescales', '.'] -> Target: END\n",
            "Context: ['timescales', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: one\n",
            "Context: ['anatomy', 'is', 'one'] -> Target: of\n",
            "Context: ['is', 'one', 'of'] -> Target: the\n",
            "Context: ['one', 'of', 'the'] -> Target: basic\n",
            "Context: ['of', 'the', 'basic'] -> Target: essential\n",
            "Context: ['the', 'basic', 'essential'] -> Target: sciences\n",
            "Context: ['basic', 'essential', 'sciences'] -> Target: of\n",
            "Context: ['essential', 'sciences', 'of'] -> Target: medicine\n",
            "Context: ['sciences', 'of', 'medicine'] -> Target: .\n",
            "Context: ['of', 'medicine', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['times', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: inherently\n",
            "Context: ['anatomy', 'is', 'inherently'] -> Target: tied\n",
            "Context: ['is', 'inherently', 'tied'] -> Target: to\n",
            "Context: ['inherently', 'tied', 'to'] -> Target: embryology\n",
            "Context: ['tied', 'to', 'embryology'] -> Target: ,\n",
            "Context: ['to', 'embryology', ','] -> Target: comparative\n",
            "Context: ['embryology', ',', 'comparative'] -> Target: anatomy\n",
            "Context: [',', 'comparative', 'anatomy'] -> Target: ,\n",
            "Context: ['comparative', 'anatomy', ','] -> Target: evolutionary\n",
            "Context: ['anatomy', ',', 'evolutionary'] -> Target: biology\n",
            "Context: [',', 'evolutionary', 'biology'] -> Target: ,\n",
            "Context: ['evolutionary', 'biology', ','] -> Target: and\n",
            "Context: ['biology', ',', 'and'] -> Target: phylogeny\n",
            "Context: [',', 'and', 'phylogeny'] -> Target: ,\n",
            "Context: ['and', 'phylogeny', ','] -> Target: as\n",
            "Context: ['phylogeny', ',', 'as'] -> Target: these\n",
            "Context: [',', 'as', 'these'] -> Target: are\n",
            "Context: ['as', 'these', 'are'] -> Target: the\n",
            "Context: ['these', 'are', 'the'] -> Target: processes\n",
            "Context: ['are', 'the', 'processes'] -> Target: by\n",
            "Context: ['the', 'processes', 'by'] -> Target: which\n",
            "Context: ['processes', 'by', 'which'] -> Target: anatomy\n",
            "Context: ['by', 'which', 'anatomy'] -> Target: is\n",
            "Context: ['which', 'anatomy', 'is'] -> Target: generated\n",
            "Context: ['anatomy', 'is', 'generated'] -> Target: over\n",
            "Context: ['is', 'generated', 'over'] -> Target: immediate\n",
            "Context: ['generated', 'over', 'immediate'] -> Target: (\n",
            "Context: ['over', 'immediate', '('] -> Target: embryology\n",
            "Context: ['immediate', '(', 'embryology'] -> Target: )\n",
            "Context: ['(', 'embryology', ')'] -> Target: and\n",
            "Context: ['embryology', ')', 'and'] -> Target: long\n",
            "Context: [')', 'and', 'long'] -> Target: (\n",
            "Context: ['and', 'long', '('] -> Target: evolution\n",
            "Context: ['long', '(', 'evolution'] -> Target: )\n",
            "Context: ['(', 'evolution', ')'] -> Target: timescales\n",
            "Context: ['evolution', ')', 'timescales'] -> Target: .\n",
            "Context: [')', 'timescales', '.'] -> Target: END\n",
            "Context: ['timescales', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: one\n",
            "Context: ['anatomy', 'is', 'one'] -> Target: of\n",
            "Context: ['is', 'one', 'of'] -> Target: the\n",
            "Context: ['one', 'of', 'the'] -> Target: basic\n",
            "Context: ['of', 'the', 'basic'] -> Target: essential\n",
            "Context: ['the', 'basic', 'essential'] -> Target: sciences\n",
            "Context: ['basic', 'essential', 'sciences'] -> Target: of\n",
            "Context: ['essential', 'sciences', 'of'] -> Target: medicine\n",
            "Context: ['sciences', 'of', 'medicine'] -> Target: .\n",
            "Context: ['of', 'medicine', '.'] -> Target: END\n",
            "Context: ['medicine', '.', 'END'] -> Target: the\n",
            "Context: ['.', 'END', 'the'] -> Target: discipline\n",
            "Context: ['END', 'the', 'discipline'] -> Target: of\n",
            "Context: ['the', 'discipline', 'of'] -> Target: anatomy\n",
            "Context: ['discipline', 'of', 'anatomy'] -> Target: is\n",
            "Context: ['of', 'anatomy', 'is'] -> Target: divided\n",
            "Context: ['anatomy', 'is', 'divided'] -> Target: into\n",
            "Context: ['is', 'divided', 'into'] -> Target: macroscopic\n",
            "Context: ['divided', 'into', 'macroscopic'] -> Target: and\n",
            "Context: ['into', 'macroscopic', 'and'] -> Target: microscopic\n",
            "Context: ['macroscopic', 'and', 'microscopic'] -> Target: anatomy\n",
            "Context: ['and', 'microscopic', 'anatomy'] -> Target: .\n",
            "Context: ['microscopic', 'anatomy', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['times', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: inherently\n",
            "Context: ['anatomy', 'is', 'inherently'] -> Target: tied\n",
            "Context: ['is', 'inherently', 'tied'] -> Target: to\n",
            "Context: ['inherently', 'tied', 'to'] -> Target: embryology\n",
            "Context: ['tied', 'to', 'embryology'] -> Target: ,\n",
            "Context: ['to', 'embryology', ','] -> Target: comparative\n",
            "Context: ['embryology', ',', 'comparative'] -> Target: anatomy\n",
            "Context: [',', 'comparative', 'anatomy'] -> Target: ,\n",
            "Context: ['comparative', 'anatomy', ','] -> Target: evolutionary\n",
            "Context: ['anatomy', ',', 'evolutionary'] -> Target: biology\n",
            "Context: [',', 'evolutionary', 'biology'] -> Target: ,\n",
            "Context: ['evolutionary', 'biology', ','] -> Target: and\n",
            "Context: ['biology', ',', 'and'] -> Target: phylogeny\n",
            "Context: [',', 'and', 'phylogeny'] -> Target: ,\n",
            "Context: ['and', 'phylogeny', ','] -> Target: as\n",
            "Context: ['phylogeny', ',', 'as'] -> Target: these\n",
            "Context: [',', 'as', 'these'] -> Target: are\n",
            "Context: ['as', 'these', 'are'] -> Target: the\n",
            "Context: ['these', 'are', 'the'] -> Target: processes\n",
            "Context: ['are', 'the', 'processes'] -> Target: by\n",
            "Context: ['the', 'processes', 'by'] -> Target: which\n",
            "Context: ['processes', 'by', 'which'] -> Target: anatomy\n",
            "Context: ['by', 'which', 'anatomy'] -> Target: is\n",
            "Context: ['which', 'anatomy', 'is'] -> Target: generated\n",
            "Context: ['anatomy', 'is', 'generated'] -> Target: over\n",
            "Context: ['is', 'generated', 'over'] -> Target: immediate\n",
            "Context: ['generated', 'over', 'immediate'] -> Target: (\n",
            "Context: ['over', 'immediate', '('] -> Target: embryology\n",
            "Context: ['immediate', '(', 'embryology'] -> Target: )\n",
            "Context: ['(', 'embryology', ')'] -> Target: and\n",
            "Context: ['embryology', ')', 'and'] -> Target: long\n",
            "Context: [')', 'and', 'long'] -> Target: (\n",
            "Context: ['and', 'long', '('] -> Target: evolution\n",
            "Context: ['long', '(', 'evolution'] -> Target: )\n",
            "Context: ['(', 'evolution', ')'] -> Target: timescales\n",
            "Context: ['evolution', ')', 'timescales'] -> Target: .\n",
            "Context: [')', 'timescales', '.'] -> Target: END\n",
            "Context: ['timescales', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: one\n",
            "Context: ['anatomy', 'is', 'one'] -> Target: of\n",
            "Context: ['is', 'one', 'of'] -> Target: the\n",
            "Context: ['one', 'of', 'the'] -> Target: basic\n",
            "Context: ['of', 'the', 'basic'] -> Target: essential\n",
            "Context: ['the', 'basic', 'essential'] -> Target: sciences\n",
            "Context: ['basic', 'essential', 'sciences'] -> Target: of\n",
            "Context: ['essential', 'sciences', 'of'] -> Target: medicine\n",
            "Context: ['sciences', 'of', 'medicine'] -> Target: .\n",
            "Context: ['of', 'medicine', '.'] -> Target: END\n",
            "Context: ['medicine', '.', 'END'] -> Target: the\n",
            "Context: ['.', 'END', 'the'] -> Target: discipline\n",
            "Context: ['END', 'the', 'discipline'] -> Target: of\n",
            "Context: ['the', 'discipline', 'of'] -> Target: anatomy\n",
            "Context: ['discipline', 'of', 'anatomy'] -> Target: is\n",
            "Context: ['of', 'anatomy', 'is'] -> Target: divided\n",
            "Context: ['anatomy', 'is', 'divided'] -> Target: into\n",
            "Context: ['is', 'divided', 'into'] -> Target: macroscopic\n",
            "Context: ['divided', 'into', 'macroscopic'] -> Target: and\n",
            "Context: ['into', 'macroscopic', 'and'] -> Target: microscopic\n",
            "Context: ['macroscopic', 'and', 'microscopic'] -> Target: anatomy\n",
            "Context: ['and', 'microscopic', 'anatomy'] -> Target: .\n",
            "Context: ['microscopic', 'anatomy', '.'] -> Target: END\n",
            "Context: ['anatomy', '.', 'END'] -> Target: macroscopic\n",
            "Context: ['.', 'END', 'macroscopic'] -> Target: anatomy\n",
            "Context: ['END', 'macroscopic', 'anatomy'] -> Target: ,\n",
            "Context: ['macroscopic', 'anatomy', ','] -> Target: or\n",
            "Context: ['anatomy', ',', 'or'] -> Target: gross\n",
            "Context: [',', 'or', 'gross'] -> Target: anatomy\n",
            "Context: ['or', 'gross', 'anatomy'] -> Target: ,\n",
            "Context: ['gross', 'anatomy', ','] -> Target: is\n",
            "Context: ['anatomy', ',', 'is'] -> Target: the\n",
            "Context: [',', 'is', 'the'] -> Target: examination\n",
            "Context: ['is', 'the', 'examination'] -> Target: of\n",
            "Context: ['the', 'examination', 'of'] -> Target: an\n",
            "Context: ['examination', 'of', 'an'] -> Target: animal\n",
            "Context: ['of', 'an', 'animal'] -> Target: 's\n",
            "Context: ['an', 'animal', \"'s\"] -> Target: body\n",
            "Context: ['animal', \"'s\", 'body'] -> Target: parts\n",
            "Context: [\"'s\", 'body', 'parts'] -> Target: using\n",
            "Context: ['body', 'parts', 'using'] -> Target: unaided\n",
            "Context: ['parts', 'using', 'unaided'] -> Target: eyesight\n",
            "Context: ['using', 'unaided', 'eyesight'] -> Target: .\n",
            "Context: ['unaided', 'eyesight', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['times', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: inherently\n",
            "Context: ['anatomy', 'is', 'inherently'] -> Target: tied\n",
            "Context: ['is', 'inherently', 'tied'] -> Target: to\n",
            "Context: ['inherently', 'tied', 'to'] -> Target: embryology\n",
            "Context: ['tied', 'to', 'embryology'] -> Target: ,\n",
            "Context: ['to', 'embryology', ','] -> Target: comparative\n",
            "Context: ['embryology', ',', 'comparative'] -> Target: anatomy\n",
            "Context: [',', 'comparative', 'anatomy'] -> Target: ,\n",
            "Context: ['comparative', 'anatomy', ','] -> Target: evolutionary\n",
            "Context: ['anatomy', ',', 'evolutionary'] -> Target: biology\n",
            "Context: [',', 'evolutionary', 'biology'] -> Target: ,\n",
            "Context: ['evolutionary', 'biology', ','] -> Target: and\n",
            "Context: ['biology', ',', 'and'] -> Target: phylogeny\n",
            "Context: [',', 'and', 'phylogeny'] -> Target: ,\n",
            "Context: ['and', 'phylogeny', ','] -> Target: as\n",
            "Context: ['phylogeny', ',', 'as'] -> Target: these\n",
            "Context: [',', 'as', 'these'] -> Target: are\n",
            "Context: ['as', 'these', 'are'] -> Target: the\n",
            "Context: ['these', 'are', 'the'] -> Target: processes\n",
            "Context: ['are', 'the', 'processes'] -> Target: by\n",
            "Context: ['the', 'processes', 'by'] -> Target: which\n",
            "Context: ['processes', 'by', 'which'] -> Target: anatomy\n",
            "Context: ['by', 'which', 'anatomy'] -> Target: is\n",
            "Context: ['which', 'anatomy', 'is'] -> Target: generated\n",
            "Context: ['anatomy', 'is', 'generated'] -> Target: over\n",
            "Context: ['is', 'generated', 'over'] -> Target: immediate\n",
            "Context: ['generated', 'over', 'immediate'] -> Target: (\n",
            "Context: ['over', 'immediate', '('] -> Target: embryology\n",
            "Context: ['immediate', '(', 'embryology'] -> Target: )\n",
            "Context: ['(', 'embryology', ')'] -> Target: and\n",
            "Context: ['embryology', ')', 'and'] -> Target: long\n",
            "Context: [')', 'and', 'long'] -> Target: (\n",
            "Context: ['and', 'long', '('] -> Target: evolution\n",
            "Context: ['long', '(', 'evolution'] -> Target: )\n",
            "Context: ['(', 'evolution', ')'] -> Target: timescales\n",
            "Context: ['evolution', ')', 'timescales'] -> Target: .\n",
            "Context: [')', 'timescales', '.'] -> Target: END\n",
            "Context: ['timescales', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: one\n",
            "Context: ['anatomy', 'is', 'one'] -> Target: of\n",
            "Context: ['is', 'one', 'of'] -> Target: the\n",
            "Context: ['one', 'of', 'the'] -> Target: basic\n",
            "Context: ['of', 'the', 'basic'] -> Target: essential\n",
            "Context: ['the', 'basic', 'essential'] -> Target: sciences\n",
            "Context: ['basic', 'essential', 'sciences'] -> Target: of\n",
            "Context: ['essential', 'sciences', 'of'] -> Target: medicine\n",
            "Context: ['sciences', 'of', 'medicine'] -> Target: .\n",
            "Context: ['of', 'medicine', '.'] -> Target: END\n",
            "Context: ['medicine', '.', 'END'] -> Target: the\n",
            "Context: ['.', 'END', 'the'] -> Target: discipline\n",
            "Context: ['END', 'the', 'discipline'] -> Target: of\n",
            "Context: ['the', 'discipline', 'of'] -> Target: anatomy\n",
            "Context: ['discipline', 'of', 'anatomy'] -> Target: is\n",
            "Context: ['of', 'anatomy', 'is'] -> Target: divided\n",
            "Context: ['anatomy', 'is', 'divided'] -> Target: into\n",
            "Context: ['is', 'divided', 'into'] -> Target: macroscopic\n",
            "Context: ['divided', 'into', 'macroscopic'] -> Target: and\n",
            "Context: ['into', 'macroscopic', 'and'] -> Target: microscopic\n",
            "Context: ['macroscopic', 'and', 'microscopic'] -> Target: anatomy\n",
            "Context: ['and', 'microscopic', 'anatomy'] -> Target: .\n",
            "Context: ['microscopic', 'anatomy', '.'] -> Target: END\n",
            "Context: ['anatomy', '.', 'END'] -> Target: macroscopic\n",
            "Context: ['.', 'END', 'macroscopic'] -> Target: anatomy\n",
            "Context: ['END', 'macroscopic', 'anatomy'] -> Target: ,\n",
            "Context: ['macroscopic', 'anatomy', ','] -> Target: or\n",
            "Context: ['anatomy', ',', 'or'] -> Target: gross\n",
            "Context: [',', 'or', 'gross'] -> Target: anatomy\n",
            "Context: ['or', 'gross', 'anatomy'] -> Target: ,\n",
            "Context: ['gross', 'anatomy', ','] -> Target: is\n",
            "Context: ['anatomy', ',', 'is'] -> Target: the\n",
            "Context: [',', 'is', 'the'] -> Target: examination\n",
            "Context: ['is', 'the', 'examination'] -> Target: of\n",
            "Context: ['the', 'examination', 'of'] -> Target: an\n",
            "Context: ['examination', 'of', 'an'] -> Target: animal\n",
            "Context: ['of', 'an', 'animal'] -> Target: 's\n",
            "Context: ['an', 'animal', \"'s\"] -> Target: body\n",
            "Context: ['animal', \"'s\", 'body'] -> Target: parts\n",
            "Context: [\"'s\", 'body', 'parts'] -> Target: using\n",
            "Context: ['body', 'parts', 'using'] -> Target: unaided\n",
            "Context: ['parts', 'using', 'unaided'] -> Target: eyesight\n",
            "Context: ['using', 'unaided', 'eyesight'] -> Target: .\n",
            "Context: ['unaided', 'eyesight', '.'] -> Target: END\n",
            "Context: ['eyesight', '.', 'END'] -> Target: gross\n",
            "Context: ['.', 'END', 'gross'] -> Target: anatomy\n",
            "Context: ['END', 'gross', 'anatomy'] -> Target: also\n",
            "Context: ['gross', 'anatomy', 'also'] -> Target: includes\n",
            "Context: ['anatomy', 'also', 'includes'] -> Target: the\n",
            "Context: ['also', 'includes', 'the'] -> Target: branch\n",
            "Context: ['includes', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: superficial\n",
            "Context: ['branch', 'of', 'superficial'] -> Target: anatomy\n",
            "Context: ['of', 'superficial', 'anatomy'] -> Target: .\n",
            "Context: ['superficial', 'anatomy', '.'] -> Target: END\n",
            "Context: ['BEGINNING', 'BEGINNING', 'BEGINNING'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'BEGINNING', 'anatomy'] -> Target: anatomy\n",
            "Context: ['BEGINNING', 'anatomy', 'anatomy'] -> Target: (\n",
            "Context: ['anatomy', 'anatomy', '('] -> Target: anatomē\n",
            "Context: ['anatomy', '(', 'anatomē'] -> Target: ,\n",
            "Context: ['(', 'anatomē', ','] -> Target: “\n",
            "Context: ['anatomē', ',', '“'] -> Target: dissection\n",
            "Context: [',', '“', 'dissection'] -> Target: ”\n",
            "Context: ['“', 'dissection', '”'] -> Target: )\n",
            "Context: ['dissection', '”', ')'] -> Target: is\n",
            "Context: ['”', ')', 'is'] -> Target: the\n",
            "Context: [')', 'is', 'the'] -> Target: branch\n",
            "Context: ['is', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: biology\n",
            "Context: ['branch', 'of', 'biology'] -> Target: concerned\n",
            "Context: ['of', 'biology', 'concerned'] -> Target: with\n",
            "Context: ['biology', 'concerned', 'with'] -> Target: the\n",
            "Context: ['concerned', 'with', 'the'] -> Target: study\n",
            "Context: ['with', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: structure\n",
            "Context: ['of', 'the', 'structure'] -> Target: of\n",
            "Context: ['the', 'structure', 'of'] -> Target: organisms\n",
            "Context: ['structure', 'of', 'organisms'] -> Target: and\n",
            "Context: ['of', 'organisms', 'and'] -> Target: their\n",
            "Context: ['organisms', 'and', 'their'] -> Target: parts\n",
            "Context: ['and', 'their', 'parts'] -> Target: .\n",
            "Context: ['their', 'parts', '.'] -> Target: END\n",
            "Context: ['parts', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: a\n",
            "Context: ['anatomy', 'is', 'a'] -> Target: branch\n",
            "Context: ['is', 'a', 'branch'] -> Target: of\n",
            "Context: ['a', 'branch', 'of'] -> Target: natural\n",
            "Context: ['branch', 'of', 'natural'] -> Target: science\n",
            "Context: ['of', 'natural', 'science'] -> Target: dealing\n",
            "Context: ['natural', 'science', 'dealing'] -> Target: with\n",
            "Context: ['science', 'dealing', 'with'] -> Target: the\n",
            "Context: ['dealing', 'with', 'the'] -> Target: structural\n",
            "Context: ['with', 'the', 'structural'] -> Target: organization\n",
            "Context: ['the', 'structural', 'organization'] -> Target: of\n",
            "Context: ['structural', 'organization', 'of'] -> Target: living\n",
            "Context: ['organization', 'of', 'living'] -> Target: things\n",
            "Context: ['of', 'living', 'things'] -> Target: .\n",
            "Context: ['living', 'things', '.'] -> Target: END\n",
            "Context: ['things', '.', 'END'] -> Target: is\n",
            "Context: ['.', 'END', 'is'] -> Target: an\n",
            "Context: ['END', 'is', 'an'] -> Target: old\n",
            "Context: ['is', 'an', 'old'] -> Target: science\n",
            "Context: ['an', 'old', 'science'] -> Target: ,\n",
            "Context: ['old', 'science', ','] -> Target: having\n",
            "Context: ['science', ',', 'having'] -> Target: its\n",
            "Context: [',', 'having', 'its'] -> Target: beginnings\n",
            "Context: ['having', 'its', 'beginnings'] -> Target: in\n",
            "Context: ['its', 'beginnings', 'in'] -> Target: prehistoric\n",
            "Context: ['beginnings', 'in', 'prehistoric'] -> Target: times\n",
            "Context: ['in', 'prehistoric', 'times'] -> Target: .\n",
            "Context: ['prehistoric', 'times', '.'] -> Target: END\n",
            "Context: ['times', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: inherently\n",
            "Context: ['anatomy', 'is', 'inherently'] -> Target: tied\n",
            "Context: ['is', 'inherently', 'tied'] -> Target: to\n",
            "Context: ['inherently', 'tied', 'to'] -> Target: embryology\n",
            "Context: ['tied', 'to', 'embryology'] -> Target: ,\n",
            "Context: ['to', 'embryology', ','] -> Target: comparative\n",
            "Context: ['embryology', ',', 'comparative'] -> Target: anatomy\n",
            "Context: [',', 'comparative', 'anatomy'] -> Target: ,\n",
            "Context: ['comparative', 'anatomy', ','] -> Target: evolutionary\n",
            "Context: ['anatomy', ',', 'evolutionary'] -> Target: biology\n",
            "Context: [',', 'evolutionary', 'biology'] -> Target: ,\n",
            "Context: ['evolutionary', 'biology', ','] -> Target: and\n",
            "Context: ['biology', ',', 'and'] -> Target: phylogeny\n",
            "Context: [',', 'and', 'phylogeny'] -> Target: ,\n",
            "Context: ['and', 'phylogeny', ','] -> Target: as\n",
            "Context: ['phylogeny', ',', 'as'] -> Target: these\n",
            "Context: [',', 'as', 'these'] -> Target: are\n",
            "Context: ['as', 'these', 'are'] -> Target: the\n",
            "Context: ['these', 'are', 'the'] -> Target: processes\n",
            "Context: ['are', 'the', 'processes'] -> Target: by\n",
            "Context: ['the', 'processes', 'by'] -> Target: which\n",
            "Context: ['processes', 'by', 'which'] -> Target: anatomy\n",
            "Context: ['by', 'which', 'anatomy'] -> Target: is\n",
            "Context: ['which', 'anatomy', 'is'] -> Target: generated\n",
            "Context: ['anatomy', 'is', 'generated'] -> Target: over\n",
            "Context: ['is', 'generated', 'over'] -> Target: immediate\n",
            "Context: ['generated', 'over', 'immediate'] -> Target: (\n",
            "Context: ['over', 'immediate', '('] -> Target: embryology\n",
            "Context: ['immediate', '(', 'embryology'] -> Target: )\n",
            "Context: ['(', 'embryology', ')'] -> Target: and\n",
            "Context: ['embryology', ')', 'and'] -> Target: long\n",
            "Context: [')', 'and', 'long'] -> Target: (\n",
            "Context: ['and', 'long', '('] -> Target: evolution\n",
            "Context: ['long', '(', 'evolution'] -> Target: )\n",
            "Context: ['(', 'evolution', ')'] -> Target: timescales\n",
            "Context: ['evolution', ')', 'timescales'] -> Target: .\n",
            "Context: [')', 'timescales', '.'] -> Target: END\n",
            "Context: ['timescales', '.', 'END'] -> Target: anatomy\n",
            "Context: ['.', 'END', 'anatomy'] -> Target: is\n",
            "Context: ['END', 'anatomy', 'is'] -> Target: one\n",
            "Context: ['anatomy', 'is', 'one'] -> Target: of\n",
            "Context: ['is', 'one', 'of'] -> Target: the\n",
            "Context: ['one', 'of', 'the'] -> Target: basic\n",
            "Context: ['of', 'the', 'basic'] -> Target: essential\n",
            "Context: ['the', 'basic', 'essential'] -> Target: sciences\n",
            "Context: ['basic', 'essential', 'sciences'] -> Target: of\n",
            "Context: ['essential', 'sciences', 'of'] -> Target: medicine\n",
            "Context: ['sciences', 'of', 'medicine'] -> Target: .\n",
            "Context: ['of', 'medicine', '.'] -> Target: END\n",
            "Context: ['medicine', '.', 'END'] -> Target: the\n",
            "Context: ['.', 'END', 'the'] -> Target: discipline\n",
            "Context: ['END', 'the', 'discipline'] -> Target: of\n",
            "Context: ['the', 'discipline', 'of'] -> Target: anatomy\n",
            "Context: ['discipline', 'of', 'anatomy'] -> Target: is\n",
            "Context: ['of', 'anatomy', 'is'] -> Target: divided\n",
            "Context: ['anatomy', 'is', 'divided'] -> Target: into\n",
            "Context: ['is', 'divided', 'into'] -> Target: macroscopic\n",
            "Context: ['divided', 'into', 'macroscopic'] -> Target: and\n",
            "Context: ['into', 'macroscopic', 'and'] -> Target: microscopic\n",
            "Context: ['macroscopic', 'and', 'microscopic'] -> Target: anatomy\n",
            "Context: ['and', 'microscopic', 'anatomy'] -> Target: .\n",
            "Context: ['microscopic', 'anatomy', '.'] -> Target: END\n",
            "Context: ['anatomy', '.', 'END'] -> Target: macroscopic\n",
            "Context: ['.', 'END', 'macroscopic'] -> Target: anatomy\n",
            "Context: ['END', 'macroscopic', 'anatomy'] -> Target: ,\n",
            "Context: ['macroscopic', 'anatomy', ','] -> Target: or\n",
            "Context: ['anatomy', ',', 'or'] -> Target: gross\n",
            "Context: [',', 'or', 'gross'] -> Target: anatomy\n",
            "Context: ['or', 'gross', 'anatomy'] -> Target: ,\n",
            "Context: ['gross', 'anatomy', ','] -> Target: is\n",
            "Context: ['anatomy', ',', 'is'] -> Target: the\n",
            "Context: [',', 'is', 'the'] -> Target: examination\n",
            "Context: ['is', 'the', 'examination'] -> Target: of\n",
            "Context: ['the', 'examination', 'of'] -> Target: an\n",
            "Context: ['examination', 'of', 'an'] -> Target: animal\n",
            "Context: ['of', 'an', 'animal'] -> Target: 's\n",
            "Context: ['an', 'animal', \"'s\"] -> Target: body\n",
            "Context: ['animal', \"'s\", 'body'] -> Target: parts\n",
            "Context: [\"'s\", 'body', 'parts'] -> Target: using\n",
            "Context: ['body', 'parts', 'using'] -> Target: unaided\n",
            "Context: ['parts', 'using', 'unaided'] -> Target: eyesight\n",
            "Context: ['using', 'unaided', 'eyesight'] -> Target: .\n",
            "Context: ['unaided', 'eyesight', '.'] -> Target: END\n",
            "Context: ['eyesight', '.', 'END'] -> Target: gross\n",
            "Context: ['.', 'END', 'gross'] -> Target: anatomy\n",
            "Context: ['END', 'gross', 'anatomy'] -> Target: also\n",
            "Context: ['gross', 'anatomy', 'also'] -> Target: includes\n",
            "Context: ['anatomy', 'also', 'includes'] -> Target: the\n",
            "Context: ['also', 'includes', 'the'] -> Target: branch\n",
            "Context: ['includes', 'the', 'branch'] -> Target: of\n",
            "Context: ['the', 'branch', 'of'] -> Target: superficial\n",
            "Context: ['branch', 'of', 'superficial'] -> Target: anatomy\n",
            "Context: ['of', 'superficial', 'anatomy'] -> Target: .\n",
            "Context: ['superficial', 'anatomy', '.'] -> Target: END\n",
            "Context: ['anatomy', '.', 'END'] -> Target: microscopic\n",
            "Context: ['.', 'END', 'microscopic'] -> Target: anatomy\n",
            "Context: ['END', 'microscopic', 'anatomy'] -> Target: involves\n",
            "Context: ['microscopic', 'anatomy', 'involves'] -> Target: the\n",
            "Context: ['anatomy', 'involves', 'the'] -> Target: use\n",
            "Context: ['involves', 'the', 'use'] -> Target: of\n",
            "Context: ['the', 'use', 'of'] -> Target: optical\n",
            "Context: ['use', 'of', 'optical'] -> Target: instruments\n",
            "Context: ['of', 'optical', 'instruments'] -> Target: in\n",
            "Context: ['optical', 'instruments', 'in'] -> Target: the\n",
            "Context: ['instruments', 'in', 'the'] -> Target: study\n",
            "Context: ['in', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: the\n",
            "Context: ['study', 'of', 'the'] -> Target: tissues\n",
            "Context: ['of', 'the', 'tissues'] -> Target: of\n",
            "Context: ['the', 'tissues', 'of'] -> Target: various\n",
            "Context: ['tissues', 'of', 'various'] -> Target: structures\n",
            "Context: ['of', 'various', 'structures'] -> Target: ,\n",
            "Context: ['various', 'structures', ','] -> Target: as\n",
            "Context: ['structures', ',', 'as'] -> Target: ,\n",
            "Context: [',', 'as', ','] -> Target: and\n",
            "Context: ['as', ',', 'and'] -> Target: also\n",
            "Context: [',', 'and', 'also'] -> Target: in\n",
            "Context: ['and', 'also', 'in'] -> Target: the\n",
            "Context: ['also', 'in', 'the'] -> Target: study\n",
            "Context: ['in', 'the', 'study'] -> Target: of\n",
            "Context: ['the', 'study', 'of'] -> Target: .\n",
            "Context: ['study', 'of', '.'] -> Target: END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "class TorchDataset(Dataset):\n",
        "    def __init__(self, csv_file, context_window_size):\n",
        "        # Read CSV file using pandas\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.context_window_size = context_window_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # The length of the dataset is the number of rows in the CSV\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a specific row from the data and convert it to a tensor\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # The context is all tokens except for the last one (target)\n",
        "        context = torch.tensor(row[:-1].values, dtype=torch.long)\n",
        "\n",
        "        # The target is the last token in the row\n",
        "        target = torch.tensor(row[-1], dtype=torch.long)\n",
        "\n",
        "        return context, target\n",
        "\n",
        "def TorchDataLoader(training_sequences_csv, batch_size, context_window_size):\n",
        "    # Create the Dataset instance\n",
        "    dataset = TorchDataset(training_sequences_csv, context_window_size)\n",
        "\n",
        "    # Create the DataLoader instance to handle batching\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# Example usage:\n",
        "# Define your CSV file, batch size, and context window size\n",
        "csv_file = 'training_sequences.csv'\n",
        "batch_size = 32\n",
        "context_window_size = 10\n",
        "\n",
        "# Create the DataLoader\n",
        "trainloader = TorchDataLoader(csv_file, batch_size, context_window_size)"
      ],
      "metadata": {
        "id": "3Oxs8q0EWaXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3"
      ],
      "metadata": {
        "id": "6cxG6iHlclgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# EarlyStopping class remains the same\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
        "        self.patience = patience  # Number of epochs to wait for improvement\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.path = path  # Path to save the best model\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss < self.best_score - self.delta:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Save model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class SimpleANN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, layer_sizes,activation=nn.ReLU,last_layer_activation=nn.Softmax,dropout=0):\n",
        "\n",
        "        super(SimpleANN, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes)-2):\n",
        "          self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "          self.layers.append(nn.Dropout(dropout))\n",
        "          self.layers.append(activation())\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "        if last_layer_activation is not None:\n",
        "         self.layers.append(nn.Dropout(dropout))\n",
        "         self.layers.append(last_layer_activation())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is a batch of word indices (e.g., [batch_size])\n",
        "        embeddings = self.embeddings(x)  # Get word embeddings for each word in the batch\n",
        "\n",
        "        # Flatten the input embeddings (if necessary, depending on your task)\n",
        "        x = embeddings.view(-1, np.prod(embeddings.shape[1:]))  # Flatten for fully connected layers\n",
        "\n",
        "        #x = x.view(-1, np.prod(x.shape[1:])) # Flatten the input\n",
        "        x = x.float()\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Q2CkGhOV1JpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleANN(layer_sizes=[48, 64, 65000], vocab_size=65000, embed_size=16)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "patience = 5\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "number_of_epochs = 10\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    print(f\"--- Epoch {epoch+1}/{number_of_epochs} ---\")\n",
        "    for batch_context, batch_target in tqdm(trainloader):\n",
        "        #FORWARD PASS:\n",
        "        X = batch_context\n",
        "        Y = batch_target\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        outputs = model(X)  # Model output for X\n",
        "        loss = criterion(outputs, Y) # Compute the loss between model output and Y\n",
        "\n",
        "        #BACKWARD PASS (updating the model parameters):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    print(f\"Training perplexity: {np.exp(loss.item()):.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradient computation for validation\n",
        "        for inputs, targets in valloader:\n",
        "            X = inputs\n",
        "            Y = targets\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, Y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
        "    print(f\"Validation perplexity: {np.exp(avg_val_loss):.6f}\")\n",
        "\n",
        "    # Call early stopping after each epoch\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, load the best model after training\n",
        "model.load_state_dict(torch.load('checkpoint.pth'))"
      ],
      "metadata": {
        "id": "7iUtwI8nbBO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e9d120aa-a96b-4ace-da08-fcb829b008a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 1/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]<ipython-input-21-3492036d8cfc>:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  target = torch.tensor(row[-1], dtype=torch.long)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "100%|██████████| 30/30 [00:01<00:00, 24.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training perplexity: 65000.0803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'valloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-68ddda73f100>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No gradient computation for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'valloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "Hsm3CLoWgub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\"This is very\",\n",
        "                  \"A tall building\",\n",
        "                  \"The next sentence\",\n",
        "                  \"Not a big\"]\n",
        "\n",
        "encoded_sentences = []\n",
        "for sentence in test_sentences:\n",
        "  encoded_sentences.append([vocab_builder.get_token_id(sentence) for word in sentence.split(\" \")])\n",
        "\n",
        "output = model(torch.tensor(encoded_sentences)).detach().numpy()\n",
        "\n",
        "# Predict\n",
        "predictions = np.argmax(output, axis=1)\n",
        "\n",
        "for prediction in predictions:\n",
        "  print(vocab_builder.get_token_str(prediction))"
      ],
      "metadata": {
        "id": "HOp6TdEPgtmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = np.exp(avg_val_loss)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "lb9-NZAhzYLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbors(emb, voc, word, n_neighbors=5):\n",
        "\n",
        "    # Look up the embedding for the test word.\n",
        "    test_emb = emb.weight[voc.get_token_id(word)]\n",
        "\n",
        "    # We'll use a cosine similarity function to find the most similar words.\n",
        "    sim_func = nn.CosineSimilarity(dim=1)\n",
        "    cosine_scores = sim_func(test_emb, emb.weight)\n",
        "\n",
        "    # Find the positions of the highest cosine values.\n",
        "    near_nbr = cosine_scores.topk(n_neighbors+1)\n",
        "    topk_cos = near_nbr.values[1:]\n",
        "    topk_indices = near_nbr.indices[1:]\n",
        "    # NB: the first word in the top-k list is the query word itself!\n",
        "    # That's why we skip the first position in the code above.\n",
        "\n",
        "    # Finally, map word indices back to strings, and put the result in a list.\n",
        "    return [ (voc.get_token_str(ix.item()), cos.item()) for ix, cos in zip(topk_indices, topk_cos) ]\n",
        "\n",
        "nearest_neighbors(model.embeddings, vocab_builder, \"sweden\")\n",
        "nearest_neighbors(model.embeddings, vocab_builder, \"2005\")"
      ],
      "metadata": {
        "id": "Vm5IGathjhMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_embeddings_pca(emb, voc, words):\n",
        "    vectors = np.vstack([emb.weight[voc.get_token_id(w)].cpu().detach().numpy() for w in words])\n",
        "    vectors -= vectors.mean(axis=0)\n",
        "    twodim = TruncatedSVD(n_components=2).fit_transform(vectors)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.02, y, word)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_embeddings_pca(model.embeddings, vocab_builder, ['sweden', 'denmark', 'europe', 'africa', 'london', 'stockholm', 'large', 'small', 'great', 'black', '3', '7', '10', 'seven', 'three', 'ten', '1984', '2005', '2010'])\n"
      ],
      "metadata": {
        "id": "RpQwos6mj8di"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}